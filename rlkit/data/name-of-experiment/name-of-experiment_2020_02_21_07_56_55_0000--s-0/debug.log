2020-02-21 07:57:15.409996 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 0 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                       19.4911
trainer/QF2 Loss                       19.4309
trainer/Policy Loss                    -1.33257
trainer/Q1 Predictions Mean             0.00686314
trainer/Q1 Predictions Std              0.00105353
trainer/Q1 Predictions Max              0.00933575
trainer/Q1 Predictions Min              0.00456988
trainer/Q2 Predictions Mean            -0.000818429
trainer/Q2 Predictions Std              0.00080086
trainer/Q2 Predictions Max              0.000861524
trainer/Q2 Predictions Min             -0.00262513
trainer/Q Targets Mean                 -3.75331
trainer/Q Targets Std                   2.31254
trainer/Q Targets Max                   1.58361
trainer/Q Targets Min                  -7.87589
trainer/Log Pis Mean                   -1.33334
trainer/Log Pis Std                     0.31831
trainer/Log Pis Max                    -0.581365
trainer/Log Pis Min                    -1.97293
trainer/Policy mu Mean                 -0.000166976
trainer/Policy mu Std                   0.000288361
trainer/Policy mu Max                   0.00037044
trainer/Policy mu Min                  -0.000700469
trainer/Policy log std Mean            -0.000449511
trainer/Policy log std Std              0.00055809
trainer/Policy log std Max              0.000386469
trainer/Policy log std Min             -0.00159169
trainer/Alpha                           0.9997
trainer/Alpha Loss                     -0
exploration/num steps total           100
exploration/num paths total             1
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -5.58416
exploration/Rewards Std                 2.53815
exploration/Rewards Max                -0.0244947
exploration/Rewards Min                -9.62947
exploration/Returns Mean             -558.416
exploration/Returns Std                 0
exploration/Returns Max              -558.416
exploration/Returns Min              -558.416
exploration/Actions Mean                0.0974618
exploration/Actions Std                 1.1614
exploration/Actions Max                 3.01891
exploration/Actions Min                -2.71959
exploration/Num Paths                   1
exploration/Average Returns          -558.416
evaluation/num steps total           5000
evaluation/num paths total              5
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -5.38414
evaluation/Rewards Std                  1.82476
evaluation/Rewards Max                 -0.0301845
evaluation/Rewards Min                 -8.48153
evaluation/Returns Mean             -5384.14
evaluation/Returns Std                598.639
evaluation/Returns Max              -4818.72
evaluation/Returns Min              -6255.65
evaluation/Actions Mean                -0.00241139
evaluation/Actions Std                  1.01273
evaluation/Actions Max                  4.61081
evaluation/Actions Min                 -3.52036
evaluation/Num Paths                    5
evaluation/Average Returns          -5384.14
time/data storing (s)                   0.000496949
time/evaluation sampling (s)            2.68415
time/exploration real sampling (s)      0.0643859
time/exploration sim sampling (s)       5.862e-06
time/logging (s)                        0.0106549
time/saving (s)                         0.0124041
time/training (s)                      16.3692
time/epoch (s)                         19.1413
time/total (s)                         19.9848
Epoch                                   0
----------------------------------  ---------------
2020-02-21 07:57:38.792750 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 1 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.17631
trainer/QF2 Loss                        1.13202
trainer/Policy Loss                    15.1888
trainer/Q1 Predictions Mean           -17.2059
trainer/Q1 Predictions Std              9.92112
trainer/Q1 Predictions Max              2.79308
trainer/Q1 Predictions Min            -35.1772
trainer/Q2 Predictions Mean           -17.2301
trainer/Q2 Predictions Std              9.91572
trainer/Q2 Predictions Max              2.7252
trainer/Q2 Predictions Min            -35.3795
trainer/Q Targets Mean                -17.1682
trainer/Q Targets Std                  10.0433
trainer/Q Targets Max                   2.41996
trainer/Q Targets Min                 -34.7926
trainer/Log Pis Mean                   -0.350333
trainer/Log Pis Std                     1.18284
trainer/Log Pis Max                     1.7779
trainer/Log Pis Min                    -5.03822
trainer/Policy mu Mean                 -0.805883
trainer/Policy mu Std                   0.202059
trainer/Policy mu Max                  -0.284899
trainer/Policy mu Min                  -1.07848
trainer/Policy log std Mean            -0.357077
trainer/Policy log std Std              0.0428336
trainer/Policy log std Max             -0.221947
trainer/Policy log std Min             -0.438342
trainer/Alpha                           0.753491
trainer/Alpha Loss                     -0.66467
exploration/num steps total           200
exploration/num paths total             2
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.82317
exploration/Rewards Std                 2.1854
exploration/Rewards Max                -0.0071415
exploration/Rewards Min                -9.11154
exploration/Returns Mean             -182.317
exploration/Returns Std                 0
exploration/Returns Max              -182.317
exploration/Returns Min              -182.317
exploration/Actions Mean               -0.432839
exploration/Actions Std                 1.13266
exploration/Actions Max                 3.11452
exploration/Actions Min                -4.28786
exploration/Num Paths                   1
exploration/Average Returns          -182.317
evaluation/num steps total          10000
evaluation/num paths total             10
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.37111
evaluation/Rewards Std                  1.61673
evaluation/Rewards Max                 -0.00109727
evaluation/Rewards Min                 -7.19155
evaluation/Returns Mean             -1371.11
evaluation/Returns Std               1464.37
evaluation/Returns Max               -509.388
evaluation/Returns Min              -4294.6
evaluation/Actions Mean                -0.516741
evaluation/Actions Std                  0.997465
evaluation/Actions Max                  3.75766
evaluation/Actions Min                 -4.32215
evaluation/Num Paths                    5
evaluation/Average Returns          -1371.11
time/data storing (s)                   0.000631022
time/evaluation sampling (s)            2.9568
time/exploration real sampling (s)      0.0768477
time/exploration sim sampling (s)       6.619e-06
time/logging (s)                        0.0142791
time/saving (s)                         0.0113534
time/training (s)                      20.3215
time/epoch (s)                         23.3814
time/total (s)                         43.3706
Epoch                                   1
----------------------------------  ---------------
2020-02-21 07:58:02.792286 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 2 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        2.23443
trainer/QF2 Loss                        2.23245
trainer/Policy Loss                    20.488
trainer/Q1 Predictions Mean           -23.6579
trainer/Q1 Predictions Std             18.3206
trainer/Q1 Predictions Max              4.72263
trainer/Q1 Predictions Min            -51.8515
trainer/Q2 Predictions Mean           -23.6119
trainer/Q2 Predictions Std             18.3228
trainer/Q2 Predictions Max              4.56965
trainer/Q2 Predictions Min            -51.8045
trainer/Q Targets Mean                -23.6384
trainer/Q Targets Std                  18.4136
trainer/Q Targets Max                   4.13754
trainer/Q Targets Min                 -50.897
trainer/Log Pis Mean                    0.808884
trainer/Log Pis Std                     1.41341
trainer/Log Pis Max                     3.20288
trainer/Log Pis Min                    -2.60563
trainer/Policy mu Mean                 -1.07992
trainer/Policy mu Std                   0.381144
trainer/Policy mu Max                  -0.240895
trainer/Policy mu Min                  -1.62757
trainer/Policy log std Mean            -0.607151
trainer/Policy log std Std              0.0784778
trainer/Policy log std Max             -0.366466
trainer/Policy log std Min             -0.726774
trainer/Alpha                           0.615945
trainer/Alpha Loss                     -0.577004
exploration/num steps total           300
exploration/num paths total             3
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.918225
exploration/Rewards Std                 0.748579
exploration/Rewards Max                -0.0105397
exploration/Rewards Min                -3.10747
exploration/Returns Mean              -91.8225
exploration/Returns Std                 0
exploration/Returns Max               -91.8225
exploration/Returns Min               -91.8225
exploration/Actions Mean               -0.458633
exploration/Actions Std                 1.13849
exploration/Actions Max                 2.87006
exploration/Actions Min                -3.48654
exploration/Num Paths                   1
exploration/Average Returns           -91.8225
evaluation/num steps total          15000
evaluation/num paths total             15
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.77961
evaluation/Rewards Std                  1.81235
evaluation/Rewards Max                 -0.00060562
evaluation/Rewards Min                 -7.00596
evaluation/Returns Mean             -2779.61
evaluation/Returns Std               1748.61
evaluation/Returns Max               -612.541
evaluation/Returns Min              -4228.61
evaluation/Actions Mean                -0.6393
evaluation/Actions Std                  1.01442
evaluation/Actions Max                  3.64899
evaluation/Actions Min                 -4.39001
evaluation/Num Paths                    5
evaluation/Average Returns          -2779.61
time/data storing (s)                   0.000608635
time/evaluation sampling (s)            2.82333
time/exploration real sampling (s)      0.0679203
time/exploration sim sampling (s)       6.261e-06
time/logging (s)                        0.0145128
time/saving (s)                         0.011483
time/training (s)                      21.0759
time/epoch (s)                         23.9937
time/total (s)                         67.3696
Epoch                                   2
----------------------------------  ---------------
2020-02-21 07:58:27.938307 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 3 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.1812
trainer/QF2 Loss                        3.30469
trainer/Policy Loss                    17.2546
trainer/Q1 Predictions Mean           -21.9575
trainer/Q1 Predictions Std             21.1185
trainer/Q1 Predictions Max              5.32257
trainer/Q1 Predictions Min            -54.8567
trainer/Q2 Predictions Mean           -21.9235
trainer/Q2 Predictions Std             21.0826
trainer/Q2 Predictions Max              4.98185
trainer/Q2 Predictions Min            -55.0372
trainer/Q Targets Mean                -21.9189
trainer/Q Targets Std                  21.0777
trainer/Q Targets Max                   4.79178
trainer/Q Targets Min                 -52.2341
trainer/Log Pis Mean                    0.890389
trainer/Log Pis Std                     1.5867
trainer/Log Pis Max                     3.72426
trainer/Log Pis Min                    -4.62717
trainer/Policy mu Mean                 -1.03635
trainer/Policy mu Std                   0.514704
trainer/Policy mu Max                  -0.0178479
trainer/Policy mu Min                  -1.81583
trainer/Policy log std Mean            -0.658396
trainer/Policy log std Std              0.153496
trainer/Policy log std Max             -0.238868
trainer/Policy log std Min             -0.836612
trainer/Alpha                           0.505217
trainer/Alpha Loss                     -0.757388
exploration/num steps total           400
exploration/num paths total             4
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.17821
exploration/Rewards Std                 0.94819
exploration/Rewards Max                -0.00363502
exploration/Rewards Min                -5.33874
exploration/Returns Mean             -117.821
exploration/Returns Std                 0
exploration/Returns Max              -117.821
exploration/Returns Min              -117.821
exploration/Actions Mean               -0.379933
exploration/Actions Std                 1.06447
exploration/Actions Max                 2.45523
exploration/Actions Min                -3.30905
exploration/Num Paths                   1
exploration/Average Returns          -117.821
evaluation/num steps total          20000
evaluation/num paths total             20
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.83048
evaluation/Rewards Std                  1.79557
evaluation/Rewards Max                 -0.0016653
evaluation/Rewards Min                 -6.98163
evaluation/Returns Mean             -2830.48
evaluation/Returns Std               1711.34
evaluation/Returns Max               -724.438
evaluation/Returns Min              -4243.31
evaluation/Actions Mean                -0.57359
evaluation/Actions Std                  1.03503
evaluation/Actions Max                  3.19783
evaluation/Actions Min                 -4.41178
evaluation/Num Paths                    5
evaluation/Average Returns          -2830.48
time/data storing (s)                   0.000631586
time/evaluation sampling (s)            3.04474
time/exploration real sampling (s)      0.0712917
time/exploration sim sampling (s)       5.98301e-06
time/logging (s)                        0.0140221
time/saving (s)                         0.0123931
time/training (s)                      21.9959
time/epoch (s)                         25.139
time/total (s)                         92.5141
Epoch                                   3
----------------------------------  ---------------
2020-02-21 07:58:53.274883 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 4 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        2.05734
trainer/QF2 Loss                        2.07552
trainer/Policy Loss                    11.76
trainer/Q1 Predictions Mean           -15.7955
trainer/Q1 Predictions Std             18.542
trainer/Q1 Predictions Max              4.76007
trainer/Q1 Predictions Min            -48.4867
trainer/Q2 Predictions Mean           -15.7814
trainer/Q2 Predictions Std             18.5898
trainer/Q2 Predictions Max              4.75314
trainer/Q2 Predictions Min            -48.9725
trainer/Q Targets Mean                -16.176
trainer/Q Targets Std                  18.7889
trainer/Q Targets Max                   4.47895
trainer/Q Targets Min                 -48.4398
trainer/Log Pis Mean                    0.63003
trainer/Log Pis Std                     1.56331
trainer/Log Pis Max                     3.82555
trainer/Log Pis Min                    -4.19272
trainer/Policy mu Mean                 -0.952898
trainer/Policy mu Std                   0.511447
trainer/Policy mu Max                  -0.092867
trainer/Policy mu Min                  -1.97639
trainer/Policy log std Mean            -0.686041
trainer/Policy log std Std              0.164519
trainer/Policy log std Max             -0.318306
trainer/Policy log std Min             -0.970106
trainer/Alpha                           0.398748
trainer/Alpha Loss                     -1.25924
exploration/num steps total           500
exploration/num paths total             5
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.753842
exploration/Rewards Std                 0.702842
exploration/Rewards Max                -0.00316906
exploration/Rewards Min                -3.01726
exploration/Returns Mean              -75.3842
exploration/Returns Std                 0
exploration/Returns Max               -75.3842
exploration/Returns Min               -75.3842
exploration/Actions Mean               -0.531242
exploration/Actions Std                 1.08029
exploration/Actions Max                 2.05526
exploration/Actions Min                -2.75918
exploration/Num Paths                   1
exploration/Average Returns           -75.3842
evaluation/num steps total          25000
evaluation/num paths total             25
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.81695
evaluation/Rewards Std                  1.83336
evaluation/Rewards Max                 -0.00176389
evaluation/Rewards Min                 -6.57936
evaluation/Returns Mean             -2816.95
evaluation/Returns Std               1754.5
evaluation/Returns Max               -595.646
evaluation/Returns Min              -4266.19
evaluation/Actions Mean                -0.552978
evaluation/Actions Std                  1.03076
evaluation/Actions Max                  3.76293
evaluation/Actions Min                 -3.86229
evaluation/Num Paths                    5
evaluation/Average Returns          -2816.95
time/data storing (s)                   0.000593726
time/evaluation sampling (s)            3.0459
time/exploration real sampling (s)      0.0655585
time/exploration sim sampling (s)       6.45e-06
time/logging (s)                        0.0163517
time/saving (s)                         0.0221204
time/training (s)                      22.1819
time/epoch (s)                         25.3325
time/total (s)                        117.852
Epoch                                   4
----------------------------------  ---------------
2020-02-21 07:59:19.196016 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 5 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.66683
trainer/QF2 Loss                        1.64134
trainer/Policy Loss                    11.859
trainer/Q1 Predictions Mean           -15.1989
trainer/Q1 Predictions Std             16.4501
trainer/Q1 Predictions Max              4.00837
trainer/Q1 Predictions Min            -43.8327
trainer/Q2 Predictions Mean           -15.1602
trainer/Q2 Predictions Std             16.4704
trainer/Q2 Predictions Max              3.98109
trainer/Q2 Predictions Min            -43.6145
trainer/Q Targets Mean                -14.9717
trainer/Q Targets Std                  16.3481
trainer/Q Targets Max                   3.78482
trainer/Q Targets Min                 -44.5423
trainer/Log Pis Mean                    0.656354
trainer/Log Pis Std                     1.68151
trainer/Log Pis Max                     4.05674
trainer/Log Pis Min                    -4.01481
trainer/Policy mu Mean                 -0.989217
trainer/Policy mu Std                   0.507329
trainer/Policy mu Max                   0.073863
trainer/Policy mu Min                  -1.90471
trainer/Policy log std Mean            -0.734289
trainer/Policy log std Std              0.165202
trainer/Policy log std Max             -0.328045
trainer/Policy log std Min             -1.1161
trainer/Alpha                           0.306659
trainer/Alpha Loss                     -1.58787
exploration/num steps total           600
exploration/num paths total             6
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.34554
exploration/Rewards Std                 1.61938
exploration/Rewards Max                -0.00548278
exploration/Rewards Min                -7.0383
exploration/Returns Mean             -134.554
exploration/Returns Std                 0
exploration/Returns Max              -134.554
exploration/Returns Min              -134.554
exploration/Actions Mean               -0.43497
exploration/Actions Std                 1.12203
exploration/Actions Max                 3.30995
exploration/Actions Min                -3.2583
exploration/Num Paths                   1
exploration/Average Returns          -134.554
evaluation/num steps total          30000
evaluation/num paths total             30
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.07011
evaluation/Rewards Std                  1.92768
evaluation/Rewards Max                 -0.00107257
evaluation/Rewards Min                 -6.80833
evaluation/Returns Mean             -2070.11
evaluation/Returns Std               1829.85
evaluation/Returns Max               -566.269
evaluation/Returns Min              -4320.64
evaluation/Actions Mean                -0.537794
evaluation/Actions Std                  1.02882
evaluation/Actions Max                  3.11614
evaluation/Actions Min                 -4.17491
evaluation/Num Paths                    5
evaluation/Average Returns          -2070.11
time/data storing (s)                   0.000637402
time/evaluation sampling (s)            3.45492
time/exploration real sampling (s)      0.0994214
time/exploration sim sampling (s)       6.543e-06
time/logging (s)                        0.0151427
time/saving (s)                         0.0135079
time/training (s)                      22.3306
time/epoch (s)                         25.9143
time/total (s)                        143.771
Epoch                                   5
----------------------------------  ---------------
2020-02-21 07:59:44.557498 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 6 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.4336
trainer/QF2 Loss                        1.39269
trainer/Policy Loss                    12.0643
trainer/Q1 Predictions Mean           -15.2407
trainer/Q1 Predictions Std             15.7696
trainer/Q1 Predictions Max              3.32705
trainer/Q1 Predictions Min            -44.6674
trainer/Q2 Predictions Mean           -15.2196
trainer/Q2 Predictions Std             15.7763
trainer/Q2 Predictions Max              3.14164
trainer/Q2 Predictions Min            -44.2852
trainer/Q Targets Mean                -15.153
trainer/Q Targets Std                  15.8823
trainer/Q Targets Max                   3.29951
trainer/Q Targets Min                 -47.4035
trainer/Log Pis Mean                    1.1632
trainer/Log Pis Std                     1.58184
trainer/Log Pis Max                     4.41714
trainer/Log Pis Min                    -2.86601
trainer/Policy mu Mean                 -1.05292
trainer/Policy mu Std                   0.521009
trainer/Policy mu Max                   0.132226
trainer/Policy mu Min                  -2.06714
trainer/Policy log std Mean            -0.7921
trainer/Policy log std Std              0.177408
trainer/Policy log std Max             -0.352471
trainer/Policy log std Min             -1.28319
trainer/Alpha                           0.237461
trainer/Alpha Loss                     -1.20291
exploration/num steps total           700
exploration/num paths total             7
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.04857
exploration/Rewards Std                 0.903248
exploration/Rewards Max                -0.00828291
exploration/Rewards Min                -4.02124
exploration/Returns Mean             -104.857
exploration/Returns Std                 0
exploration/Returns Max              -104.857
exploration/Returns Min              -104.857
exploration/Actions Mean               -0.427458
exploration/Actions Std                 1.11424
exploration/Actions Max                 2.36614
exploration/Actions Min                -3.36888
exploration/Num Paths                   1
exploration/Average Returns          -104.857
evaluation/num steps total          35000
evaluation/num paths total             35
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.84624
evaluation/Rewards Std                  1.90004
evaluation/Rewards Max                 -0.00214408
evaluation/Rewards Min                 -7.14149
evaluation/Returns Mean             -2846.24
evaluation/Returns Std               1810.71
evaluation/Returns Max               -556.451
evaluation/Returns Min              -4340.1
evaluation/Actions Mean                -0.558171
evaluation/Actions Std                  1.05063
evaluation/Actions Max                  3.28825
evaluation/Actions Min                 -5.19883
evaluation/Num Paths                    5
evaluation/Average Returns          -2846.24
time/data storing (s)                   0.000648678
time/evaluation sampling (s)            3.22799
time/exploration real sampling (s)      0.111468
time/exploration sim sampling (s)       1.2364e-05
time/logging (s)                        0.0140985
time/saving (s)                         0.0121802
time/training (s)                      21.9878
time/epoch (s)                         25.3542
time/total (s)                        169.131
Epoch                                   6
----------------------------------  ---------------
2020-02-21 08:00:08.557006 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 7 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.11853
trainer/QF2 Loss                        1.0205
trainer/Policy Loss                    10.8324
trainer/Q1 Predictions Mean           -13.2846
trainer/Q1 Predictions Std             15.622
trainer/Q1 Predictions Max              2.87759
trainer/Q1 Predictions Min            -42.8327
trainer/Q2 Predictions Mean           -13.2472
trainer/Q2 Predictions Std             15.6125
trainer/Q2 Predictions Max              3.47502
trainer/Q2 Predictions Min            -42.6327
trainer/Q Targets Mean                -12.8639
trainer/Q Targets Std                  15.5589
trainer/Q Targets Max                   2.71912
trainer/Q Targets Min                 -40.393
trainer/Log Pis Mean                    1.19519
trainer/Log Pis Std                     1.75197
trainer/Log Pis Max                     4.91862
trainer/Log Pis Min                    -3.83166
trainer/Policy mu Mean                 -1.09358
trainer/Policy mu Std                   0.517578
trainer/Policy mu Max                   0.031877
trainer/Policy mu Min                  -2.1199
trainer/Policy log std Mean            -0.818138
trainer/Policy log std Std              0.18859
trainer/Policy log std Max             -0.420607
trainer/Policy log std Min             -1.39893
trainer/Alpha                           0.186649
trainer/Alpha Loss                     -1.35072
exploration/num steps total           800
exploration/num paths total             8
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.772465
exploration/Rewards Std                 1.11438
exploration/Rewards Max                -0.00359225
exploration/Rewards Min                -5.36754
exploration/Returns Mean              -77.2465
exploration/Returns Std                 0
exploration/Returns Max               -77.2465
exploration/Returns Min               -77.2465
exploration/Actions Mean               -0.581988
exploration/Actions Std                 0.984074
exploration/Actions Max                 2.23315
exploration/Actions Min                -3.38497
exploration/Num Paths                   1
exploration/Average Returns           -77.2465
evaluation/num steps total          40000
evaluation/num paths total             40
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.75812
evaluation/Rewards Std                  1.92582
evaluation/Rewards Max                 -0.000870029
evaluation/Rewards Min                 -6.32082
evaluation/Returns Mean             -2758.12
evaluation/Returns Std               1875
evaluation/Returns Max               -445.17
evaluation/Returns Min              -4296.74
evaluation/Actions Mean                -0.60044
evaluation/Actions Std                  1.05138
evaluation/Actions Max                  3.05997
evaluation/Actions Min                 -5.05028
evaluation/Num Paths                    5
evaluation/Average Returns          -2758.12
time/data storing (s)                   0.000602965
time/evaluation sampling (s)            2.89273
time/exploration real sampling (s)      0.0679541
time/exploration sim sampling (s)       6.651e-06
time/logging (s)                        0.0128526
time/saving (s)                         0.0193459
time/training (s)                      20.9987
time/epoch (s)                         23.9922
time/total (s)                        193.128
Epoch                                   7
----------------------------------  ---------------
2020-02-21 08:00:31.351731 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 8 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.20439
trainer/QF2 Loss                        1.22186
trainer/Policy Loss                    11.3968
trainer/Q1 Predictions Mean           -13.4936
trainer/Q1 Predictions Std             14.4189
trainer/Q1 Predictions Max              1.84111
trainer/Q1 Predictions Min            -39.6705
trainer/Q2 Predictions Mean           -13.481
trainer/Q2 Predictions Std             14.4063
trainer/Q2 Predictions Max              1.82269
trainer/Q2 Predictions Min            -39.9532
trainer/Q Targets Mean                -13.3814
trainer/Q Targets Std                  14.4963
trainer/Q Targets Max                   1.87601
trainer/Q Targets Min                 -40.3881
trainer/Log Pis Mean                    1.57491
trainer/Log Pis Std                     1.82395
trainer/Log Pis Max                     5.48313
trainer/Log Pis Min                    -3.36264
trainer/Policy mu Mean                 -1.21504
trainer/Policy mu Std                   0.545273
trainer/Policy mu Max                   0.0599169
trainer/Policy mu Min                  -2.27331
trainer/Policy log std Mean            -0.862242
trainer/Policy log std Std              0.184613
trainer/Policy log std Max             -0.443278
trainer/Policy log std Min             -1.52125
trainer/Alpha                           0.148421
trainer/Alpha Loss                     -0.810865
exploration/num steps total           900
exploration/num paths total             9
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.873229
exploration/Rewards Std                 1.0919
exploration/Rewards Max                -0.00275496
exploration/Rewards Min                -6.46058
exploration/Returns Mean              -87.3229
exploration/Returns Std                 0
exploration/Returns Max               -87.3229
exploration/Returns Min               -87.3229
exploration/Actions Mean               -0.579847
exploration/Actions Std                 1.1049
exploration/Actions Max                 2.14288
exploration/Actions Min                -3.60738
exploration/Num Paths                   1
exploration/Average Returns           -87.3229
evaluation/num steps total          45000
evaluation/num paths total             45
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.78357
evaluation/Rewards Std                  1.9378
evaluation/Rewards Max                 -0.00112948
evaluation/Rewards Min                 -6.93022
evaluation/Returns Mean             -2783.57
evaluation/Returns Std               1874.35
evaluation/Returns Max               -460.996
evaluation/Returns Min              -4352.65
evaluation/Actions Mean                -0.579108
evaluation/Actions Std                  1.03611
evaluation/Actions Max                  3.14799
evaluation/Actions Min                 -4.47567
evaluation/Num Paths                    5
evaluation/Average Returns          -2783.57
time/data storing (s)                   0.000630253
time/evaluation sampling (s)            3.15641
time/exploration real sampling (s)      0.0812008
time/exploration sim sampling (s)       7.118e-06
time/logging (s)                        0.0138728
time/saving (s)                         0.0123953
time/training (s)                      19.5254
time/epoch (s)                         22.7899
time/total (s)                        215.923
Epoch                                   8
----------------------------------  ---------------
2020-02-21 08:00:55.442285 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 9 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.669593
trainer/QF2 Loss                        0.613714
trainer/Policy Loss                    11.1897
trainer/Q1 Predictions Mean           -12.7781
trainer/Q1 Predictions Std             14.6946
trainer/Q1 Predictions Max              1.4577
trainer/Q1 Predictions Min            -45.3108
trainer/Q2 Predictions Mean           -12.8246
trainer/Q2 Predictions Std             14.745
trainer/Q2 Predictions Max              1.47754
trainer/Q2 Predictions Min            -45.9127
trainer/Q Targets Mean                -12.8917
trainer/Q Targets Std                  14.7699
trainer/Q Targets Max                   1.25258
trainer/Q Targets Min                 -47.4287
trainer/Log Pis Mean                    1.72189
trainer/Log Pis Std                     1.90322
trainer/Log Pis Max                     5.68851
trainer/Log Pis Min                    -3.62148
trainer/Policy mu Mean                 -1.22387
trainer/Policy mu Std                   0.577801
trainer/Policy mu Max                   0.130855
trainer/Policy mu Min                  -2.42913
trainer/Policy log std Mean            -0.860791
trainer/Policy log std Std              0.171113
trainer/Policy log std Max             -0.468856
trainer/Policy log std Min             -1.51465
trainer/Alpha                           0.120684
trainer/Alpha Loss                     -0.588041
exploration/num steps total          1000
exploration/num paths total            10
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.814839
exploration/Rewards Std                 0.943449
exploration/Rewards Max                -0.00746389
exploration/Rewards Min                -4.76276
exploration/Returns Mean              -81.4839
exploration/Returns Std                 0
exploration/Returns Max               -81.4839
exploration/Returns Min               -81.4839
exploration/Actions Mean               -0.523739
exploration/Actions Std                 0.999862
exploration/Actions Max                 2.46813
exploration/Actions Min                -3.40647
exploration/Num Paths                   1
exploration/Average Returns           -81.4839
evaluation/num steps total          50000
evaluation/num paths total             50
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.26095
evaluation/Rewards Std                  1.65191
evaluation/Rewards Max                 -0.00241778
evaluation/Rewards Min                 -6.62646
evaluation/Returns Mean             -1260.95
evaluation/Returns Std               1537.38
evaluation/Returns Max               -436.896
evaluation/Returns Min              -4334.62
evaluation/Actions Mean                -0.605269
evaluation/Actions Std                  1.03231
evaluation/Actions Max                  3.16558
evaluation/Actions Min                 -4.51677
evaluation/Num Paths                    5
evaluation/Average Returns          -1260.95
time/data storing (s)                   0.000604375
time/evaluation sampling (s)            2.72101
time/exploration real sampling (s)      0.0643522
time/exploration sim sampling (s)       6.33e-06
time/logging (s)                        0.0153702
time/saving (s)                         0.0101963
time/training (s)                      21.2747
time/epoch (s)                         24.0862
time/total (s)                        240.014
Epoch                                   9
----------------------------------  ---------------
2020-02-21 08:01:19.646687 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 10 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.633132
trainer/QF2 Loss                        0.573313
trainer/Policy Loss                    10.7288
trainer/Q1 Predictions Mean           -12.1208
trainer/Q1 Predictions Std             14.7007
trainer/Q1 Predictions Max              0.711328
trainer/Q1 Predictions Min            -42.2158
trainer/Q2 Predictions Mean           -12.0929
trainer/Q2 Predictions Std             14.7116
trainer/Q2 Predictions Max              0.759543
trainer/Q2 Predictions Min            -41.9684
trainer/Q Targets Mean                -11.9111
trainer/Q Targets Std                  14.6329
trainer/Q Targets Max                   0.739488
trainer/Q Targets Min                 -41.8386
trainer/Log Pis Mean                    1.49605
trainer/Log Pis Std                     1.79849
trainer/Log Pis Max                     5.93044
trainer/Log Pis Min                    -4.52419
trainer/Policy mu Mean                 -1.23146
trainer/Policy mu Std                   0.568903
trainer/Policy mu Max                  -0.0955966
trainer/Policy mu Min                  -2.48516
trainer/Policy log std Mean            -0.819643
trainer/Policy log std Std              0.167447
trainer/Policy log std Max             -0.507877
trainer/Policy log std Min             -1.48797
trainer/Alpha                           0.10073
trainer/Alpha Loss                     -1.15666
exploration/num steps total          1100
exploration/num paths total            11
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.58533
exploration/Rewards Std                 0.613934
exploration/Rewards Max                -0.0146626
exploration/Rewards Min                -3.36078
exploration/Returns Mean              -58.533
exploration/Returns Std                 0
exploration/Returns Max               -58.533
exploration/Returns Min               -58.533
exploration/Actions Mean               -0.544742
exploration/Actions Std                 0.952333
exploration/Actions Max                 2.02031
exploration/Actions Min                -2.489
exploration/Num Paths                   1
exploration/Average Returns           -58.533
evaluation/num steps total          55000
evaluation/num paths total             55
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.02563
evaluation/Rewards Std                  1.98821
evaluation/Rewards Max                 -0.000503805
evaluation/Rewards Min                 -6.649
evaluation/Returns Mean             -2025.63
evaluation/Returns Std               1936.68
evaluation/Returns Max               -410.35
evaluation/Returns Min              -4432.38
evaluation/Actions Mean                -0.579083
evaluation/Actions Std                  1.05585
evaluation/Actions Max                  3.61271
evaluation/Actions Min                 -5.00635
evaluation/Num Paths                    5
evaluation/Average Returns          -2025.63
time/data storing (s)                   0.000625496
time/evaluation sampling (s)            2.94074
time/exploration real sampling (s)      0.066661
time/exploration sim sampling (s)       6.865e-06
time/logging (s)                        0.0129197
time/saving (s)                         0.0177804
time/training (s)                      21.1555
time/epoch (s)                         24.1942
time/total (s)                        264.215
Epoch                                  10
----------------------------------  ---------------
2020-02-21 08:01:43.084143 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 11 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.68451
trainer/QF2 Loss                        0.669273
trainer/Policy Loss                    10.4808
trainer/Q1 Predictions Mean           -12.0352
trainer/Q1 Predictions Std             13.988
trainer/Q1 Predictions Max              0.51333
trainer/Q1 Predictions Min            -40.6819
trainer/Q2 Predictions Mean           -12.0407
trainer/Q2 Predictions Std             13.9487
trainer/Q2 Predictions Max              0.476751
trainer/Q2 Predictions Min            -40.7947
trainer/Q Targets Mean                -12.2859
trainer/Q Targets Std                  14.1201
trainer/Q Targets Max                   0.298906
trainer/Q Targets Min                 -40.5912
trainer/Log Pis Mean                    1.54927
trainer/Log Pis Std                     1.92776
trainer/Log Pis Max                     5.60738
trainer/Log Pis Min                    -5.54212
trainer/Policy mu Mean                 -1.16939
trainer/Policy mu Std                   0.628939
trainer/Policy mu Max                   0.350659
trainer/Policy mu Min                  -2.473
trainer/Policy log std Mean            -0.90176
trainer/Policy log std Std              0.223723
trainer/Policy log std Max             -0.460187
trainer/Policy log std Min             -1.61547
trainer/Alpha                           0.0882645
trainer/Alpha Loss                     -1.09407
exploration/num steps total          1200
exploration/num paths total            12
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.922326
exploration/Rewards Std                 1.01626
exploration/Rewards Max                -0.00454158
exploration/Rewards Min                -4.63825
exploration/Returns Mean              -92.2326
exploration/Returns Std                 0
exploration/Returns Max               -92.2326
exploration/Returns Min               -92.2326
exploration/Actions Mean               -0.584978
exploration/Actions Std                 1.11512
exploration/Actions Max                 2.72188
exploration/Actions Min                -3.89557
exploration/Num Paths                   1
exploration/Average Returns           -92.2326
evaluation/num steps total          60000
evaluation/num paths total             60
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.82697
evaluation/Rewards Std                  1.95459
evaluation/Rewards Max                 -0.00218573
evaluation/Rewards Min                 -6.9532
evaluation/Returns Mean             -2826.97
evaluation/Returns Std               1881.51
evaluation/Returns Max               -449.369
evaluation/Returns Min              -4397.12
evaluation/Actions Mean                -0.557617
evaluation/Actions Std                  1.07577
evaluation/Actions Max                  3.54898
evaluation/Actions Min                 -4.38617
evaluation/Num Paths                    5
evaluation/Average Returns          -2826.97
time/data storing (s)                   0.000607423
time/evaluation sampling (s)            2.8775
time/exploration real sampling (s)      0.0782914
time/exploration sim sampling (s)       6.481e-06
time/logging (s)                        0.0135089
time/saving (s)                         0.0123125
time/training (s)                      20.4489
time/epoch (s)                         23.4312
time/total (s)                        287.652
Epoch                                  11
----------------------------------  ---------------
2020-02-21 08:02:06.874389 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 12 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.877402
trainer/QF2 Loss                        0.784834
trainer/Policy Loss                    11.0721
trainer/Q1 Predictions Mean           -12.1082
trainer/Q1 Predictions Std             14.1923
trainer/Q1 Predictions Max              0.10989
trainer/Q1 Predictions Min            -40.623
trainer/Q2 Predictions Mean           -12.1394
trainer/Q2 Predictions Std             14.2512
trainer/Q2 Predictions Max              0.308103
trainer/Q2 Predictions Min            -41.2629
trainer/Q Targets Mean                -12.1187
trainer/Q Targets Std                  14.3624
trainer/Q Targets Max                  -0.0982016
trainer/Q Targets Min                 -42.2747
trainer/Log Pis Mean                    1.9696
trainer/Log Pis Std                     2.0132
trainer/Log Pis Max                     6.83753
trainer/Log Pis Min                    -3.51144
trainer/Policy mu Mean                 -1.2704
trainer/Policy mu Std                   0.651873
trainer/Policy mu Max                   0.320703
trainer/Policy mu Min                  -2.64063
trainer/Policy log std Mean            -0.914861
trainer/Policy log std Std              0.259011
trainer/Policy log std Max             -0.405913
trainer/Policy log std Min             -1.64937
trainer/Alpha                           0.0835588
trainer/Alpha Loss                     -0.0754515
exploration/num steps total          1300
exploration/num paths total            13
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.460703
exploration/Rewards Std                 0.501019
exploration/Rewards Max                -0.00383059
exploration/Rewards Min                -3.01251
exploration/Returns Mean              -46.0703
exploration/Returns Std                 0
exploration/Returns Max               -46.0703
exploration/Returns Min               -46.0703
exploration/Actions Mean               -0.573542
exploration/Actions Std                 0.924111
exploration/Actions Max                 2.04885
exploration/Actions Min                -2.83439
exploration/Num Paths                   1
exploration/Average Returns           -46.0703
evaluation/num steps total          65000
evaluation/num paths total             65
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.77318
evaluation/Rewards Std                  1.94406
evaluation/Rewards Max                 -0.00069027
evaluation/Rewards Min                 -6.78777
evaluation/Returns Mean             -2773.18
evaluation/Returns Std               1894.31
evaluation/Returns Max               -449.208
evaluation/Returns Min              -4329
evaluation/Actions Mean                -0.577544
evaluation/Actions Std                  1.08017
evaluation/Actions Max                  3.19915
evaluation/Actions Min                 -4.98528
evaluation/Num Paths                    5
evaluation/Average Returns          -2773.18
time/data storing (s)                   0.000608001
time/evaluation sampling (s)            2.89176
time/exploration real sampling (s)      0.0650477
time/exploration sim sampling (s)       7.658e-06
time/logging (s)                        0.014563
time/saving (s)                         0.0110773
time/training (s)                      20.8019
time/epoch (s)                         23.785
time/total (s)                        311.442
Epoch                                  12
----------------------------------  ---------------
2020-02-21 08:02:31.590733 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 13 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.510037
trainer/QF2 Loss                        0.378958
trainer/Policy Loss                    10.8617
trainer/Q1 Predictions Mean           -11.4218
trainer/Q1 Predictions Std             13.4401
trainer/Q1 Predictions Max             -0.427633
trainer/Q1 Predictions Min            -41.592
trainer/Q2 Predictions Mean           -11.4193
trainer/Q2 Predictions Std             13.473
trainer/Q2 Predictions Max              0.216163
trainer/Q2 Predictions Min            -41.676
trainer/Q Targets Mean                -11.3731
trainer/Q Targets Std                  13.4938
trainer/Q Targets Max                  -0.443836
trainer/Q Targets Min                 -41.7925
trainer/Log Pis Mean                    2.05539
trainer/Log Pis Std                     1.84202
trainer/Log Pis Max                     6.08612
trainer/Log Pis Min                    -2.80767
trainer/Policy mu Mean                 -1.23836
trainer/Policy mu Std                   0.664359
trainer/Policy mu Max                   0.271678
trainer/Policy mu Min                  -2.72812
trainer/Policy log std Mean            -0.93093
trainer/Policy log std Std              0.273286
trainer/Policy log std Max             -0.52222
trainer/Policy log std Min             -1.60138
trainer/Alpha                           0.0805089
trainer/Alpha Loss                      0.139551
exploration/num steps total          1400
exploration/num paths total            14
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.781028
exploration/Rewards Std                 1.1553
exploration/Rewards Max                -0.00701188
exploration/Rewards Min                -7.2137
exploration/Returns Mean              -78.1028
exploration/Returns Std                 0
exploration/Returns Max               -78.1028
exploration/Returns Min               -78.1028
exploration/Actions Mean               -0.660752
exploration/Actions Std                 1.10953
exploration/Actions Max                 2.15318
exploration/Actions Min                -5.65759
exploration/Num Paths                   1
exploration/Average Returns           -78.1028
evaluation/num steps total          70000
evaluation/num paths total             70
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.11026
evaluation/Rewards Std                  1.98759
evaluation/Rewards Max                 -0.000298919
evaluation/Rewards Min                 -6.95971
evaluation/Returns Mean             -2110.26
evaluation/Returns Std               1857.43
evaluation/Returns Max               -430.442
evaluation/Returns Min              -4404.08
evaluation/Actions Mean                -0.599905
evaluation/Actions Std                  1.07288
evaluation/Actions Max                  3.48354
evaluation/Actions Min                 -4.74307
evaluation/Num Paths                    5
evaluation/Average Returns          -2110.26
time/data storing (s)                   0.000622463
time/evaluation sampling (s)            3.07739
time/exploration real sampling (s)      0.064644
time/exploration sim sampling (s)       6.645e-06
time/logging (s)                        0.013947
time/saving (s)                         0.0130823
time/training (s)                      21.5389
time/epoch (s)                         24.7085
time/total (s)                        336.157
Epoch                                  13
----------------------------------  ---------------
2020-02-21 08:02:56.245639 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 14 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.6856
trainer/QF2 Loss                        0.548029
trainer/Policy Loss                    10.957
trainer/Q1 Predictions Mean           -11.7406
trainer/Q1 Predictions Std             13.8995
trainer/Q1 Predictions Max             -0.783824
trainer/Q1 Predictions Min            -43.5705
trainer/Q2 Predictions Mean           -11.6924
trainer/Q2 Predictions Std             13.84
trainer/Q2 Predictions Max             -0.444675
trainer/Q2 Predictions Min            -42.9776
trainer/Q Targets Mean                -11.4963
trainer/Q Targets Std                  13.6541
trainer/Q Targets Max                  -0.663833
trainer/Q Targets Min                 -41.8499
trainer/Log Pis Mean                    2.03189
trainer/Log Pis Std                     1.91559
trainer/Log Pis Max                     6.59627
trainer/Log Pis Min                    -3.43978
trainer/Policy mu Mean                 -1.23238
trainer/Policy mu Std                   0.691125
trainer/Policy mu Max                   0.548854
trainer/Policy mu Min                  -2.62736
trainer/Policy log std Mean            -0.965991
trainer/Policy log std Std              0.311217
trainer/Policy log std Max             -0.580049
trainer/Policy log std Min             -1.90425
trainer/Alpha                           0.0784995
trainer/Alpha Loss                      0.0811511
exploration/num steps total          1500
exploration/num paths total            15
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.495518
exploration/Rewards Std                 0.630183
exploration/Rewards Max                -0.00864489
exploration/Rewards Min                -3.91084
exploration/Returns Mean              -49.5518
exploration/Returns Std                 0
exploration/Returns Max               -49.5518
exploration/Returns Min               -49.5518
exploration/Actions Mean               -0.641516
exploration/Actions Std                 0.986615
exploration/Actions Max                 2.05668
exploration/Actions Min                -3.19894
exploration/Num Paths                   1
exploration/Average Returns           -49.5518
evaluation/num steps total          75000
evaluation/num paths total             75
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.33553
evaluation/Rewards Std                  1.68792
evaluation/Rewards Max                 -0.00104512
evaluation/Rewards Min                 -7.65232
evaluation/Returns Mean             -1335.53
evaluation/Returns Std               1539.1
evaluation/Returns Max               -429.712
evaluation/Returns Min              -4387.03
evaluation/Actions Mean                -0.622319
evaluation/Actions Std                  1.05127
evaluation/Actions Max                  3.26035
evaluation/Actions Min                 -4.63018
evaluation/Num Paths                    5
evaluation/Average Returns          -1335.53
time/data storing (s)                   0.000593266
time/evaluation sampling (s)            2.95152
time/exploration real sampling (s)      0.0641609
time/exploration sim sampling (s)       6.319e-06
time/logging (s)                        0.0140926
time/saving (s)                         0.0131823
time/training (s)                      21.6041
time/epoch (s)                         24.6476
time/total (s)                        360.811
Epoch                                  14
----------------------------------  ---------------
2020-02-21 08:03:20.791640 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 15 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.369266
trainer/QF2 Loss                        0.339326
trainer/Policy Loss                     9.32899
trainer/Q1 Predictions Mean            -9.60197
trainer/Q1 Predictions Std             12.1139
trainer/Q1 Predictions Max             -0.840765
trainer/Q1 Predictions Min            -44.0437
trainer/Q2 Predictions Mean            -9.6114
trainer/Q2 Predictions Std             12.1509
trainer/Q2 Predictions Max             -0.888618
trainer/Q2 Predictions Min            -44.1018
trainer/Q Targets Mean                 -9.59282
trainer/Q Targets Std                  12.1582
trainer/Q Targets Max                  -1.08444
trainer/Q Targets Min                 -44.3822
trainer/Log Pis Mean                    1.84077
trainer/Log Pis Std                     1.85493
trainer/Log Pis Max                     6.27355
trainer/Log Pis Min                    -3.29062
trainer/Policy mu Mean                 -1.21064
trainer/Policy mu Std                   0.674823
trainer/Policy mu Max                   0.471875
trainer/Policy mu Min                  -2.76534
trainer/Policy log std Mean            -0.912259
trainer/Policy log std Std              0.278584
trainer/Policy log std Max             -0.488477
trainer/Policy log std Min             -1.55557
trainer/Alpha                           0.0762696
trainer/Alpha Loss                     -0.409791
exploration/num steps total          1600
exploration/num paths total            16
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.06696
exploration/Rewards Std                 1.12201
exploration/Rewards Max                -0.00970511
exploration/Rewards Min                -4.89458
exploration/Returns Mean             -106.696
exploration/Returns Std                 0
exploration/Returns Max              -106.696
exploration/Returns Min              -106.696
exploration/Actions Mean               -0.517925
exploration/Actions Std                 1.12246
exploration/Actions Max                 2.83792
exploration/Actions Min                -3.8132
exploration/Num Paths                   1
exploration/Average Returns          -106.696
evaluation/num steps total          80000
evaluation/num paths total             80
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.04409
evaluation/Rewards Std                  1.97799
evaluation/Rewards Max                 -0.000252767
evaluation/Rewards Min                 -7.25678
evaluation/Returns Mean             -2044.09
evaluation/Returns Std               1894.8
evaluation/Returns Max               -488.439
evaluation/Returns Min              -4369.15
evaluation/Actions Mean                -0.598503
evaluation/Actions Std                  1.06652
evaluation/Actions Max                  3.31385
evaluation/Actions Min                 -4.12723
evaluation/Num Paths                    5
evaluation/Average Returns          -2044.09
time/data storing (s)                   0.000614086
time/evaluation sampling (s)            2.85417
time/exploration real sampling (s)      0.0691408
time/exploration sim sampling (s)       6.591e-06
time/logging (s)                        0.0135462
time/saving (s)                         0.0130525
time/training (s)                      21.5882
time/epoch (s)                         24.5388
time/total (s)                        385.355
Epoch                                  15
----------------------------------  ---------------
2020-02-21 08:03:45.642035 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 16 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.523257
trainer/QF2 Loss                        0.384404
trainer/Policy Loss                    12.099
trainer/Q1 Predictions Mean           -12.5145
trainer/Q1 Predictions Std             14.1284
trainer/Q1 Predictions Max             -1.42686
trainer/Q1 Predictions Min            -44.3783
trainer/Q2 Predictions Mean           -12.4852
trainer/Q2 Predictions Std             14.1124
trainer/Q2 Predictions Max             -1.45794
trainer/Q2 Predictions Min            -44.4821
trainer/Q Targets Mean                -12.5003
trainer/Q Targets Std                  14.2285
trainer/Q Targets Max                  -1.32259
trainer/Q Targets Min                 -46.8926
trainer/Log Pis Mean                    2.13988
trainer/Log Pis Std                     1.90586
trainer/Log Pis Max                     6.47217
trainer/Log Pis Min                    -5.06981
trainer/Policy mu Mean                 -1.31959
trainer/Policy mu Std                   0.709782
trainer/Policy mu Max                   0.690125
trainer/Policy mu Min                  -2.789
trainer/Policy log std Mean            -0.895732
trainer/Policy log std Std              0.287068
trainer/Policy log std Max             -0.3824
trainer/Policy log std Min             -1.62416
trainer/Alpha                           0.0741973
trainer/Alpha Loss                      0.363841
exploration/num steps total          1700
exploration/num paths total            17
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.579424
exploration/Rewards Std                 0.758161
exploration/Rewards Max                -0.0163976
exploration/Rewards Min                -4.78278
exploration/Returns Mean              -57.9424
exploration/Returns Std                 0
exploration/Returns Max               -57.9424
exploration/Returns Min               -57.9424
exploration/Actions Mean               -0.589372
exploration/Actions Std                 1.0527
exploration/Actions Max                 2.3791
exploration/Actions Min                -3.813
exploration/Num Paths                   1
exploration/Average Returns           -57.9424
evaluation/num steps total          85000
evaluation/num paths total             85
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.85751
evaluation/Rewards Std                  1.99646
evaluation/Rewards Max                 -0.000350247
evaluation/Rewards Min                 -7.08717
evaluation/Returns Mean             -2857.51
evaluation/Returns Std               1939.36
evaluation/Returns Max               -460.563
evaluation/Returns Min              -4460.52
evaluation/Actions Mean                -0.551007
evaluation/Actions Std                  1.10336
evaluation/Actions Max                  3.26294
evaluation/Actions Min                 -4.9895
evaluation/Num Paths                    5
evaluation/Average Returns          -2857.51
time/data storing (s)                   0.000629957
time/evaluation sampling (s)            3.10743
time/exploration real sampling (s)      0.0696878
time/exploration sim sampling (s)       8.362e-06
time/logging (s)                        0.0139839
time/saving (s)                         0.0118891
time/training (s)                      21.6401
time/epoch (s)                         24.8438
time/total (s)                        410.205
Epoch                                  16
----------------------------------  ---------------
2020-02-21 08:04:10.645186 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 17 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.913938
trainer/QF2 Loss                        0.80292
trainer/Policy Loss                    11.9584
trainer/Q1 Predictions Mean           -12.3436
trainer/Q1 Predictions Std             13.3388
trainer/Q1 Predictions Max             -1.77204
trainer/Q1 Predictions Min            -40.3868
trainer/Q2 Predictions Mean           -12.4698
trainer/Q2 Predictions Std             13.4068
trainer/Q2 Predictions Max             -1.8513
trainer/Q2 Predictions Min            -41.2128
trainer/Q Targets Mean                -12.4776
trainer/Q Targets Std                  13.4641
trainer/Q Targets Max                  -1.74825
trainer/Q Targets Min                 -43.0123
trainer/Log Pis Mean                    1.88993
trainer/Log Pis Std                     1.92901
trainer/Log Pis Max                     6.97904
trainer/Log Pis Min                    -4.42684
trainer/Policy mu Mean                 -1.21329
trainer/Policy mu Std                   0.737202
trainer/Policy mu Max                   0.788287
trainer/Policy mu Min                  -2.8307
trainer/Policy log std Mean            -0.965715
trainer/Policy log std Std              0.30398
trainer/Policy log std Max             -0.328007
trainer/Policy log std Min             -1.8593
trainer/Alpha                           0.0721066
trainer/Alpha Loss                     -0.289427
exploration/num steps total          1800
exploration/num paths total            18
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.758809
exploration/Rewards Std                 1.11436
exploration/Rewards Max                -0.00193083
exploration/Rewards Min                -5.09031
exploration/Returns Mean              -75.8809
exploration/Returns Std                 0
exploration/Returns Max               -75.8809
exploration/Returns Min               -75.8809
exploration/Actions Mean               -0.618052
exploration/Actions Std                 1.00348
exploration/Actions Max                 1.93851
exploration/Actions Min                -3.20596
exploration/Num Paths                   1
exploration/Average Returns           -75.8809
evaluation/num steps total          90000
evaluation/num paths total             90
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.0793
evaluation/Rewards Std                  2.0152
evaluation/Rewards Max                 -0.000665405
evaluation/Rewards Min                 -6.93516
evaluation/Returns Mean             -2079.3
evaluation/Returns Std               1945.34
evaluation/Returns Max               -435.86
evaluation/Returns Min              -4480.7
evaluation/Actions Mean                -0.548451
evaluation/Actions Std                  1.07172
evaluation/Actions Max                  3.43335
evaluation/Actions Min                 -4.26432
evaluation/Num Paths                    5
evaluation/Average Returns          -2079.3
time/data storing (s)                   0.000598597
time/evaluation sampling (s)            2.80693
time/exploration real sampling (s)      0.0658306
time/exploration sim sampling (s)       6.422e-06
time/logging (s)                        0.013118
time/saving (s)                         0.0144033
time/training (s)                      22.0941
time/epoch (s)                         24.995
time/total (s)                        435.206
Epoch                                  17
----------------------------------  ---------------
2020-02-21 08:04:35.174918 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 18 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        0.386555
trainer/QF2 Loss                        0.370681
trainer/Policy Loss                    11.2588
trainer/Q1 Predictions Mean           -11.6328
trainer/Q1 Predictions Std             12.8377
trainer/Q1 Predictions Max             -2.11487
trainer/Q1 Predictions Min            -42.6616
trainer/Q2 Predictions Mean           -11.6236
trainer/Q2 Predictions Std             12.8675
trainer/Q2 Predictions Max             -2.15987
trainer/Q2 Predictions Min            -42.5083
trainer/Q Targets Mean                -11.4695
trainer/Q Targets Std                  12.7506
trainer/Q Targets Max                  -2.12902
trainer/Q Targets Min                 -43.033
trainer/Log Pis Mean                    1.90912
trainer/Log Pis Std                     2.11154
trainer/Log Pis Max                     6.66162
trainer/Log Pis Min                    -3.66575
trainer/Policy mu Mean                 -1.18338
trainer/Policy mu Std                   0.743988
trainer/Policy mu Max                   0.794481
trainer/Policy mu Min                  -2.76308
trainer/Policy log std Mean            -0.948613
trainer/Policy log std Std              0.303716
trainer/Policy log std Max             -0.390416
trainer/Policy log std Min             -1.79297
trainer/Alpha                           0.0707439
trainer/Alpha Loss                     -0.2407
exploration/num steps total          1900
exploration/num paths total            19
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.68203
exploration/Rewards Std                 0.702251
exploration/Rewards Max                -0.00435843
exploration/Rewards Min                -3.72497
exploration/Returns Mean              -68.203
exploration/Returns Std                 0
exploration/Returns Max               -68.203
exploration/Returns Min               -68.203
exploration/Actions Mean               -0.545509
exploration/Actions Std                 1.05567
exploration/Actions Max                 3.36824
exploration/Actions Min                -3.47581
exploration/Num Paths                   1
exploration/Average Returns           -68.203
evaluation/num steps total          95000
evaluation/num paths total             95
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.06008
evaluation/Rewards Std                  1.99836
evaluation/Rewards Max                 -0.000586244
evaluation/Rewards Min                 -6.2249
evaluation/Returns Mean             -2060.08
evaluation/Returns Std               1927.71
evaluation/Returns Max               -434.843
evaluation/Returns Min              -4422.99
evaluation/Actions Mean                -0.563581
evaluation/Actions Std                  1.09153
evaluation/Actions Max                  3.5478
evaluation/Actions Min                 -4.70478
evaluation/Num Paths                    5
evaluation/Average Returns          -2060.08
time/data storing (s)                   0.000630925
time/evaluation sampling (s)            2.887
time/exploration real sampling (s)      0.066689
time/exploration sim sampling (s)       6.578e-06
time/logging (s)                        0.014259
time/saving (s)                         0.0109855
time/training (s)                      21.5437
time/epoch (s)                         24.5232
time/total (s)                        459.736
Epoch                                  18
----------------------------------  ---------------
2020-02-21 08:04:59.512926 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 19 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.494515
trainer/QF2 Loss                         0.460865
trainer/Policy Loss                     10.5943
trainer/Q1 Predictions Mean            -10.9099
trainer/Q1 Predictions Std              12.5539
trainer/Q1 Predictions Max              -2.37681
trainer/Q1 Predictions Min             -43.1234
trainer/Q2 Predictions Mean            -10.8975
trainer/Q2 Predictions Std              12.5768
trainer/Q2 Predictions Max              -2.26427
trainer/Q2 Predictions Min             -43.4075
trainer/Q Targets Mean                 -11.0621
trainer/Q Targets Std                   12.6083
trainer/Q Targets Max                   -2.39247
trainer/Q Targets Min                  -43.2608
trainer/Log Pis Mean                     1.90652
trainer/Log Pis Std                      2.11574
trainer/Log Pis Max                      7.24176
trainer/Log Pis Min                     -5.68579
trainer/Policy mu Mean                  -1.2352
trainer/Policy mu Std                    0.71719
trainer/Policy mu Max                    0.811167
trainer/Policy mu Min                   -2.8597
trainer/Policy log std Mean             -0.906429
trainer/Policy log std Std               0.307555
trainer/Policy log std Max              -0.283076
trainer/Policy log std Min              -1.96856
trainer/Alpha                            0.0705919
trainer/Alpha Loss                      -0.247801
exploration/num steps total           2000
exploration/num paths total             20
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.717551
exploration/Rewards Std                  0.735066
exploration/Rewards Max                 -0.0199689
exploration/Rewards Min                 -3.27132
exploration/Returns Mean               -71.7551
exploration/Returns Std                  0
exploration/Returns Max                -71.7551
exploration/Returns Min                -71.7551
exploration/Actions Mean                -0.521441
exploration/Actions Std                  1.09375
exploration/Actions Max                  2.1811
exploration/Actions Min                 -3.32651
exploration/Num Paths                    1
exploration/Average Returns            -71.7551
evaluation/num steps total          100000
evaluation/num paths total             100
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.08324
evaluation/Rewards Std                   2.00164
evaluation/Rewards Max                  -0.00127475
evaluation/Rewards Min                  -7.13795
evaluation/Returns Mean              -2083.24
evaluation/Returns Std                1916.7
evaluation/Returns Max                -463.082
evaluation/Returns Min               -4451.9
evaluation/Actions Mean                 -0.566611
evaluation/Actions Std                   1.0934
evaluation/Actions Max                   4.01977
evaluation/Actions Min                  -4.65044
evaluation/Num Paths                     5
evaluation/Average Returns           -2083.24
time/data storing (s)                    0.000588304
time/evaluation sampling (s)             2.84242
time/exploration real sampling (s)       0.0662768
time/exploration sim sampling (s)        6.52e-06
time/logging (s)                         0.0136352
time/saving (s)                          0.013823
time/training (s)                       21.3937
time/epoch (s)                          24.3304
time/total (s)                         484.072
Epoch                                   19
----------------------------------  ----------------
2020-02-21 08:05:24.017215 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 20 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.689234
trainer/QF2 Loss                         0.623367
trainer/Policy Loss                     12.6645
trainer/Q1 Predictions Mean            -12.9732
trainer/Q1 Predictions Std              13.8597
trainer/Q1 Predictions Max              -2.6076
trainer/Q1 Predictions Min             -49.8775
trainer/Q2 Predictions Mean            -12.9419
trainer/Q2 Predictions Std              13.834
trainer/Q2 Predictions Max              -2.64532
trainer/Q2 Predictions Min             -50.1613
trainer/Q Targets Mean                 -12.9142
trainer/Q Targets Std                   13.8985
trainer/Q Targets Max                   -2.67506
trainer/Q Targets Min                  -50.7754
trainer/Log Pis Mean                     2.32123
trainer/Log Pis Std                      2.13419
trainer/Log Pis Max                      7.17702
trainer/Log Pis Min                     -3.34452
trainer/Policy mu Mean                  -1.32796
trainer/Policy mu Std                    0.779489
trainer/Policy mu Max                    0.676528
trainer/Policy mu Min                   -3.16612
trainer/Policy log std Mean             -0.914149
trainer/Policy log std Std               0.327118
trainer/Policy log std Max              -0.358695
trainer/Policy log std Min              -1.94368
trainer/Alpha                            0.0693979
trainer/Alpha Loss                       0.857027
exploration/num steps total           2100
exploration/num paths total             21
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.582136
exploration/Rewards Std                  0.664269
exploration/Rewards Max                 -0.00396545
exploration/Rewards Min                 -2.96927
exploration/Returns Mean               -58.2136
exploration/Returns Std                  0
exploration/Returns Max                -58.2136
exploration/Returns Min                -58.2136
exploration/Actions Mean                -0.617211
exploration/Actions Std                  1.06653
exploration/Actions Max                  2.30474
exploration/Actions Min                 -3.81707
exploration/Num Paths                    1
exploration/Average Returns            -58.2136
evaluation/num steps total          105000
evaluation/num paths total             105
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.33526
evaluation/Rewards Std                   1.69674
evaluation/Rewards Max                  -0.000530795
evaluation/Rewards Min                  -7.74649
evaluation/Returns Mean              -1335.26
evaluation/Returns Std                1528.58
evaluation/Returns Max                -417.682
evaluation/Returns Min               -4381.41
evaluation/Actions Mean                 -0.603298
evaluation/Actions Std                   1.0503
evaluation/Actions Max                   4.19275
evaluation/Actions Min                  -4.57113
evaluation/Num Paths                     5
evaluation/Average Returns           -1335.26
time/data storing (s)                    0.000630437
time/evaluation sampling (s)             2.74724
time/exploration real sampling (s)       0.0659076
time/exploration sim sampling (s)        6.404e-06
time/logging (s)                         0.0132673
time/saving (s)                          0.0134337
time/training (s)                       21.6577
time/epoch (s)                          24.4982
time/total (s)                         508.575
Epoch                                   20
----------------------------------  ----------------
2020-02-21 08:05:47.376713 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 21 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.34703
trainer/QF2 Loss                         0.320004
trainer/Policy Loss                     10.6292
trainer/Q1 Predictions Mean            -11.0129
trainer/Q1 Predictions Std              12.3247
trainer/Q1 Predictions Max              -2.9282
trainer/Q1 Predictions Min             -42.4994
trainer/Q2 Predictions Mean            -10.9073
trainer/Q2 Predictions Std              12.2584
trainer/Q2 Predictions Max              -2.85205
trainer/Q2 Predictions Min             -42.2398
trainer/Q Targets Mean                 -10.9012
trainer/Q Targets Std                   12.3246
trainer/Q Targets Max                   -2.79092
trainer/Q Targets Min                  -42.1693
trainer/Log Pis Mean                     1.74687
trainer/Log Pis Std                      2.10791
trainer/Log Pis Max                      6.99299
trainer/Log Pis Min                     -3.18434
trainer/Policy mu Mean                  -1.20525
trainer/Policy mu Std                    0.736323
trainer/Policy mu Max                    0.77539
trainer/Policy mu Min                   -2.99801
trainer/Policy log std Mean             -0.912839
trainer/Policy log std Std               0.302205
trainer/Policy log std Max              -0.32197
trainer/Policy log std Min              -2.12987
trainer/Alpha                            0.0685654
trainer/Alpha Loss                      -0.678381
exploration/num steps total           2200
exploration/num paths total             22
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.837034
exploration/Rewards Std                  1.4043
exploration/Rewards Max                 -0.0106403
exploration/Rewards Min                 -6.87175
exploration/Returns Mean               -83.7034
exploration/Returns Std                  0
exploration/Returns Max                -83.7034
exploration/Returns Min                -83.7034
exploration/Actions Mean                -0.638471
exploration/Actions Std                  1.01749
exploration/Actions Max                  1.99152
exploration/Actions Min                 -3.08476
exploration/Num Paths                    1
exploration/Average Returns            -83.7034
evaluation/num steps total          110000
evaluation/num paths total             110
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.07322
evaluation/Rewards Std                   1.99849
evaluation/Rewards Max                  -0.000225673
evaluation/Rewards Min                  -6.36525
evaluation/Returns Mean              -2073.22
evaluation/Returns Std                1919.22
evaluation/Returns Max                -466.386
evaluation/Returns Min               -4438.81
evaluation/Actions Mean                 -0.576583
evaluation/Actions Std                   1.10323
evaluation/Actions Max                   3.93306
evaluation/Actions Min                  -4.51523
evaluation/Num Paths                     5
evaluation/Average Returns           -2073.22
time/data storing (s)                    0.000604211
time/evaluation sampling (s)             2.81422
time/exploration real sampling (s)       0.0655802
time/exploration sim sampling (s)        6.866e-06
time/logging (s)                         0.0153473
time/saving (s)                          0.0117776
time/training (s)                       20.4481
time/epoch (s)                          23.3556
time/total (s)                         531.935
Epoch                                   21
----------------------------------  ----------------
2020-02-21 08:06:11.884626 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 22 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.501403
trainer/QF2 Loss                         0.49413
trainer/Policy Loss                     11.2972
trainer/Q1 Predictions Mean            -11.8519
trainer/Q1 Predictions Std              12.4996
trainer/Q1 Predictions Max              -2.87302
trainer/Q1 Predictions Min             -41.7573
trainer/Q2 Predictions Mean            -11.8379
trainer/Q2 Predictions Std              12.4982
trainer/Q2 Predictions Max              -2.53009
trainer/Q2 Predictions Min             -41.4337
trainer/Q Targets Mean                 -12.0388
trainer/Q Targets Std                   12.5632
trainer/Q Targets Max                   -3.06443
trainer/Q Targets Min                  -42.4129
trainer/Log Pis Mean                     2.06351
trainer/Log Pis Std                      2.07453
trainer/Log Pis Max                      7.65639
trainer/Log Pis Min                     -3.80488
trainer/Policy mu Mean                  -1.25335
trainer/Policy mu Std                    0.768438
trainer/Policy mu Max                    0.872462
trainer/Policy mu Min                   -2.99031
trainer/Policy log std Mean             -0.903174
trainer/Policy log std Std               0.298884
trainer/Policy log std Max              -0.316945
trainer/Policy log std Min              -2.17168
trainer/Alpha                            0.067345
trainer/Alpha Loss                       0.17134
exploration/num steps total           2300
exploration/num paths total             23
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.44646
exploration/Rewards Std                  0.519969
exploration/Rewards Max                 -0.00696587
exploration/Rewards Min                 -2.13345
exploration/Returns Mean               -44.646
exploration/Returns Std                  0
exploration/Returns Max                -44.646
exploration/Returns Min                -44.646
exploration/Actions Mean                -0.592037
exploration/Actions Std                  1.04189
exploration/Actions Max                  2.02556
exploration/Actions Min                 -3.92283
exploration/Num Paths                    1
exploration/Average Returns            -44.646
evaluation/num steps total          115000
evaluation/num paths total             115
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.36087
evaluation/Rewards Std                   1.71193
evaluation/Rewards Max                  -0.000198567
evaluation/Rewards Min                  -6.66326
evaluation/Returns Mean              -1360.87
evaluation/Returns Std                1538.52
evaluation/Returns Max                -431.411
evaluation/Returns Min               -4429.2
evaluation/Actions Mean                 -0.60525
evaluation/Actions Std                   1.05463
evaluation/Actions Max                   3.68211
evaluation/Actions Min                  -4.34706
evaluation/Num Paths                     5
evaluation/Average Returns           -1360.87
time/data storing (s)                    0.000684804
time/evaluation sampling (s)             2.80427
time/exploration real sampling (s)       0.0662817
time/exploration sim sampling (s)        7.227e-06
time/logging (s)                         0.0147837
time/saving (s)                          0.0163135
time/training (s)                       21.598
time/epoch (s)                          24.5004
time/total (s)                         556.441
Epoch                                   22
----------------------------------  ----------------
2020-02-21 08:06:36.812451 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 23 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.503016
trainer/QF2 Loss                         0.424693
trainer/Policy Loss                     11.5254
trainer/Q1 Predictions Mean            -11.4143
trainer/Q1 Predictions Std              12.4258
trainer/Q1 Predictions Max              -3.16438
trainer/Q1 Predictions Min             -43.5321
trainer/Q2 Predictions Mean            -11.4777
trainer/Q2 Predictions Std              12.4744
trainer/Q2 Predictions Max              -3.27485
trainer/Q2 Predictions Min             -44.5907
trainer/Q Targets Mean                 -11.3915
trainer/Q Targets Std                   12.4308
trainer/Q Targets Max                   -3.24564
trainer/Q Targets Min                  -43.4067
trainer/Log Pis Mean                     2.09134
trainer/Log Pis Std                      2.03967
trainer/Log Pis Max                      7.28367
trainer/Log Pis Min                     -3.764
trainer/Policy mu Mean                  -1.26579
trainer/Policy mu Std                    0.768831
trainer/Policy mu Max                    0.791892
trainer/Policy mu Min                   -2.86703
trainer/Policy log std Mean             -0.962609
trainer/Policy log std Std               0.28801
trainer/Policy log std Max              -0.442514
trainer/Policy log std Min              -2.10754
trainer/Alpha                            0.0653983
trainer/Alpha Loss                       0.24912
exploration/num steps total           2400
exploration/num paths total             24
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.603908
exploration/Rewards Std                  0.671599
exploration/Rewards Max                 -0.00657322
exploration/Rewards Min                 -3.64936
exploration/Returns Mean               -60.3908
exploration/Returns Std                  0
exploration/Returns Max                -60.3908
exploration/Returns Min                -60.3908
exploration/Actions Mean                -0.647284
exploration/Actions Std                  1.05352
exploration/Actions Max                  1.97796
exploration/Actions Min                 -3.23958
exploration/Num Paths                    1
exploration/Average Returns            -60.3908
evaluation/num steps total          120000
evaluation/num paths total             120
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.27909
evaluation/Rewards Std                   1.67975
evaluation/Rewards Max                  -0.00067242
evaluation/Rewards Min                  -7.09795
evaluation/Returns Mean              -1279.09
evaluation/Returns Std                1571.18
evaluation/Returns Max                -455.463
evaluation/Returns Min               -4420.63
evaluation/Actions Mean                 -0.601839
evaluation/Actions Std                   1.05659
evaluation/Actions Max                   3.5763
evaluation/Actions Min                  -4.41783
evaluation/Num Paths                     5
evaluation/Average Returns           -1279.09
time/data storing (s)                    0.000618287
time/evaluation sampling (s)             2.96175
time/exploration real sampling (s)       0.0670473
time/exploration sim sampling (s)        7.121e-06
time/logging (s)                         0.0132027
time/saving (s)                          0.013792
time/training (s)                       21.8635
time/epoch (s)                          24.9199
time/total (s)                         581.366
Epoch                                   23
----------------------------------  ----------------
2020-02-21 08:07:01.156284 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 24 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.455261
trainer/QF2 Loss                         0.402933
trainer/Policy Loss                     10.5825
trainer/Q1 Predictions Mean            -10.6829
trainer/Q1 Predictions Std              11.3489
trainer/Q1 Predictions Max              -3.23174
trainer/Q1 Predictions Min             -43.5563
trainer/Q2 Predictions Mean            -10.6681
trainer/Q2 Predictions Std              11.3892
trainer/Q2 Predictions Max              -3.25681
trainer/Q2 Predictions Min             -44.0921
trainer/Q Targets Mean                 -10.9093
trainer/Q Targets Std                   11.4306
trainer/Q Targets Max                   -3.43833
trainer/Q Targets Min                  -43.5345
trainer/Log Pis Mean                     1.93431
trainer/Log Pis Std                      2.11248
trainer/Log Pis Max                      7.50836
trainer/Log Pis Min                     -2.80889
trainer/Policy mu Mean                  -1.16807
trainer/Policy mu Std                    0.781127
trainer/Policy mu Max                    0.947698
trainer/Policy mu Min                   -2.8904
trainer/Policy log std Mean             -0.916939
trainer/Policy log std Std               0.315922
trainer/Policy log std Max              -0.25825
trainer/Policy log std Min              -2.10635
trainer/Alpha                            0.0637255
trainer/Alpha Loss                      -0.180846
exploration/num steps total           2500
exploration/num paths total             25
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.508342
exploration/Rewards Std                  0.489809
exploration/Rewards Max                 -0.0207876
exploration/Rewards Min                 -2.00803
exploration/Returns Mean               -50.8342
exploration/Returns Std                  0
exploration/Returns Max                -50.8342
exploration/Returns Min                -50.8342
exploration/Actions Mean                -0.570487
exploration/Actions Std                  1.05021
exploration/Actions Max                  2.3404
exploration/Actions Min                 -3.44168
exploration/Num Paths                    1
exploration/Average Returns            -50.8342
evaluation/num steps total          125000
evaluation/num paths total             125
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.8597
evaluation/Rewards Std                   1.97663
evaluation/Rewards Max                  -0.00335056
evaluation/Rewards Min                  -7.80171
evaluation/Returns Mean              -2859.7
evaluation/Returns Std                1903.08
evaluation/Returns Max                -513.67
evaluation/Returns Min               -4430.9
evaluation/Actions Mean                 -0.551085
evaluation/Actions Std                   1.14659
evaluation/Actions Max                   3.89089
evaluation/Actions Min                  -4.42197
evaluation/Num Paths                     5
evaluation/Average Returns           -2859.7
time/data storing (s)                    0.000614813
time/evaluation sampling (s)             2.9134
time/exploration real sampling (s)       0.07425
time/exploration sim sampling (s)        6.317e-06
time/logging (s)                         0.0135601
time/saving (s)                          0.0142886
time/training (s)                       21.3219
time/epoch (s)                          24.3381
time/total (s)                         605.709
Epoch                                   24
----------------------------------  ----------------
2020-02-21 08:07:29.534155 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 25 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.471802
trainer/QF2 Loss                         0.397757
trainer/Policy Loss                     11.8582
trainer/Q1 Predictions Mean            -12.0007
trainer/Q1 Predictions Std              11.8754
trainer/Q1 Predictions Max              -3.59348
trainer/Q1 Predictions Min             -42.8903
trainer/Q2 Predictions Mean            -12.102
trainer/Q2 Predictions Std              11.9602
trainer/Q2 Predictions Max              -3.67544
trainer/Q2 Predictions Min             -42.9749
trainer/Q Targets Mean                 -11.9834
trainer/Q Targets Std                   11.9117
trainer/Q Targets Max                   -3.66545
trainer/Q Targets Min                  -42.2926
trainer/Log Pis Mean                     2.01103
trainer/Log Pis Std                      2.29587
trainer/Log Pis Max                      7.40691
trainer/Log Pis Min                     -5.54788
trainer/Policy mu Mean                  -1.23463
trainer/Policy mu Std                    0.810688
trainer/Policy mu Max                    0.951113
trainer/Policy mu Min                   -2.95958
trainer/Policy log std Mean             -0.947341
trainer/Policy log std Std               0.320111
trainer/Policy log std Max              -0.393876
trainer/Policy log std Min              -2.20693
trainer/Alpha                            0.0612869
trainer/Alpha Loss                       0.0307954
exploration/num steps total           2600
exploration/num paths total             26
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.953101
exploration/Rewards Std                  1.22472
exploration/Rewards Max                 -0.00534443
exploration/Rewards Min                 -5.58102
exploration/Returns Mean               -95.3101
exploration/Returns Std                  0
exploration/Returns Max                -95.3101
exploration/Returns Min                -95.3101
exploration/Actions Mean                -0.480103
exploration/Actions Std                  1.03894
exploration/Actions Max                  2.3535
exploration/Actions Min                 -3.66668
exploration/Num Paths                    1
exploration/Average Returns            -95.3101
evaluation/num steps total          130000
evaluation/num paths total             130
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.06345
evaluation/Rewards Std                   1.99391
evaluation/Rewards Max                  -0.00026537
evaluation/Rewards Min                  -7.73601
evaluation/Returns Mean              -2063.45
evaluation/Returns Std                1919.29
evaluation/Returns Max                -409.498
evaluation/Returns Min               -4420.07
evaluation/Actions Mean                 -0.574097
evaluation/Actions Std                   1.09967
evaluation/Actions Max                   3.49655
evaluation/Actions Min                  -4.32323
evaluation/Num Paths                     5
evaluation/Average Returns           -2063.45
time/data storing (s)                    0.000606658
time/evaluation sampling (s)             3.05681
time/exploration real sampling (s)       0.0666692
time/exploration sim sampling (s)        6.515e-06
time/logging (s)                         0.0158838
time/saving (s)                          0.0154497
time/training (s)                       25.2187
time/epoch (s)                          28.3741
time/total (s)                         634.088
Epoch                                   25
----------------------------------  ----------------
2020-02-21 08:07:54.491111 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 26 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.511167
trainer/QF2 Loss                         0.44919
trainer/Policy Loss                     11.2481
trainer/Q1 Predictions Mean            -11.1046
trainer/Q1 Predictions Std              11.4674
trainer/Q1 Predictions Max              -3.91804
trainer/Q1 Predictions Min             -41.7604
trainer/Q2 Predictions Mean            -11.0005
trainer/Q2 Predictions Std              11.4579
trainer/Q2 Predictions Max              -3.77386
trainer/Q2 Predictions Min             -41.4718
trainer/Q Targets Mean                 -10.9916
trainer/Q Targets Std                   11.4705
trainer/Q Targets Max                   -3.82136
trainer/Q Targets Min                  -43.3918
trainer/Log Pis Mean                     2.08124
trainer/Log Pis Std                      2.10058
trainer/Log Pis Max                      7.22202
trainer/Log Pis Min                     -2.81497
trainer/Policy mu Mean                  -1.24434
trainer/Policy mu Std                    0.742201
trainer/Policy mu Max                    0.89845
trainer/Policy mu Min                   -2.91279
trainer/Policy log std Mean             -0.902682
trainer/Policy log std Std               0.321188
trainer/Policy log std Max              -0.244624
trainer/Policy log std Min              -2.12845
trainer/Alpha                            0.0593871
trainer/Alpha Loss                       0.229397
exploration/num steps total           2700
exploration/num paths total             27
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.799642
exploration/Rewards Std                  1.2225
exploration/Rewards Max                 -0.00635025
exploration/Rewards Min                 -7.30147
exploration/Returns Mean               -79.9642
exploration/Returns Std                  0
exploration/Returns Max                -79.9642
exploration/Returns Min                -79.9642
exploration/Actions Mean                -0.65796
exploration/Actions Std                  1.04554
exploration/Actions Max                  1.98969
exploration/Actions Min                 -3.42553
exploration/Num Paths                    1
exploration/Average Returns            -79.9642
evaluation/num steps total          135000
evaluation/num paths total             135
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.0901
evaluation/Rewards Std                   1.98399
evaluation/Rewards Max                  -0.00129612
evaluation/Rewards Min                  -6.74559
evaluation/Returns Mean              -2090.1
evaluation/Returns Std                1885.11
evaluation/Returns Max                -446.068
evaluation/Returns Min               -4401.08
evaluation/Actions Mean                 -0.577638
evaluation/Actions Std                   1.077
evaluation/Actions Max                   3.48733
evaluation/Actions Min                  -4.87824
evaluation/Num Paths                     5
evaluation/Average Returns           -2090.1
time/data storing (s)                    0.000894147
time/evaluation sampling (s)             3.13513
time/exploration real sampling (s)       0.0811982
time/exploration sim sampling (s)        6.776e-06
time/logging (s)                         0.0140749
time/saving (s)                          0.0155358
time/training (s)                       21.7017
time/epoch (s)                          24.9485
time/total (s)                         659.042
Epoch                                   26
----------------------------------  ----------------
2020-02-21 08:08:20.280220 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 27 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.416514
trainer/QF2 Loss                         0.335151
trainer/Policy Loss                     11.6997
trainer/Q1 Predictions Mean            -11.4408
trainer/Q1 Predictions Std              11.828
trainer/Q1 Predictions Max              -4.0318
trainer/Q1 Predictions Min             -52.4963
trainer/Q2 Predictions Mean            -11.3882
trainer/Q2 Predictions Std              11.8202
trainer/Q2 Predictions Max              -3.97969
trainer/Q2 Predictions Min             -52.0838
trainer/Q Targets Mean                 -11.3643
trainer/Q Targets Std                   11.8042
trainer/Q Targets Max                   -4.01226
trainer/Q Targets Min                  -52.0741
trainer/Log Pis Mean                     2.21049
trainer/Log Pis Std                      2.18151
trainer/Log Pis Max                      7.16244
trainer/Log Pis Min                     -3.74319
trainer/Policy mu Mean                  -1.27985
trainer/Policy mu Std                    0.746684
trainer/Policy mu Max                    0.933156
trainer/Policy mu Min                   -2.95052
trainer/Policy log std Mean             -0.888098
trainer/Policy log std Std               0.315376
trainer/Policy log std Max              -0.324776
trainer/Policy log std Min              -2.11193
trainer/Alpha                            0.0587542
trainer/Alpha Loss                       0.596602
exploration/num steps total           2800
exploration/num paths total             28
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.727064
exploration/Rewards Std                  0.771719
exploration/Rewards Max                 -0.00561563
exploration/Rewards Min                 -3.59154
exploration/Returns Mean               -72.7064
exploration/Returns Std                  0
exploration/Returns Max                -72.7064
exploration/Returns Min                -72.7064
exploration/Actions Mean                -0.623203
exploration/Actions Std                  1.08369
exploration/Actions Max                  2.29998
exploration/Actions Min                 -3.6228
exploration/Num Paths                    1
exploration/Average Returns            -72.7064
evaluation/num steps total          140000
evaluation/num paths total             140
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -3.59634
evaluation/Rewards Std                   1.63665
evaluation/Rewards Max                  -0.00266878
evaluation/Rewards Min                  -6.23284
evaluation/Returns Mean              -3596.34
evaluation/Returns Std                1595.63
evaluation/Returns Max                -405.176
evaluation/Returns Min               -4413.45
evaluation/Actions Mean                 -0.521645
evaluation/Actions Std                   1.15311
evaluation/Actions Max                   4.20264
evaluation/Actions Min                  -4.44307
evaluation/Num Paths                     5
evaluation/Average Returns           -3596.34
time/data storing (s)                    0.000609978
time/evaluation sampling (s)             3.08547
time/exploration real sampling (s)       0.0655016
time/exploration sim sampling (s)        6.451e-06
time/logging (s)                         0.0138429
time/saving (s)                          0.0151833
time/training (s)                       22.6016
time/epoch (s)                          25.7822
time/total (s)                         684.83
Epoch                                   27
----------------------------------  ----------------
2020-02-21 08:08:44.795896 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 28 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.308298
trainer/QF2 Loss                         0.290853
trainer/Policy Loss                     11.2656
trainer/Q1 Predictions Mean            -10.9718
trainer/Q1 Predictions Std              11.0063
trainer/Q1 Predictions Max              -4.21618
trainer/Q1 Predictions Min             -47.394
trainer/Q2 Predictions Mean            -11.0418
trainer/Q2 Predictions Std              11.0428
trainer/Q2 Predictions Max              -4.31749
trainer/Q2 Predictions Min             -47.5574
trainer/Q Targets Mean                 -10.9056
trainer/Q Targets Std                   10.9558
trainer/Q Targets Max                   -4.15885
trainer/Q Targets Min                  -46.9431
trainer/Log Pis Mean                     2.20853
trainer/Log Pis Std                      2.12887
trainer/Log Pis Max                      7.29992
trainer/Log Pis Min                     -3.70806
trainer/Policy mu Mean                  -1.26239
trainer/Policy mu Std                    0.751472
trainer/Policy mu Max                    0.819231
trainer/Policy mu Min                   -3.00424
trainer/Policy log std Mean             -0.956999
trainer/Policy log std Std               0.31119
trainer/Policy log std Max              -0.339388
trainer/Policy log std Min              -2.42301
trainer/Alpha                            0.0567526
trainer/Alpha Loss                       0.598305
exploration/num steps total           2900
exploration/num paths total             29
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.718143
exploration/Rewards Std                  1.13877
exploration/Rewards Max                 -0.00653193
exploration/Rewards Min                 -7.03627
exploration/Returns Mean               -71.8143
exploration/Returns Std                  0
exploration/Returns Max                -71.8143
exploration/Returns Min                -71.8143
exploration/Actions Mean                -0.607293
exploration/Actions Std                  1.02986
exploration/Actions Max                  2.681
exploration/Actions Min                 -3.46895
exploration/Num Paths                    1
exploration/Average Returns            -71.8143
evaluation/num steps total          145000
evaluation/num paths total             145
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.18024
evaluation/Rewards Std                   1.98339
evaluation/Rewards Max                  -0.00028154
evaluation/Rewards Min                  -6.66569
evaluation/Returns Mean              -2180.24
evaluation/Returns Std                1815.12
evaluation/Returns Max                -651.935
evaluation/Returns Min               -4405.3
evaluation/Actions Mean                 -0.600497
evaluation/Actions Std                   1.08124
evaluation/Actions Max                   3.47909
evaluation/Actions Min                  -5.09926
evaluation/Num Paths                     5
evaluation/Average Returns           -2180.24
time/data storing (s)                    0.000621515
time/evaluation sampling (s)             3.18218
time/exploration real sampling (s)       0.0640605
time/exploration sim sampling (s)        6.444e-06
time/logging (s)                         0.0135675
time/saving (s)                          0.0147007
time/training (s)                       21.2343
time/epoch (s)                          24.5094
time/total (s)                         709.344
Epoch                                   28
----------------------------------  ----------------
2020-02-21 08:09:09.439493 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 29 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.541706
trainer/QF2 Loss                         0.462592
trainer/Policy Loss                     12.1099
trainer/Q1 Predictions Mean            -11.9156
trainer/Q1 Predictions Std              11.8181
trainer/Q1 Predictions Max              -4.41055
trainer/Q1 Predictions Min             -45.9929
trainer/Q2 Predictions Mean            -11.921
trainer/Q2 Predictions Std              11.8533
trainer/Q2 Predictions Max              -4.43685
trainer/Q2 Predictions Min             -46.4573
trainer/Q Targets Mean                 -11.9022
trainer/Q Targets Std                   11.9587
trainer/Q Targets Max                   -4.30929
trainer/Q Targets Min                  -47.1079
trainer/Log Pis Mean                     2.20584
trainer/Log Pis Std                      2.16266
trainer/Log Pis Max                      7.29671
trainer/Log Pis Min                     -4.54952
trainer/Policy mu Mean                  -1.26305
trainer/Policy mu Std                    0.769711
trainer/Policy mu Max                    0.891041
trainer/Policy mu Min                   -3.01965
trainer/Policy log std Mean             -0.940223
trainer/Policy log std Std               0.309032
trainer/Policy log std Max              -0.379425
trainer/Policy log std Min              -2.4687
trainer/Alpha                            0.0550245
trainer/Alpha Loss                       0.596925
exploration/num steps total           3000
exploration/num paths total             30
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.815695
exploration/Rewards Std                  1.39822
exploration/Rewards Max                 -0.00199538
exploration/Rewards Min                 -6.25076
exploration/Returns Mean               -81.5695
exploration/Returns Std                  0
exploration/Returns Max                -81.5695
exploration/Returns Min                -81.5695
exploration/Actions Mean                -0.631674
exploration/Actions Std                  1.04506
exploration/Actions Max                  2.42267
exploration/Actions Min                 -3.38802
exploration/Num Paths                    1
exploration/Average Returns            -81.5695
evaluation/num steps total          150000
evaluation/num paths total             150
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.486184
evaluation/Rewards Std                   0.667827
evaluation/Rewards Max                  -0.00114369
evaluation/Rewards Min                  -6.70782
evaluation/Returns Mean               -486.184
evaluation/Returns Std                  59.5518
evaluation/Returns Max                -423.931
evaluation/Returns Min                -571.026
evaluation/Actions Mean                 -0.643942
evaluation/Actions Std                   1.0126
evaluation/Actions Max                   2.9786
evaluation/Actions Min                  -4.62838
evaluation/Num Paths                     5
evaluation/Average Returns            -486.184
time/data storing (s)                    0.000606974
time/evaluation sampling (s)             2.76443
time/exploration real sampling (s)       0.0671578
time/exploration sim sampling (s)        7.409e-06
time/logging (s)                         0.0133407
time/saving (s)                          0.0150206
time/training (s)                       21.7766
time/epoch (s)                          24.6371
time/total (s)                         733.986
Epoch                                   29
----------------------------------  ----------------
2020-02-21 08:09:34.925183 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 30 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.419804
trainer/QF2 Loss                         0.333373
trainer/Policy Loss                     13.2184
trainer/Q1 Predictions Mean            -13.276
trainer/Q1 Predictions Std              12.2272
trainer/Q1 Predictions Max              -4.63464
trainer/Q1 Predictions Min             -43.354
trainer/Q2 Predictions Mean            -13.1983
trainer/Q2 Predictions Std              12.2443
trainer/Q2 Predictions Max              -4.54081
trainer/Q2 Predictions Min             -42.8129
trainer/Q Targets Mean                 -13.0673
trainer/Q Targets Std                   12.0536
trainer/Q Targets Max                   -4.51867
trainer/Q Targets Min                  -42.5163
trainer/Log Pis Mean                     2.13803
trainer/Log Pis Std                      2.40285
trainer/Log Pis Max                      7.24406
trainer/Log Pis Min                     -8.79261
trainer/Policy mu Mean                  -1.24711
trainer/Policy mu Std                    0.805686
trainer/Policy mu Max                    1.00611
trainer/Policy mu Min                   -2.97273
trainer/Policy log std Mean             -0.944298
trainer/Policy log std Std               0.337072
trainer/Policy log std Max              -0.393795
trainer/Policy log std Min              -2.53918
trainer/Alpha                            0.0541763
trainer/Alpha Loss                       0.402416
exploration/num steps total           3100
exploration/num paths total             31
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.22856
exploration/Rewards Std                  1.64938
exploration/Rewards Max                 -0.0111354
exploration/Rewards Min                 -7.85864
exploration/Returns Mean              -122.856
exploration/Returns Std                  0
exploration/Returns Max               -122.856
exploration/Returns Min               -122.856
exploration/Actions Mean                -0.468399
exploration/Actions Std                  0.974876
exploration/Actions Max                  2.57203
exploration/Actions Min                 -3.30566
exploration/Num Paths                    1
exploration/Average Returns           -122.856
evaluation/num steps total          155000
evaluation/num paths total             155
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.04364
evaluation/Rewards Std                   2.01057
evaluation/Rewards Max                  -0.00100023
evaluation/Rewards Min                  -6.92329
evaluation/Returns Mean              -2043.64
evaluation/Returns Std                1962.91
evaluation/Returns Max                -421.573
evaluation/Returns Min               -4451.15
evaluation/Actions Mean                 -0.55881
evaluation/Actions Std                   1.09169
evaluation/Actions Max                   4.22465
evaluation/Actions Min                  -4.31702
evaluation/Num Paths                     5
evaluation/Average Returns           -2043.64
time/data storing (s)                    0.000626248
time/evaluation sampling (s)             2.84995
time/exploration real sampling (s)       0.0664671
time/exploration sim sampling (s)        6.695e-06
time/logging (s)                         0.0134964
time/saving (s)                          0.0149812
time/training (s)                       22.534
time/epoch (s)                          25.4795
time/total (s)                         759.47
Epoch                                   30
----------------------------------  ----------------
2020-02-21 08:09:59.429088 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 31 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.339956
trainer/QF2 Loss                         0.266231
trainer/Policy Loss                     11.6037
trainer/Q1 Predictions Mean            -11.3348
trainer/Q1 Predictions Std              10.4715
trainer/Q1 Predictions Max              -4.63796
trainer/Q1 Predictions Min             -42.6027
trainer/Q2 Predictions Mean            -11.4195
trainer/Q2 Predictions Std              10.5377
trainer/Q2 Predictions Max              -4.71347
trainer/Q2 Predictions Min             -42.5177
trainer/Q Targets Mean                 -11.334
trainer/Q Targets Std                   10.5035
trainer/Q Targets Max                   -4.6218
trainer/Q Targets Min                  -42.7378
trainer/Log Pis Mean                     2.05751
trainer/Log Pis Std                      2.20895
trainer/Log Pis Max                      8.00897
trainer/Log Pis Min                     -4.10822
trainer/Policy mu Mean                  -1.2236
trainer/Policy mu Std                    0.761346
trainer/Policy mu Max                    1.00016
trainer/Policy mu Min                   -2.92005
trainer/Policy log std Mean             -0.984863
trainer/Policy log std Std               0.31724
trainer/Policy log std Max              -0.440223
trainer/Policy log std Min              -2.11132
trainer/Alpha                            0.053291
trainer/Alpha Loss                       0.168624
exploration/num steps total           3200
exploration/num paths total             32
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.30237
exploration/Rewards Std                  1.57877
exploration/Rewards Max                 -0.0103891
exploration/Rewards Min                 -7.08563
exploration/Returns Mean              -130.237
exploration/Returns Std                  0
exploration/Returns Max               -130.237
exploration/Returns Min               -130.237
exploration/Actions Mean                -0.505593
exploration/Actions Std                  1.07713
exploration/Actions Max                  2.49097
exploration/Actions Min                 -2.74609
exploration/Num Paths                    1
exploration/Average Returns           -130.237
evaluation/num steps total          160000
evaluation/num paths total             160
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.02809
evaluation/Rewards Std                   2.01972
evaluation/Rewards Max                  -0.000679383
evaluation/Rewards Min                  -5.87628
evaluation/Returns Mean              -2028.09
evaluation/Returns Std                1974.43
evaluation/Returns Max                -407.479
evaluation/Returns Min               -4456.02
evaluation/Actions Mean                 -0.585104
evaluation/Actions Std                   1.09749
evaluation/Actions Max                   3.46906
evaluation/Actions Min                  -4.38387
evaluation/Num Paths                     5
evaluation/Average Returns           -2028.09
time/data storing (s)                    0.00059871
time/evaluation sampling (s)             2.88226
time/exploration real sampling (s)       0.0709517
time/exploration sim sampling (s)        6.544e-06
time/logging (s)                         0.0135923
time/saving (s)                          0.0147153
time/training (s)                       21.5159
time/epoch (s)                          24.498
time/total (s)                         783.973
Epoch                                   31
----------------------------------  ----------------
2020-02-21 08:10:22.927919 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 32 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.279896
trainer/QF2 Loss                         0.226662
trainer/Policy Loss                     13.0875
trainer/Q1 Predictions Mean            -12.8531
trainer/Q1 Predictions Std              11.5617
trainer/Q1 Predictions Max              -4.85208
trainer/Q1 Predictions Min             -41.9395
trainer/Q2 Predictions Mean            -12.7678
trainer/Q2 Predictions Std              11.5128
trainer/Q2 Predictions Max              -4.7299
trainer/Q2 Predictions Min             -41.3846
trainer/Q Targets Mean                 -12.804
trainer/Q Targets Std                   11.6147
trainer/Q Targets Max                   -4.79243
trainer/Q Targets Min                  -43.1258
trainer/Log Pis Mean                     2.1042
trainer/Log Pis Std                      2.22635
trainer/Log Pis Max                      7.45967
trainer/Log Pis Min                     -2.19592
trainer/Policy mu Mean                  -1.26885
trainer/Policy mu Std                    0.822131
trainer/Policy mu Max                    1.35448
trainer/Policy mu Min                   -3.00923
trainer/Policy log std Mean             -0.880369
trainer/Policy log std Std               0.311031
trainer/Policy log std Max              -0.350669
trainer/Policy log std Min              -2.32328
trainer/Alpha                            0.0523234
trainer/Alpha Loss                       0.307423
exploration/num steps total           3300
exploration/num paths total             33
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.578995
exploration/Rewards Std                  0.646214
exploration/Rewards Max                 -0.00825519
exploration/Rewards Min                 -3.5911
exploration/Returns Mean               -57.8995
exploration/Returns Std                  0
exploration/Returns Max                -57.8995
exploration/Returns Min                -57.8995
exploration/Actions Mean                -0.544932
exploration/Actions Std                  1.1072
exploration/Actions Max                  2.50494
exploration/Actions Min                 -3.89493
exploration/Num Paths                    1
exploration/Average Returns            -57.8995
evaluation/num steps total          165000
evaluation/num paths total             165
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.441293
evaluation/Rewards Std                   0.501141
evaluation/Rewards Max                  -0.000309126
evaluation/Rewards Min                  -5.73262
evaluation/Returns Mean               -441.293
evaluation/Returns Std                  21.1677
evaluation/Returns Max                -407.021
evaluation/Returns Min                -465.127
evaluation/Actions Mean                 -0.605977
evaluation/Actions Std                   1.02112
evaluation/Actions Max                   3.31181
evaluation/Actions Min                  -4.37542
evaluation/Num Paths                     5
evaluation/Average Returns            -441.293
time/data storing (s)                    0.000594006
time/evaluation sampling (s)             2.77951
time/exploration real sampling (s)       0.0651406
time/exploration sim sampling (s)        6.373e-06
time/logging (s)                         0.0134928
time/saving (s)                          0.0176447
time/training (s)                       20.6158
time/epoch (s)                          23.4922
time/total (s)                         807.471
Epoch                                   32
----------------------------------  ----------------
2020-02-21 08:10:48.995464 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 33 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.322313
trainer/QF2 Loss                         0.272555
trainer/Policy Loss                     11.9277
trainer/Q1 Predictions Mean            -11.7778
trainer/Q1 Predictions Std              10.4239
trainer/Q1 Predictions Max              -4.9264
trainer/Q1 Predictions Min             -42.3978
trainer/Q2 Predictions Mean            -11.855
trainer/Q2 Predictions Std              10.4851
trainer/Q2 Predictions Max              -4.96032
trainer/Q2 Predictions Min             -42.4235
trainer/Q Targets Mean                 -11.6496
trainer/Q Targets Std                   10.4016
trainer/Q Targets Max                   -4.92667
trainer/Q Targets Min                  -40.8689
trainer/Log Pis Mean                     2.16478
trainer/Log Pis Std                      2.2559
trainer/Log Pis Max                      7.55964
trainer/Log Pis Min                     -5.08353
trainer/Policy mu Mean                  -1.22785
trainer/Policy mu Std                    0.801365
trainer/Policy mu Max                    1.2058
trainer/Policy mu Min                   -2.95282
trainer/Policy log std Mean             -0.966856
trainer/Policy log std Std               0.325367
trainer/Policy log std Max              -0.408341
trainer/Policy log std Min              -2.36894
trainer/Alpha                            0.0512581
trainer/Alpha Loss                       0.489551
exploration/num steps total           3400
exploration/num paths total             34
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.651765
exploration/Rewards Std                  0.854503
exploration/Rewards Max                 -0.00244794
exploration/Rewards Min                 -4.18141
exploration/Returns Mean               -65.1765
exploration/Returns Std                  0
exploration/Returns Max                -65.1765
exploration/Returns Min                -65.1765
exploration/Actions Mean                -0.593701
exploration/Actions Std                  1.08442
exploration/Actions Max                  2.32714
exploration/Actions Min                 -3.86719
exploration/Num Paths                    1
exploration/Average Returns            -65.1765
evaluation/num steps total          170000
evaluation/num paths total             170
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.07185
evaluation/Rewards Std                   1.99338
evaluation/Rewards Max                  -0.000556267
evaluation/Rewards Min                  -7.00835
evaluation/Returns Mean              -2071.85
evaluation/Returns Std                1902.74
evaluation/Returns Max                -456.606
evaluation/Returns Min               -4409.09
evaluation/Actions Mean                 -0.604245
evaluation/Actions Std                   1.09604
evaluation/Actions Max                   4.28077
evaluation/Actions Min                  -5.3356
evaluation/Num Paths                     5
evaluation/Average Returns           -2071.85
time/data storing (s)                    0.000624113
time/evaluation sampling (s)             2.95614
time/exploration real sampling (s)       0.070324
time/exploration sim sampling (s)        7.074e-06
time/logging (s)                         0.0146035
time/saving (s)                          0.0164537
time/training (s)                       23.004
time/epoch (s)                          26.0622
time/total (s)                         833.538
Epoch                                   33
----------------------------------  ----------------
2020-02-21 08:11:14.567034 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 34 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.262144
trainer/QF2 Loss                         0.254113
trainer/Policy Loss                     11.0596
trainer/Q1 Predictions Mean            -10.9861
trainer/Q1 Predictions Std              10.1037
trainer/Q1 Predictions Max              -4.83903
trainer/Q1 Predictions Min             -45.6455
trainer/Q2 Predictions Mean            -11.009
trainer/Q2 Predictions Std              10.105
trainer/Q2 Predictions Max              -4.90881
trainer/Q2 Predictions Min             -45.3462
trainer/Q Targets Mean                 -11.0628
trainer/Q Targets Std                   10.0717
trainer/Q Targets Max                   -4.99164
trainer/Q Targets Min                  -44.3107
trainer/Log Pis Mean                     1.87346
trainer/Log Pis Std                      2.19305
trainer/Log Pis Max                      7.89606
trainer/Log Pis Min                     -6.86
trainer/Policy mu Mean                  -1.16875
trainer/Policy mu Std                    0.802486
trainer/Policy mu Max                    1.56842
trainer/Policy mu Min                   -2.93072
trainer/Policy log std Mean             -0.936769
trainer/Policy log std Std               0.30057
trainer/Policy log std Max              -0.342226
trainer/Policy log std Min              -2.53257
trainer/Alpha                            0.0501782
trainer/Alpha Loss                      -0.378604
exploration/num steps total           3500
exploration/num paths total             35
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.789112
exploration/Rewards Std                  1.13917
exploration/Rewards Max                 -0.0098697
exploration/Rewards Min                 -6.42786
exploration/Returns Mean               -78.9112
exploration/Returns Std                  0
exploration/Returns Max                -78.9112
exploration/Returns Min                -78.9112
exploration/Actions Mean                -0.566798
exploration/Actions Std                  0.976389
exploration/Actions Max                  1.63765
exploration/Actions Min                 -2.90321
exploration/Num Paths                    1
exploration/Average Returns            -78.9112
evaluation/num steps total          175000
evaluation/num paths total             175
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.25841
evaluation/Rewards Std                   1.67513
evaluation/Rewards Max                  -0.00146857
evaluation/Rewards Min                  -6.92319
evaluation/Returns Mean              -1258.41
evaluation/Returns Std                1582.93
evaluation/Returns Max                -419.451
evaluation/Returns Min               -4421.64
evaluation/Actions Mean                 -0.598858
evaluation/Actions Std                   1.06471
evaluation/Actions Max                   3.95589
evaluation/Actions Min                  -4.29867
evaluation/Num Paths                     5
evaluation/Average Returns           -1258.41
time/data storing (s)                    0.000673455
time/evaluation sampling (s)             4.03767
time/exploration real sampling (s)       0.0836601
time/exploration sim sampling (s)        7.054e-06
time/logging (s)                         0.0135256
time/saving (s)                          0.0157394
time/training (s)                       21.4126
time/epoch (s)                          25.5638
time/total (s)                         859.107
Epoch                                   34
----------------------------------  ----------------
2020-02-21 08:11:40.527332 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 35 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.222331
trainer/QF2 Loss                         0.169869
trainer/Policy Loss                     11.5461
trainer/Q1 Predictions Mean            -11.4599
trainer/Q1 Predictions Std              10.4411
trainer/Q1 Predictions Max              -5.28984
trainer/Q1 Predictions Min             -43.9626
trainer/Q2 Predictions Mean            -11.4212
trainer/Q2 Predictions Std              10.3706
trainer/Q2 Predictions Max              -5.30811
trainer/Q2 Predictions Min             -44.3471
trainer/Q Targets Mean                 -11.2235
trainer/Q Targets Std                   10.321
trainer/Q Targets Max                   -5.17208
trainer/Q Targets Min                  -43.0232
trainer/Log Pis Mean                     1.87621
trainer/Log Pis Std                      2.17281
trainer/Log Pis Max                      7.7382
trainer/Log Pis Min                     -4.43356
trainer/Policy mu Mean                  -1.23591
trainer/Policy mu Std                    0.744972
trainer/Policy mu Max                    1.56064
trainer/Policy mu Min                   -2.87971
trainer/Policy log std Mean             -0.942932
trainer/Policy log std Std               0.31606
trainer/Policy log std Max              -0.329537
trainer/Policy log std Min              -2.13461
trainer/Alpha                            0.0498319
trainer/Alpha Loss                      -0.371241
exploration/num steps total           3600
exploration/num paths total             36
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.04955
exploration/Rewards Std                  1.36156
exploration/Rewards Max                 -0.00314269
exploration/Rewards Min                 -7.45591
exploration/Returns Mean              -104.955
exploration/Returns Std                  0
exploration/Returns Max               -104.955
exploration/Returns Min               -104.955
exploration/Actions Mean                -0.564175
exploration/Actions Std                  1.06137
exploration/Actions Max                  2.16961
exploration/Actions Min                 -3.63452
exploration/Num Paths                    1
exploration/Average Returns           -104.955
evaluation/num steps total          180000
evaluation/num paths total             180
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.10847
evaluation/Rewards Std                   1.99199
evaluation/Rewards Max                  -0.000414259
evaluation/Rewards Min                  -6.96919
evaluation/Returns Mean              -2108.47
evaluation/Returns Std                1878.39
evaluation/Returns Max                -516.022
evaluation/Returns Min               -4424.26
evaluation/Actions Mean                 -0.593614
evaluation/Actions Std                   1.1008
evaluation/Actions Max                   4.19397
evaluation/Actions Min                  -4.61166
evaluation/Num Paths                     5
evaluation/Average Returns           -2108.47
time/data storing (s)                    0.000616851
time/evaluation sampling (s)             3.24988
time/exploration real sampling (s)       0.068554
time/exploration sim sampling (s)        6.659e-06
time/logging (s)                         0.0135864
time/saving (s)                          0.0160155
time/training (s)                       22.6047
time/epoch (s)                          25.9534
time/total (s)                         885.065
Epoch                                   35
----------------------------------  ----------------
2020-02-21 08:12:05.852287 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 36 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.187102
trainer/QF2 Loss                         0.151784
trainer/Policy Loss                     11.2793
trainer/Q1 Predictions Mean            -11.0575
trainer/Q1 Predictions Std              10.3821
trainer/Q1 Predictions Max              -5.10973
trainer/Q1 Predictions Min             -44.8256
trainer/Q2 Predictions Mean            -11.0654
trainer/Q2 Predictions Std              10.4074
trainer/Q2 Predictions Max              -5.19375
trainer/Q2 Predictions Min             -45.0731
trainer/Q Targets Mean                 -11.161
trainer/Q Targets Std                   10.4339
trainer/Q Targets Max                   -5.24958
trainer/Q Targets Min                  -44.7053
trainer/Log Pis Mean                     2.04179
trainer/Log Pis Std                      2.16808
trainer/Log Pis Max                      7.08994
trainer/Log Pis Min                     -8.91981
trainer/Policy mu Mean                  -1.19911
trainer/Policy mu Std                    0.769728
trainer/Policy mu Max                    1.62539
trainer/Policy mu Min                   -2.8039
trainer/Policy log std Mean             -0.952276
trainer/Policy log std Std               0.337258
trainer/Policy log std Max              -0.373241
trainer/Policy log std Min              -2.4986
trainer/Alpha                            0.0495668
trainer/Alpha Loss                       0.125558
exploration/num steps total           3700
exploration/num paths total             37
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.820624
exploration/Rewards Std                  1.05112
exploration/Rewards Max                 -0.0073777
exploration/Rewards Min                 -6.85269
exploration/Returns Mean               -82.0624
exploration/Returns Std                  0
exploration/Returns Max                -82.0624
exploration/Returns Min                -82.0624
exploration/Actions Mean                -0.579211
exploration/Actions Std                  1.04634
exploration/Actions Max                  2.12668
exploration/Actions Min                 -3.52823
exploration/Num Paths                    1
exploration/Average Returns            -82.0624
evaluation/num steps total          185000
evaluation/num paths total             185
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.04466
evaluation/Rewards Std                   2.01997
evaluation/Rewards Max                  -0.000613283
evaluation/Rewards Min                  -6.56687
evaluation/Returns Mean              -2044.66
evaluation/Returns Std                1951.94
evaluation/Returns Max                -394.512
evaluation/Returns Min               -4434.68
evaluation/Actions Mean                 -0.586822
evaluation/Actions Std                   1.0789
evaluation/Actions Max                   4.01103
evaluation/Actions Min                  -4.34587
evaluation/Num Paths                     5
evaluation/Average Returns           -2044.66
time/data storing (s)                    0.0012261
time/evaluation sampling (s)             3.1707
time/exploration real sampling (s)       0.0818251
time/exploration sim sampling (s)        6.881e-06
time/logging (s)                         0.0133439
time/saving (s)                          0.016853
time/training (s)                       22.0343
time/epoch (s)                          25.3183
time/total (s)                         910.389
Epoch                                   36
----------------------------------  ----------------
2020-02-21 08:12:31.948477 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 37 finished
----------------------------------  ---------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.27247
trainer/QF2 Loss                         0.281071
trainer/Policy Loss                     12.2505
trainer/Q1 Predictions Mean            -12.06
trainer/Q1 Predictions Std              11.2919
trainer/Q1 Predictions Max              -5.39024
trainer/Q1 Predictions Min             -46.0644
trainer/Q2 Predictions Mean            -11.952
trainer/Q2 Predictions Std              11.2106
trainer/Q2 Predictions Max              -5.34854
trainer/Q2 Predictions Min             -44.9233
trainer/Q Targets Mean                 -12.0486
trainer/Q Targets Std                   11.3821
trainer/Q Targets Max                   -5.32896
trainer/Q Targets Min                  -46.1621
trainer/Log Pis Mean                     2.20964
trainer/Log Pis Std                      2.23296
trainer/Log Pis Max                      8.46203
trainer/Log Pis Min                     -4.92073
trainer/Policy mu Mean                  -1.22572
trainer/Policy mu Std                    0.799628
trainer/Policy mu Max                    1.35532
trainer/Policy mu Min                   -2.93565
trainer/Policy log std Mean             -0.967341
trainer/Policy log std Std               0.314746
trainer/Policy log std Max              -0.349136
trainer/Policy log std Min              -2.34215
trainer/Alpha                            0.0490554
trainer/Alpha Loss                       0.632012
exploration/num steps total           3800
exploration/num paths total             38
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.658792
exploration/Rewards Std                  0.694591
exploration/Rewards Max                 -0.00339416
exploration/Rewards Min                 -2.91707
exploration/Returns Mean               -65.8792
exploration/Returns Std                  0
exploration/Returns Max                -65.8792
exploration/Returns Min                -65.8792
exploration/Actions Mean                -0.610383
exploration/Actions Std                  1.02219
exploration/Actions Max                  1.85508
exploration/Actions Min                 -3.3128
exploration/Num Paths                    1
exploration/Average Returns            -65.8792
evaluation/num steps total          190000
evaluation/num paths total             190
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.541063
evaluation/Rewards Std                   0.780559
evaluation/Rewards Max                  -0.00085762
evaluation/Rewards Min                  -6.41683
evaluation/Returns Mean               -541.063
evaluation/Returns Std                 119.447
evaluation/Returns Max                -407.806
evaluation/Returns Min                -697.63
evaluation/Actions Mean                 -0.653225
evaluation/Actions Std                   1.023
evaluation/Actions Max                   3.31864
evaluation/Actions Min                  -4.47233
evaluation/Num Paths                     5
evaluation/Average Returns            -541.063
time/data storing (s)                    0.00065723
time/evaluation sampling (s)             2.82728
time/exploration real sampling (s)       0.0856562
time/exploration sim sampling (s)        6.11e-06
time/logging (s)                         0.0138733
time/saving (s)                          0.0173027
time/training (s)                       23.1443
time/epoch (s)                          26.089
time/total (s)                         936.484
Epoch                                   37
----------------------------------  ---------------
2020-02-21 08:12:57.173846 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 38 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.17521
trainer/QF2 Loss                         0.158821
trainer/Policy Loss                     11.2175
trainer/Q1 Predictions Mean            -11.2177
trainer/Q1 Predictions Std              10.2593
trainer/Q1 Predictions Max              -5.27906
trainer/Q1 Predictions Min             -45.4063
trainer/Q2 Predictions Mean            -11.243
trainer/Q2 Predictions Std              10.2146
trainer/Q2 Predictions Max              -5.30291
trainer/Q2 Predictions Min             -45.3685
trainer/Q Targets Mean                 -11.2774
trainer/Q Targets Std                   10.2796
trainer/Q Targets Max                   -5.31793
trainer/Q Targets Min                  -44.833
trainer/Log Pis Mean                     1.85419
trainer/Log Pis Std                      2.16104
trainer/Log Pis Max                      7.84486
trainer/Log Pis Min                     -3.57929
trainer/Policy mu Mean                  -1.21052
trainer/Policy mu Std                    0.777676
trainer/Policy mu Max                    1.71264
trainer/Policy mu Min                   -2.94855
trainer/Policy log std Mean             -0.949099
trainer/Policy log std Std               0.338201
trainer/Policy log std Max              -0.349931
trainer/Policy log std Min              -2.33899
trainer/Alpha                            0.0479817
trainer/Alpha Loss                      -0.442814
exploration/num steps total           3900
exploration/num paths total             39
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.383142
exploration/Rewards Std                  0.449026
exploration/Rewards Max                 -0.00522315
exploration/Rewards Min                 -2.15118
exploration/Returns Mean               -38.3142
exploration/Returns Std                  0
exploration/Returns Max                -38.3142
exploration/Returns Min                -38.3142
exploration/Actions Mean                -0.716669
exploration/Actions Std                  1.06007
exploration/Actions Max                  2.32488
exploration/Actions Min                 -3.11061
exploration/Num Paths                    1
exploration/Average Returns            -38.3142
evaluation/num steps total          195000
evaluation/num paths total             195
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.83759
evaluation/Rewards Std                   2.00676
evaluation/Rewards Max                  -0.00100112
evaluation/Rewards Min                  -6.81575
evaluation/Returns Mean              -2837.59
evaluation/Returns Std                1962.28
evaluation/Returns Max                -427.06
evaluation/Returns Min               -4454
evaluation/Actions Mean                 -0.552752
evaluation/Actions Std                   1.14343
evaluation/Actions Max                   3.83435
evaluation/Actions Min                  -4.38782
evaluation/Num Paths                     5
evaluation/Average Returns           -2837.59
time/data storing (s)                    0.000646234
time/evaluation sampling (s)             2.91514
time/exploration real sampling (s)       0.0647561
time/exploration sim sampling (s)        6.75301e-06
time/logging (s)                         0.0139658
time/saving (s)                          0.0160315
time/training (s)                       22.2089
time/epoch (s)                          25.2195
time/total (s)                         961.708
Epoch                                   38
----------------------------------  ----------------
2020-02-21 08:13:22.443274 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 39 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.314251
trainer/QF2 Loss                         0.271672
trainer/Policy Loss                     12.5473
trainer/Q1 Predictions Mean            -12.0038
trainer/Q1 Predictions Std              10.6986
trainer/Q1 Predictions Max              -5.46556
trainer/Q1 Predictions Min             -45.3896
trainer/Q2 Predictions Mean            -11.9211
trainer/Q2 Predictions Std              10.6905
trainer/Q2 Predictions Max              -5.29358
trainer/Q2 Predictions Min             -44.9166
trainer/Q Targets Mean                 -11.963
trainer/Q Targets Std                   10.725
trainer/Q Targets Max                   -5.45826
trainer/Q Targets Min                  -44.6765
trainer/Log Pis Mean                     2.27489
trainer/Log Pis Std                      2.29463
trainer/Log Pis Max                      8.40678
trainer/Log Pis Min                     -2.58414
trainer/Policy mu Mean                  -1.22708
trainer/Policy mu Std                    0.841083
trainer/Policy mu Max                    2.13839
trainer/Policy mu Min                   -2.97331
trainer/Policy log std Mean             -0.942043
trainer/Policy log std Std               0.3193
trainer/Policy log std Max              -0.464254
trainer/Policy log std Min              -2.52522
trainer/Alpha                            0.0467166
trainer/Alpha Loss                       0.842168
exploration/num steps total           4000
exploration/num paths total             40
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.364729
exploration/Rewards Std                  0.397452
exploration/Rewards Max                 -0.00325158
exploration/Rewards Min                 -1.9812
exploration/Returns Mean               -36.4729
exploration/Returns Std                  0
exploration/Returns Max                -36.4729
exploration/Returns Min                -36.4729
exploration/Actions Mean                -0.621107
exploration/Actions Std                  0.949138
exploration/Actions Max                  2.13108
exploration/Actions Min                 -3.12497
exploration/Num Paths                    1
exploration/Average Returns            -36.4729
evaluation/num steps total          200000
evaluation/num paths total             200
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.32006
evaluation/Rewards Std                   1.71149
evaluation/Rewards Max                  -0.000887859
evaluation/Rewards Min                  -6.50823
evaluation/Returns Mean              -1320.06
evaluation/Returns Std                1560.12
evaluation/Returns Max                -387.622
evaluation/Returns Min               -4430.26
evaluation/Actions Mean                 -0.625978
evaluation/Actions Std                   1.05711
evaluation/Actions Max                   5.18541
evaluation/Actions Min                  -4.57974
evaluation/Num Paths                     5
evaluation/Average Returns           -1320.06
time/data storing (s)                    0.000597453
time/evaluation sampling (s)             2.74329
time/exploration real sampling (s)       0.0629761
time/exploration sim sampling (s)        6.723e-06
time/logging (s)                         0.0128468
time/saving (s)                          0.0161131
time/training (s)                       22.4258
time/epoch (s)                          25.2616
time/total (s)                         986.975
Epoch                                   39
----------------------------------  ----------------
2020-02-21 08:13:47.169294 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 40 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.271036
trainer/QF2 Loss                         0.239804
trainer/Policy Loss                     11.3504
trainer/Q1 Predictions Mean            -11.0873
trainer/Q1 Predictions Std               9.56863
trainer/Q1 Predictions Max              -5.40785
trainer/Q1 Predictions Min             -41.5937
trainer/Q2 Predictions Mean            -10.9312
trainer/Q2 Predictions Std               9.46705
trainer/Q2 Predictions Max              -5.34309
trainer/Q2 Predictions Min             -41.0492
trainer/Q Targets Mean                 -11.0168
trainer/Q Targets Std                    9.59782
trainer/Q Targets Max                   -5.453
trainer/Q Targets Min                  -42.8444
trainer/Log Pis Mean                     1.92384
trainer/Log Pis Std                      2.0584
trainer/Log Pis Max                      7.66119
trainer/Log Pis Min                     -3.87966
trainer/Policy mu Mean                  -1.22402
trainer/Policy mu Std                    0.805839
trainer/Policy mu Max                    2.13007
trainer/Policy mu Min                   -3.0059
trainer/Policy log std Mean             -0.96073
trainer/Policy log std Std               0.363074
trainer/Policy log std Max              -0.45103
trainer/Policy log std Min              -2.84776
trainer/Alpha                            0.0457043
trainer/Alpha Loss                      -0.23498
exploration/num steps total           4100
exploration/num paths total             41
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.544867
exploration/Rewards Std                  0.628564
exploration/Rewards Max                 -0.00275949
exploration/Rewards Min                 -2.94905
exploration/Returns Mean               -54.4867
exploration/Returns Std                  0
exploration/Returns Max                -54.4867
exploration/Returns Min                -54.4867
exploration/Actions Mean                -0.672139
exploration/Actions Std                  1.00056
exploration/Actions Max                  2.12181
exploration/Actions Min                 -3.1659
exploration/Num Paths                    1
exploration/Average Returns            -54.4867
evaluation/num steps total          205000
evaluation/num paths total             205
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.25593
evaluation/Rewards Std                   1.68441
evaluation/Rewards Max                  -0.000919352
evaluation/Rewards Min                  -6.75563
evaluation/Returns Mean              -1255.93
evaluation/Returns Std                1578.11
evaluation/Returns Max                -397.605
evaluation/Returns Min               -4410.7
evaluation/Actions Mean                 -0.626574
evaluation/Actions Std                   1.05783
evaluation/Actions Max                   3.1687
evaluation/Actions Min                  -4.32001
evaluation/Num Paths                     5
evaluation/Average Returns           -1255.93
time/data storing (s)                    0.000618356
time/evaluation sampling (s)             2.94187
time/exploration real sampling (s)       0.0656273
time/exploration sim sampling (s)        6.748e-06
time/logging (s)                         0.0135826
time/saving (s)                          0.0175348
time/training (s)                       21.681
time/epoch (s)                          24.7202
time/total (s)                        1011.7
Epoch                                   40
----------------------------------  ----------------
2020-02-21 08:14:12.812398 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 41 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.302844
trainer/QF2 Loss                         0.204645
trainer/Policy Loss                     11.2807
trainer/Q1 Predictions Mean            -11.2519
trainer/Q1 Predictions Std              10.066
trainer/Q1 Predictions Max              -5.14568
trainer/Q1 Predictions Min             -43.583
trainer/Q2 Predictions Mean            -11.2624
trainer/Q2 Predictions Std              10.0721
trainer/Q2 Predictions Max              -5.32517
trainer/Q2 Predictions Min             -44.0752
trainer/Q Targets Mean                 -11.272
trainer/Q Targets Std                   10.1659
trainer/Q Targets Max                   -5.53506
trainer/Q Targets Min                  -44.613
trainer/Log Pis Mean                     1.79974
trainer/Log Pis Std                      2.15413
trainer/Log Pis Max                      7.49348
trainer/Log Pis Min                     -5.28435
trainer/Policy mu Mean                  -1.17345
trainer/Policy mu Std                    0.752897
trainer/Policy mu Max                    1.60625
trainer/Policy mu Min                   -2.91576
trainer/Policy log std Mean             -0.953501
trainer/Policy log std Std               0.342918
trainer/Policy log std Max              -0.288919
trainer/Policy log std Min              -2.74686
trainer/Alpha                            0.0453291
trainer/Alpha Loss                      -0.619565
exploration/num steps total           4200
exploration/num paths total             42
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.699356
exploration/Rewards Std                  1.15387
exploration/Rewards Max                 -0.0168426
exploration/Rewards Min                 -6.75054
exploration/Returns Mean               -69.9356
exploration/Returns Std                  0
exploration/Returns Max                -69.9356
exploration/Returns Min                -69.9356
exploration/Actions Mean                -0.633196
exploration/Actions Std                  1.0815
exploration/Actions Max                  2.16468
exploration/Actions Min                 -2.90829
exploration/Num Paths                    1
exploration/Average Returns            -69.9356
evaluation/num steps total          210000
evaluation/num paths total             210
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.24591
evaluation/Rewards Std                   1.68615
evaluation/Rewards Max                  -0.000614507
evaluation/Rewards Min                  -5.81902
evaluation/Returns Mean              -1245.91
evaluation/Returns Std                1592.69
evaluation/Returns Max                -383.852
evaluation/Returns Min               -4426.82
evaluation/Actions Mean                 -0.627865
evaluation/Actions Std                   1.05541
evaluation/Actions Max                   4.28683
evaluation/Actions Min                  -4.1642
evaluation/Num Paths                     5
evaluation/Average Returns           -1245.91
time/data storing (s)                    0.000639246
time/evaluation sampling (s)             3.43167
time/exploration real sampling (s)       0.0697075
time/exploration sim sampling (s)        6.512e-06
time/logging (s)                         0.0131259
time/saving (s)                          0.0163277
time/training (s)                       22.1047
time/epoch (s)                          25.6362
time/total (s)                        1037.34
Epoch                                   41
----------------------------------  ----------------
2020-02-21 08:14:36.471323 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 42 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.139991
trainer/QF2 Loss                         0.109714
trainer/Policy Loss                     11.485
trainer/Q1 Predictions Mean            -11.0956
trainer/Q1 Predictions Std              10.2552
trainer/Q1 Predictions Max              -5.51836
trainer/Q1 Predictions Min             -45.7994
trainer/Q2 Predictions Mean            -11.132
trainer/Q2 Predictions Std              10.2929
trainer/Q2 Predictions Max              -5.42249
trainer/Q2 Predictions Min             -45.6006
trainer/Q Targets Mean                 -11.1688
trainer/Q Targets Std                   10.2599
trainer/Q Targets Max                   -5.57137
trainer/Q Targets Min                  -44.5903
trainer/Log Pis Mean                     1.81079
trainer/Log Pis Std                      2.07441
trainer/Log Pis Max                      8.16664
trainer/Log Pis Min                     -3.76406
trainer/Policy mu Mean                  -1.15201
trainer/Policy mu Std                    0.731383
trainer/Policy mu Max                    2.10589
trainer/Policy mu Min                   -2.89958
trainer/Policy log std Mean             -0.952704
trainer/Policy log std Std               0.325858
trainer/Policy log std Max              -0.360878
trainer/Policy log std Min              -2.74324
trainer/Alpha                            0.0441492
trainer/Alpha Loss                      -0.590348
exploration/num steps total           4300
exploration/num paths total             43
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.630051
exploration/Rewards Std                  0.68152
exploration/Rewards Max                 -0.0109733
exploration/Rewards Min                 -3.83789
exploration/Returns Mean               -63.0051
exploration/Returns Std                  0
exploration/Returns Max                -63.0051
exploration/Returns Min                -63.0051
exploration/Actions Mean                -0.560842
exploration/Actions Std                  0.984729
exploration/Actions Max                  1.96582
exploration/Actions Min                 -3.20986
exploration/Num Paths                    1
exploration/Average Returns            -63.0051
evaluation/num steps total          215000
evaluation/num paths total             215
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.82809
evaluation/Rewards Std                   2.00144
evaluation/Rewards Max                  -0.00195703
evaluation/Rewards Min                  -6.51394
evaluation/Returns Mean              -2828.09
evaluation/Returns Std                1959.07
evaluation/Returns Max                -424.492
evaluation/Returns Min               -4441.81
evaluation/Actions Mean                 -0.547449
evaluation/Actions Std                   1.13361
evaluation/Actions Max                   4.03344
evaluation/Actions Min                  -4.22232
evaluation/Num Paths                     5
evaluation/Average Returns           -2828.09
time/data storing (s)                    0.000613072
time/evaluation sampling (s)             2.91291
time/exploration real sampling (s)       0.0754399
time/exploration sim sampling (s)        7.357e-06
time/logging (s)                         0.0128328
time/saving (s)                          0.0158272
time/training (s)                       20.6349
time/epoch (s)                          23.6526
time/total (s)                        1061
Epoch                                   42
----------------------------------  ----------------
2020-02-21 08:14:59.540985 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 43 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.348761
trainer/QF2 Loss                         0.309269
trainer/Policy Loss                     11.4204
trainer/Q1 Predictions Mean            -10.8894
trainer/Q1 Predictions Std               9.87283
trainer/Q1 Predictions Max              -5.45145
trainer/Q1 Predictions Min             -41.7722
trainer/Q2 Predictions Mean            -10.9165
trainer/Q2 Predictions Std               9.92879
trainer/Q2 Predictions Max              -5.42213
trainer/Q2 Predictions Min             -41.8855
trainer/Q Targets Mean                 -11.0907
trainer/Q Targets Std                    9.92978
trainer/Q Targets Max                   -5.60763
trainer/Q Targets Min                  -42.7954
trainer/Log Pis Mean                     1.9483
trainer/Log Pis Std                      2.25413
trainer/Log Pis Max                      7.98296
trainer/Log Pis Min                     -6.73359
trainer/Policy mu Mean                  -1.16788
trainer/Policy mu Std                    0.809368
trainer/Policy mu Max                    2.18241
trainer/Policy mu Min                   -2.91156
trainer/Policy log std Mean             -0.968683
trainer/Policy log std Std               0.390545
trainer/Policy log std Max              -0.373088
trainer/Policy log std Min              -2.87399
trainer/Alpha                            0.0434768
trainer/Alpha Loss                      -0.162102
exploration/num steps total           4400
exploration/num paths total             44
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.490016
exploration/Rewards Std                  0.506412
exploration/Rewards Max                 -0.00139347
exploration/Rewards Min                 -2.01737
exploration/Returns Mean               -49.0016
exploration/Returns Std                  0
exploration/Returns Max                -49.0016
exploration/Returns Min                -49.0016
exploration/Actions Mean                -0.539801
exploration/Actions Std                  0.96817
exploration/Actions Max                  2.26795
exploration/Actions Min                 -3.30542
exploration/Num Paths                    1
exploration/Average Returns            -49.0016
evaluation/num steps total          220000
evaluation/num paths total             220
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.495268
evaluation/Rewards Std                   0.677725
evaluation/Rewards Max                  -0.00061872
evaluation/Rewards Min                  -4.96128
evaluation/Returns Mean               -495.268
evaluation/Returns Std                  75.675
evaluation/Returns Max                -376.683
evaluation/Returns Min                -565.543
evaluation/Actions Mean                 -0.652079
evaluation/Actions Std                   1.01766
evaluation/Actions Max                   3.34585
evaluation/Actions Min                  -4.70553
evaluation/Num Paths                     5
evaluation/Average Returns            -495.268
time/data storing (s)                    0.000609233
time/evaluation sampling (s)             2.68235
time/exploration real sampling (s)       0.067124
time/exploration sim sampling (s)        6.607e-06
time/logging (s)                         0.0165397
time/saving (s)                          0.0169263
time/training (s)                       20.2834
time/epoch (s)                          23.067
time/total (s)                        1084.07
Epoch                                   43
----------------------------------  ----------------
2020-02-21 08:15:24.258127 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 44 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.15624
trainer/QF2 Loss                         0.13561
trainer/Policy Loss                     10.8959
trainer/Q1 Predictions Mean            -10.4475
trainer/Q1 Predictions Std               9.49662
trainer/Q1 Predictions Max              -5.6669
trainer/Q1 Predictions Min             -45.6547
trainer/Q2 Predictions Mean            -10.4375
trainer/Q2 Predictions Std               9.48844
trainer/Q2 Predictions Max              -5.65309
trainer/Q2 Predictions Min             -45.5722
trainer/Q Targets Mean                 -10.3643
trainer/Q Targets Std                    9.45455
trainer/Q Targets Max                   -5.63273
trainer/Q Targets Min                  -44.946
trainer/Log Pis Mean                     1.92628
trainer/Log Pis Std                      2.04611
trainer/Log Pis Max                      7.63534
trainer/Log Pis Min                     -3.45817
trainer/Policy mu Mean                  -1.18727
trainer/Policy mu Std                    0.74405
trainer/Policy mu Max                    0.769619
trainer/Policy mu Min                   -3.03473
trainer/Policy log std Mean             -0.941737
trainer/Policy log std Std               0.328299
trainer/Policy log std Max              -0.356386
trainer/Policy log std Min              -2.92592
trainer/Alpha                            0.0428436
trainer/Alpha Loss                      -0.232234
exploration/num steps total           4500
exploration/num paths total             45
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.622228
exploration/Rewards Std                  1.01195
exploration/Rewards Max                 -0.00985182
exploration/Rewards Min                 -5.57716
exploration/Returns Mean               -62.2228
exploration/Returns Std                  0
exploration/Returns Max                -62.2228
exploration/Returns Min                -62.2228
exploration/Actions Mean                -0.716594
exploration/Actions Std                  1.10488
exploration/Actions Max                  2.49142
exploration/Actions Min                 -4.58633
exploration/Num Paths                    1
exploration/Average Returns            -62.2228
evaluation/num steps total          225000
evaluation/num paths total             225
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.32478
evaluation/Rewards Std                   1.7123
evaluation/Rewards Max                  -0.00154132
evaluation/Rewards Min                  -7.33164
evaluation/Returns Mean              -1324.78
evaluation/Returns Std                1565.05
evaluation/Returns Max                -458.145
evaluation/Returns Min               -4447.67
evaluation/Actions Mean                 -0.610033
evaluation/Actions Std                   1.07416
evaluation/Actions Max                   3.80646
evaluation/Actions Min                  -4.41585
evaluation/Num Paths                     5
evaluation/Average Returns           -1324.78
time/data storing (s)                    0.000615031
time/evaluation sampling (s)             2.7661
time/exploration real sampling (s)       0.0654532
time/exploration sim sampling (s)        7.099e-06
time/logging (s)                         0.0134782
time/saving (s)                          0.016805
time/training (s)                       21.8448
time/epoch (s)                          24.7073
time/total (s)                        1108.78
Epoch                                   44
----------------------------------  ----------------
2020-02-21 08:15:48.968457 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 45 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.192947
trainer/QF2 Loss                         0.202139
trainer/Policy Loss                     10.5548
trainer/Q1 Predictions Mean             -9.95398
trainer/Q1 Predictions Std               8.78799
trainer/Q1 Predictions Max              -5.57903
trainer/Q1 Predictions Min             -44.9646
trainer/Q2 Predictions Mean             -9.96259
trainer/Q2 Predictions Std               8.83197
trainer/Q2 Predictions Max              -5.54286
trainer/Q2 Predictions Min             -44.918
trainer/Q Targets Mean                 -10.051
trainer/Q Targets Std                    8.85228
trainer/Q Targets Max                   -5.65883
trainer/Q Targets Min                  -44.999
trainer/Log Pis Mean                     2.07805
trainer/Log Pis Std                      1.98302
trainer/Log Pis Max                      7.46893
trainer/Log Pis Min                     -4.40015
trainer/Policy mu Mean                  -1.13795
trainer/Policy mu Std                    0.759214
trainer/Policy mu Max                    2.23356
trainer/Policy mu Min                   -2.89357
trainer/Policy log std Mean             -1.01119
trainer/Policy log std Std               0.325362
trainer/Policy log std Max              -0.516876
trainer/Policy log std Min              -3.01821
trainer/Alpha                            0.0420748
trainer/Alpha Loss                       0.247301
exploration/num steps total           4600
exploration/num paths total             46
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.637866
exploration/Rewards Std                  0.866309
exploration/Rewards Max                 -0.00708476
exploration/Rewards Min                 -4.21201
exploration/Returns Mean               -63.7866
exploration/Returns Std                  0
exploration/Returns Max                -63.7866
exploration/Returns Min                -63.7866
exploration/Actions Mean                -0.632138
exploration/Actions Std                  1.08783
exploration/Actions Max                  2.26807
exploration/Actions Min                 -3.52796
exploration/Num Paths                    1
exploration/Average Returns            -63.7866
evaluation/num steps total          230000
evaluation/num paths total             230
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.03653
evaluation/Rewards Std                   1.99325
evaluation/Rewards Max                  -0.00110336
evaluation/Rewards Min                  -5.77137
evaluation/Returns Mean              -2036.53
evaluation/Returns Std                1937.14
evaluation/Returns Max                -443.362
evaluation/Returns Min               -4417.85
evaluation/Actions Mean                 -0.572836
evaluation/Actions Std                   1.09979
evaluation/Actions Max                   4.06732
evaluation/Actions Min                  -5.87799
evaluation/Num Paths                     5
evaluation/Average Returns           -2036.53
time/data storing (s)                    0.000636057
time/evaluation sampling (s)             2.90441
time/exploration real sampling (s)       0.069989
time/exploration sim sampling (s)        7.385e-06
time/logging (s)                         0.014116
time/saving (s)                          0.0169655
time/training (s)                       21.699
time/epoch (s)                          24.7051
time/total (s)                        1133.49
Epoch                                   45
----------------------------------  ----------------
2020-02-21 08:16:14.439105 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 46 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.23233
trainer/QF2 Loss                         0.17204
trainer/Policy Loss                     11.4319
trainer/Q1 Predictions Mean            -10.7571
trainer/Q1 Predictions Std               9.28966
trainer/Q1 Predictions Max              -5.65895
trainer/Q1 Predictions Min             -41.0519
trainer/Q2 Predictions Mean            -10.7423
trainer/Q2 Predictions Std               9.30974
trainer/Q2 Predictions Max              -5.66851
trainer/Q2 Predictions Min             -41.2551
trainer/Q Targets Mean                 -10.7391
trainer/Q Targets Std                    9.25672
trainer/Q Targets Max                   -5.6409
trainer/Q Targets Min                  -41.509
trainer/Log Pis Mean                     2.06058
trainer/Log Pis Std                      2.15637
trainer/Log Pis Max                      7.34398
trainer/Log Pis Min                     -4.95747
trainer/Policy mu Mean                  -1.20887
trainer/Policy mu Std                    0.780649
trainer/Policy mu Max                    2.32429
trainer/Policy mu Min                   -3.00228
trainer/Policy log std Mean             -0.92816
trainer/Policy log std Std               0.327158
trainer/Policy log std Max              -0.398401
trainer/Policy log std Min              -2.44296
trainer/Alpha                            0.0414263
trainer/Alpha Loss                       0.192868
exploration/num steps total           4700
exploration/num paths total             47
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.373681
exploration/Rewards Std                  0.574342
exploration/Rewards Max                 -0.00363552
exploration/Rewards Min                 -3.80295
exploration/Returns Mean               -37.3681
exploration/Returns Std                  0
exploration/Returns Max                -37.3681
exploration/Returns Min                -37.3681
exploration/Actions Mean                -0.792537
exploration/Actions Std                  1.05961
exploration/Actions Max                  1.95445
exploration/Actions Min                 -3.0081
exploration/Num Paths                    1
exploration/Average Returns            -37.3681
evaluation/num steps total          235000
evaluation/num paths total             235
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.28129
evaluation/Rewards Std                   1.70537
evaluation/Rewards Max                  -0.00181687
evaluation/Rewards Min                  -7.62477
evaluation/Returns Mean              -1281.29
evaluation/Returns Std                1571.2
evaluation/Returns Max                -418.392
evaluation/Returns Min               -4421.53
evaluation/Actions Mean                 -0.638318
evaluation/Actions Std                   1.06075
evaluation/Actions Max                   4.21172
evaluation/Actions Min                  -4.25474
evaluation/Num Paths                     5
evaluation/Average Returns           -1281.29
time/data storing (s)                    0.000632027
time/evaluation sampling (s)             2.90521
time/exploration real sampling (s)       0.0642179
time/exploration sim sampling (s)        7.297e-06
time/logging (s)                         0.0129227
time/saving (s)                          0.0162782
time/training (s)                       22.4627
time/epoch (s)                          25.462
time/total (s)                        1158.96
Epoch                                   46
----------------------------------  ----------------
2020-02-21 08:16:39.155632 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 47 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.130136
trainer/QF2 Loss                         0.0739561
trainer/Policy Loss                     10.7074
trainer/Q1 Predictions Mean            -10.0155
trainer/Q1 Predictions Std               8.87136
trainer/Q1 Predictions Max              -5.5878
trainer/Q1 Predictions Min             -41.6495
trainer/Q2 Predictions Mean             -9.98829
trainer/Q2 Predictions Std               8.85023
trainer/Q2 Predictions Max              -5.53785
trainer/Q2 Predictions Min             -41.8007
trainer/Q Targets Mean                 -10.0615
trainer/Q Targets Std                    8.84592
trainer/Q Targets Max                   -5.69126
trainer/Q Targets Min                  -41.896
trainer/Log Pis Mean                     2.01736
trainer/Log Pis Std                      2.13793
trainer/Log Pis Max                      8.03453
trainer/Log Pis Min                     -2.16638
trainer/Policy mu Mean                  -1.22573
trainer/Policy mu Std                    0.757377
trainer/Policy mu Max                    1.92973
trainer/Policy mu Min                   -3.01473
trainer/Policy log std Mean             -1.01428
trainer/Policy log std Std               0.357365
trainer/Policy log std Max              -0.357026
trainer/Policy log std Min              -3.04127
trainer/Alpha                            0.040635
trainer/Alpha Loss                       0.055616
exploration/num steps total           4800
exploration/num paths total             48
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.10249
exploration/Rewards Std                  1.22005
exploration/Rewards Max                 -0.0136822
exploration/Rewards Min                 -5.77597
exploration/Returns Mean              -110.249
exploration/Returns Std                  0
exploration/Returns Max               -110.249
exploration/Returns Min               -110.249
exploration/Actions Mean                -0.703208
exploration/Actions Std                  1.17502
exploration/Actions Max                  2.19464
exploration/Actions Min                 -3.85501
exploration/Num Paths                    1
exploration/Average Returns           -110.249
evaluation/num steps total          240000
evaluation/num paths total             240
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.01847
evaluation/Rewards Std                   2.01722
evaluation/Rewards Max                  -0.000913953
evaluation/Rewards Min                  -6.54169
evaluation/Returns Mean              -2018.47
evaluation/Returns Std                1969.54
evaluation/Returns Max                -392.527
evaluation/Returns Min               -4449.39
evaluation/Actions Mean                 -0.59431
evaluation/Actions Std                   1.07995
evaluation/Actions Max                   4.17736
evaluation/Actions Min                  -3.99444
evaluation/Num Paths                     5
evaluation/Average Returns           -2018.47
time/data storing (s)                    0.000588685
time/evaluation sampling (s)             2.89871
time/exploration real sampling (s)       0.0694585
time/exploration sim sampling (s)        6.566e-06
time/logging (s)                         0.0135654
time/saving (s)                          0.0157481
time/training (s)                       21.7126
time/epoch (s)                          24.7106
time/total (s)                        1183.68
Epoch                                   47
----------------------------------  ----------------
2020-02-21 08:17:03.183599 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 48 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.38132
trainer/QF2 Loss                         0.347056
trainer/Policy Loss                     11.5533
trainer/Q1 Predictions Mean            -11.1434
trainer/Q1 Predictions Std               9.72231
trainer/Q1 Predictions Max              -5.39836
trainer/Q1 Predictions Min             -43.6159
trainer/Q2 Predictions Mean            -11.1875
trainer/Q2 Predictions Std               9.65744
trainer/Q2 Predictions Max              -5.57059
trainer/Q2 Predictions Min             -43.6733
trainer/Q Targets Mean                 -11.3788
trainer/Q Targets Std                    9.79303
trainer/Q Targets Max                   -5.68374
trainer/Q Targets Min                  -44.8111
trainer/Log Pis Mean                     2.23546
trainer/Log Pis Std                      2.37504
trainer/Log Pis Max                      7.83339
trainer/Log Pis Min                     -6.22474
trainer/Policy mu Mean                  -1.27014
trainer/Policy mu Std                    0.810586
trainer/Policy mu Max                    1.7165
trainer/Policy mu Min                   -3.01368
trainer/Policy log std Mean             -0.980719
trainer/Policy log std Std               0.341314
trainer/Policy log std Max              -0.440295
trainer/Policy log std Min              -3.06899
trainer/Alpha                            0.0404436
trainer/Alpha Loss                       0.75533
exploration/num steps total           4900
exploration/num paths total             49
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.712873
exploration/Rewards Std                  1.02966
exploration/Rewards Max                 -0.00220612
exploration/Rewards Min                 -4.99728
exploration/Returns Mean               -71.2873
exploration/Returns Std                  0
exploration/Returns Max                -71.2873
exploration/Returns Min                -71.2873
exploration/Actions Mean                -0.685611
exploration/Actions Std                  1.03028
exploration/Actions Max                  2.39166
exploration/Actions Min                 -3.05703
exploration/Num Paths                    1
exploration/Average Returns            -71.2873
evaluation/num steps total          245000
evaluation/num paths total             245
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.04293
evaluation/Rewards Std                   2.03141
evaluation/Rewards Max                  -0.00170103
evaluation/Rewards Min                  -7.16696
evaluation/Returns Mean              -2042.93
evaluation/Returns Std                1962.72
evaluation/Returns Max                -421.661
evaluation/Returns Min               -4452.35
evaluation/Actions Mean                 -0.605202
evaluation/Actions Std                   1.10589
evaluation/Actions Max                   3.55152
evaluation/Actions Min                  -4.79061
evaluation/Num Paths                     5
evaluation/Average Returns           -2042.93
time/data storing (s)                    0.000623736
time/evaluation sampling (s)             2.88342
time/exploration real sampling (s)       0.0665228
time/exploration sim sampling (s)        7.977e-06
time/logging (s)                         0.0136975
time/saving (s)                          0.0167672
time/training (s)                       21.0395
time/epoch (s)                          24.0206
time/total (s)                        1207.7
Epoch                                   48
----------------------------------  ----------------
2020-02-21 08:17:28.382074 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 49 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.108144
trainer/QF2 Loss                         0.105101
trainer/Policy Loss                     10.9565
trainer/Q1 Predictions Mean            -10.7007
trainer/Q1 Predictions Std              10.1442
trainer/Q1 Predictions Max              -5.61051
trainer/Q1 Predictions Min             -45.6039
trainer/Q2 Predictions Mean            -10.6855
trainer/Q2 Predictions Std              10.076
trainer/Q2 Predictions Max              -5.62142
trainer/Q2 Predictions Min             -45.4818
trainer/Q Targets Mean                 -10.6821
trainer/Q Targets Std                   10.0476
trainer/Q Targets Max                   -5.65786
trainer/Q Targets Min                  -45.058
trainer/Log Pis Mean                     1.7517
trainer/Log Pis Std                      2.18794
trainer/Log Pis Max                      8.36556
trainer/Log Pis Min                     -3.45353
trainer/Policy mu Mean                  -1.13657
trainer/Policy mu Std                    0.73515
trainer/Policy mu Max                    2.49272
trainer/Policy mu Min                   -3.10023
trainer/Policy log std Mean             -0.958232
trainer/Policy log std Std               0.292418
trainer/Policy log std Max              -0.479157
trainer/Policy log std Min              -2.67951
trainer/Alpha                            0.0399942
trainer/Alpha Loss                      -0.799219
exploration/num steps total           5000
exploration/num paths total             50
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.824657
exploration/Rewards Std                  1.22335
exploration/Rewards Max                 -0.0107085
exploration/Rewards Min                 -7.06199
exploration/Returns Mean               -82.4657
exploration/Returns Std                  0
exploration/Returns Max                -82.4657
exploration/Returns Min                -82.4657
exploration/Actions Mean                -0.594786
exploration/Actions Std                  0.998328
exploration/Actions Max                  2.28354
exploration/Actions Min                 -3.16186
exploration/Num Paths                    1
exploration/Average Returns            -82.4657
evaluation/num steps total          250000
evaluation/num paths total             250
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.04079
evaluation/Rewards Std                   2.02329
evaluation/Rewards Max                  -0.00101521
evaluation/Rewards Min                  -6.94735
evaluation/Returns Mean              -2040.79
evaluation/Returns Std                1969.3
evaluation/Returns Max                -389.444
evaluation/Returns Min               -4458.23
evaluation/Actions Mean                 -0.576389
evaluation/Actions Std                   1.09921
evaluation/Actions Max                   3.53499
evaluation/Actions Min                  -4.91352
evaluation/Num Paths                     5
evaluation/Average Returns           -2040.79
time/data storing (s)                    0.000589101
time/evaluation sampling (s)             2.81325
time/exploration real sampling (s)       0.0657373
time/exploration sim sampling (s)        6.60701e-06
time/logging (s)                         0.0132444
time/saving (s)                          0.017519
time/training (s)                       22.2804
time/epoch (s)                          25.1908
time/total (s)                        1232.9
Epoch                                   49
----------------------------------  ----------------
2020-02-21 08:17:53.772286 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 50 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.246638
trainer/QF2 Loss                         0.246504
trainer/Policy Loss                     12.659
trainer/Q1 Predictions Mean            -12.2319
trainer/Q1 Predictions Std              10.9464
trainer/Q1 Predictions Max              -5.64882
trainer/Q1 Predictions Min             -42.789
trainer/Q2 Predictions Mean            -12.216
trainer/Q2 Predictions Std              10.9136
trainer/Q2 Predictions Max              -5.64583
trainer/Q2 Predictions Min             -42.1309
trainer/Q Targets Mean                 -12.2308
trainer/Q Targets Std                   10.9772
trainer/Q Targets Max                   -5.68085
trainer/Q Targets Min                  -42.3891
trainer/Log Pis Mean                     2.204
trainer/Log Pis Std                      2.47476
trainer/Log Pis Max                      7.95665
trainer/Log Pis Min                     -4.28846
trainer/Policy mu Mean                  -1.27331
trainer/Policy mu Std                    0.830116
trainer/Policy mu Max                    2.40102
trainer/Policy mu Min                   -3.36203
trainer/Policy log std Mean             -0.99733
trainer/Policy log std Std               0.372577
trainer/Policy log std Max              -0.31744
trainer/Policy log std Min              -2.7397
trainer/Alpha                            0.0396548
trainer/Alpha Loss                       0.658413
exploration/num steps total           5100
exploration/num paths total             51
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.742123
exploration/Rewards Std                  1.02427
exploration/Rewards Max                 -0.00289114
exploration/Rewards Min                 -5.78228
exploration/Returns Mean               -74.2123
exploration/Returns Std                  0
exploration/Returns Max                -74.2123
exploration/Returns Min                -74.2123
exploration/Actions Mean                -0.61392
exploration/Actions Std                  1.0571
exploration/Actions Max                  1.9286
exploration/Actions Min                 -3.39963
exploration/Num Paths                    1
exploration/Average Returns            -74.2123
evaluation/num steps total          255000
evaluation/num paths total             255
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -1.26106
evaluation/Rewards Std                   1.68633
evaluation/Rewards Max                  -0.0013167
evaluation/Rewards Min                  -7.10954
evaluation/Returns Mean              -1261.06
evaluation/Returns Std                1581.37
evaluation/Returns Max                -400.907
evaluation/Returns Min               -4421.91
evaluation/Actions Mean                 -0.625924
evaluation/Actions Std                   1.05635
evaluation/Actions Max                   3.55154
evaluation/Actions Min                  -4.97506
evaluation/Num Paths                     5
evaluation/Average Returns           -1261.06
time/data storing (s)                    0.000613722
time/evaluation sampling (s)             2.8052
time/exploration real sampling (s)       0.0677848
time/exploration sim sampling (s)        6.859e-06
time/logging (s)                         0.0154769
time/saving (s)                          0.029792
time/training (s)                       22.4667
time/epoch (s)                          25.3855
time/total (s)                        1258.29
Epoch                                   50
----------------------------------  ----------------
2020-02-21 08:18:19.061434 EST | [name-of-experiment_2020_02_21_07_56_55_0000--s-0] Epoch 51 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.157537
trainer/QF2 Loss                         0.132854
trainer/Policy Loss                     10.8112
trainer/Q1 Predictions Mean            -10.1999
trainer/Q1 Predictions Std               8.64092
trainer/Q1 Predictions Max              -5.60221
trainer/Q1 Predictions Min             -40.4792
trainer/Q2 Predictions Mean            -10.2084
trainer/Q2 Predictions Std               8.582
trainer/Q2 Predictions Max              -5.64934
trainer/Q2 Predictions Min             -40.9568
trainer/Q Targets Mean                 -10.2557
trainer/Q Targets Std                    8.63247
trainer/Q Targets Max                   -5.6632
trainer/Q Targets Min                  -40.7953
trainer/Log Pis Mean                     1.99935
trainer/Log Pis Std                      2.15472
trainer/Log Pis Max                      7.54832
trainer/Log Pis Min                     -4.57387
trainer/Policy mu Mean                  -1.22693
trainer/Policy mu Std                    0.756306
trainer/Policy mu Max                    1.09873
trainer/Policy mu Min                   -2.92638
trainer/Policy log std Mean             -0.98889
trainer/Policy log std Std               0.354851
trainer/Policy log std Max              -0.37356
trainer/Policy log std Min              -3.02268
trainer/Alpha                            0.0388081
trainer/Alpha Loss                      -0.00210769
exploration/num steps total           5200
exploration/num paths total             52
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.996861
exploration/Rewards Std                  1.47163
exploration/Rewards Max                 -0.0033938
exploration/Rewards Min                 -7.46641
exploration/Returns Mean               -99.6861
exploration/Returns Std                  0
exploration/Returns Max                -99.6861
exploration/Returns Min                -99.6861
exploration/Actions Mean                -0.673069
exploration/Actions Std                  1.13377
exploration/Actions Max                  1.9173
exploration/Actions Min                 -3.43937
exploration/Num Paths                    1
exploration/Average Returns            -99.6861
evaluation/num steps total          260000
evaluation/num paths total             260
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -2.80806
evaluation/Rewards Std                   1.99844
evaluation/Rewards Max                  -0.000882323
evaluation/Rewards Min                  -6.08454
evaluation/Returns Mean              -2808.06
evaluation/Returns Std                1959.74
evaluation/Returns Max                -393.324
evaluation/Returns Min               -4426.38
evaluation/Actions Mean                 -0.569812
evaluation/Actions Std                   1.13907
evaluation/Actions Max                   4.43913
evaluation/Actions Min                  -4.85989
evaluation/Num Paths                     5
evaluation/Average Returns           -2808.06
time/data storing (s)                    0.000666506
time/evaluation sampling (s)             2.95622
time/exploration real sampling (s)       0.0860545
time/exploration sim sampling (s)        6.28e-06
time/logging (s)                         0.0133417
time/saving (s)                          0.0175124
time/training (s)                       22.2057
time/epoch (s)                          25.2795
time/total (s)                        1283.58
Epoch                                   51
----------------------------------  ----------------
