2020-02-20 20:42:32.494508 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 0 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                       19.6378
trainer/QF2 Loss                       19.7002
trainer/Policy Loss                    -1.34664
trainer/Q1 Predictions Mean            -0.0048233
trainer/Q1 Predictions Std              0.00138541
trainer/Q1 Predictions Max             -0.000954227
trainer/Q1 Predictions Min             -0.00955853
trainer/Q2 Predictions Mean             0.00261361
trainer/Q2 Predictions Std              0.000739378
trainer/Q2 Predictions Max              0.00424657
trainer/Q2 Predictions Min              0.000393656
trainer/Q Targets Mean                 -4.02009
trainer/Q Targets Std                   1.87502
trainer/Q Targets Max                   1.39251
trainer/Q Targets Min                  -7.7239
trainer/Log Pis Mean                   -1.35135
trainer/Log Pis Std                     0.301607
trainer/Log Pis Max                    -0.598992
trainer/Log Pis Min                    -1.83549
trainer/Policy mu Mean                 -3.76134e-05
trainer/Policy mu Std                   0.000357739
trainer/Policy mu Max                   0.00066347
trainer/Policy mu Min                  -0.000873851
trainer/Policy log std Mean             9.37892e-05
trainer/Policy log std Std              0.000831992
trainer/Policy log std Max              0.00130914
trainer/Policy log std Min             -0.00201915
trainer/Alpha                           0.9997
trainer/Alpha Loss                     -0
exploration/num steps total           100
exploration/num paths total             1
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -5.28341
exploration/Rewards Std                 1.33969
exploration/Rewards Max                -2.30737
exploration/Rewards Min                -7.26505
exploration/Returns Mean             -528.341
exploration/Returns Std                 0
exploration/Returns Max              -528.341
exploration/Returns Min              -528.341
exploration/Actions Mean               -0.10612
exploration/Actions Std                 1.1806
exploration/Actions Max                 3.19287
exploration/Actions Min                -2.81584
exploration/Num Paths                   1
exploration/Average Returns          -528.341
evaluation/num steps total           5000
evaluation/num paths total              5
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -5.17687
evaluation/Rewards Std                  2.11262
evaluation/Rewards Max                 -0.0054806
evaluation/Rewards Min                 -8.47927
evaluation/Returns Mean             -5176.87
evaluation/Returns Std                376.383
evaluation/Returns Max              -4668.23
evaluation/Returns Min              -5796.83
evaluation/Actions Mean                 0.0182255
evaluation/Actions Std                  1.0057
evaluation/Actions Max                  4.0457
evaluation/Actions Min                 -3.52339
evaluation/Num Paths                    5
evaluation/Average Returns          -5176.87
time/data storing (s)                   0.000515857
time/evaluation sampling (s)            2.46083
time/exploration real sampling (s)      0.0641191
time/exploration sim sampling (s)       6.247e-06
time/logging (s)                        0.0149807
time/saving (s)                         0.0172527
time/training (s)                      15.6259
time/epoch (s)                         18.1836
time/total (s)                         24.4336
Epoch                                   0
----------------------------------  ---------------
2020-02-20 20:42:57.201983 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 1 finished
----------------------------------  --------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.36712
trainer/QF2 Loss                        1.38235
trainer/Policy Loss                    15.8859
trainer/Q1 Predictions Mean           -17.9383
trainer/Q1 Predictions Std              7.58607
trainer/Q1 Predictions Max              3.07394
trainer/Q1 Predictions Min            -31.365
trainer/Q2 Predictions Mean           -17.9865
trainer/Q2 Predictions Std              7.59162
trainer/Q2 Predictions Max              1.89888
trainer/Q2 Predictions Min            -31.4727
trainer/Q Targets Mean                -18.0948
trainer/Q Targets Std                   7.55689
trainer/Q Targets Max                   1.71819
trainer/Q Targets Min                 -30.6773
trainer/Log Pis Mean                   -0.221265
trainer/Log Pis Std                     1.18928
trainer/Log Pis Max                     1.7978
trainer/Log Pis Min                    -5.62933
trainer/Policy mu Mean                 -0.829047
trainer/Policy mu Std                   0.209755
trainer/Policy mu Max                  -0.302329
trainer/Policy mu Min                  -1.10916
trainer/Policy log std Mean            -0.349607
trainer/Policy log std Std              0.0521202
trainer/Policy log std Max             -0.243379
trainer/Policy log std Min             -0.444492
trainer/Alpha                           0.753435
trainer/Alpha Loss                     -0.628346
exploration/num steps total           200
exploration/num paths total             2
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.57189
exploration/Rewards Std                 1.23659
exploration/Rewards Max                -0.00317007
exploration/Rewards Min                -5.48248
exploration/Returns Mean             -157.189
exploration/Returns Std                 0
exploration/Returns Max              -157.189
exploration/Returns Min              -157.189
exploration/Actions Mean               -0.424447
exploration/Actions Std                 1.14658
exploration/Actions Max                 2.55318
exploration/Actions Min                -3.64484
exploration/Num Paths                   1
exploration/Average Returns          -157.189
evaluation/num steps total          10000
evaluation/num paths total             10
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -3.551
evaluation/Rewards Std                  1.59483
evaluation/Rewards Max                 -0.00313096
evaluation/Rewards Min                 -7.25398
evaluation/Returns Mean             -3551
evaluation/Returns Std               1522.6
evaluation/Returns Max               -506.094
evaluation/Returns Min              -4347.45
evaluation/Actions Mean                -0.583162
evaluation/Actions Std                  1.02642
evaluation/Actions Max                  3.69967
evaluation/Actions Min                 -4.80737
evaluation/Num Paths                    5
evaluation/Average Returns          -3551
time/data storing (s)                   0.00062784
time/evaluation sampling (s)            3.25975
time/exploration real sampling (s)      0.077746
time/exploration sim sampling (s)       1.431e-05
time/logging (s)                        0.0149368
time/saving (s)                         0.0152873
time/training (s)                      21.332
time/epoch (s)                         24.7003
time/total (s)                         49.1401
Epoch                                   1
----------------------------------  --------------
2020-02-20 20:43:23.737467 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 2 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.46929
trainer/QF2 Loss                        3.61706
trainer/Policy Loss                    24.0486
trainer/Q1 Predictions Mean           -27.8142
trainer/Q1 Predictions Std             13.719
trainer/Q1 Predictions Max              5.37663
trainer/Q1 Predictions Min            -50.7417
trainer/Q2 Predictions Mean           -27.8985
trainer/Q2 Predictions Std             13.7398
trainer/Q2 Predictions Max              3.88207
trainer/Q2 Predictions Min            -51.2591
trainer/Q Targets Mean                -27.7649
trainer/Q Targets Std                  13.8633
trainer/Q Targets Max                   3.8492
trainer/Q Targets Min                 -48.9037
trainer/Log Pis Mean                    0.812257
trainer/Log Pis Std                     1.53578
trainer/Log Pis Max                     3.14532
trainer/Log Pis Min                    -5.89455
trainer/Policy mu Mean                 -1.16328
trainer/Policy mu Std                   0.328892
trainer/Policy mu Max                  -0.219848
trainer/Policy mu Min                  -1.67928
trainer/Policy log std Mean            -0.54955
trainer/Policy log std Std              0.094527
trainer/Policy log std Max             -0.26042
trainer/Policy log std Min             -0.674787
trainer/Alpha                           0.62385
trainer/Alpha Loss                     -0.560233
exploration/num steps total           300
exploration/num paths total             3
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.855526
exploration/Rewards Std                 0.681188
exploration/Rewards Max                -0.0177035
exploration/Rewards Min                -2.70648
exploration/Returns Mean              -85.5526
exploration/Returns Std                 0
exploration/Returns Max               -85.5526
exploration/Returns Min               -85.5526
exploration/Actions Mean               -0.47024
exploration/Actions Std                 1.03155
exploration/Actions Max                 2.22547
exploration/Actions Min                -2.70799
exploration/Num Paths                   1
exploration/Average Returns           -85.5526
evaluation/num steps total          15000
evaluation/num paths total             15
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.43441
evaluation/Rewards Std                  1.54972
evaluation/Rewards Max                 -0.00100263
evaluation/Rewards Min                 -7.19985
evaluation/Returns Mean             -1434.41
evaluation/Returns Std               1405.62
evaluation/Returns Max               -676.211
evaluation/Returns Min              -4242.45
evaluation/Actions Mean                -0.448069
evaluation/Actions Std                  1.01677
evaluation/Actions Max                  3.3018
evaluation/Actions Min                 -4.45985
evaluation/Num Paths                    5
evaluation/Average Returns          -1434.41
time/data storing (s)                   0.000640381
time/evaluation sampling (s)            3.09169
time/exploration real sampling (s)      0.0747676
time/exploration sim sampling (s)       1.12e-05
time/logging (s)                        0.013151
time/saving (s)                         0.0156733
time/training (s)                      23.3324
time/epoch (s)                         26.5284
time/total (s)                         75.673
Epoch                                   2
----------------------------------  ---------------
2020-02-20 20:43:48.449702 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 3 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        4.06079
trainer/QF2 Loss                        4.21838
trainer/Policy Loss                    21.3828
trainer/Q1 Predictions Mean           -25.9296
trainer/Q1 Predictions Std             16.7298
trainer/Q1 Predictions Max              6.00171
trainer/Q1 Predictions Min            -53.4108
trainer/Q2 Predictions Mean           -25.9612
trainer/Q2 Predictions Std             16.6973
trainer/Q2 Predictions Max              4.8082
trainer/Q2 Predictions Min            -54.2207
trainer/Q Targets Mean                -25.8521
trainer/Q Targets Std                  16.8679
trainer/Q Targets Max                   4.64826
trainer/Q Targets Min                 -53.0112
trainer/Log Pis Mean                    1.10089
trainer/Log Pis Std                     1.61156
trainer/Log Pis Max                     3.99418
trainer/Log Pis Min                    -5.9426
trainer/Policy mu Mean                 -1.15913
trainer/Policy mu Std                   0.486589
trainer/Policy mu Max                   0.0220799
trainer/Policy mu Min                  -1.87692
trainer/Policy log std Mean            -0.625592
trainer/Policy log std Std              0.138982
trainer/Policy log std Max             -0.254567
trainer/Policy log std Min             -0.809519
trainer/Alpha                           0.529785
trainer/Alpha Loss                     -0.571031
exploration/num steps total           400
exploration/num paths total             4
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.17957
exploration/Rewards Std                 1.22278
exploration/Rewards Max                -0.00573191
exploration/Rewards Min                -6.45048
exploration/Returns Mean             -117.957
exploration/Returns Std                 0
exploration/Returns Max              -117.957
exploration/Returns Min              -117.957
exploration/Actions Mean               -0.460185
exploration/Actions Std                 1.17987
exploration/Actions Max                 2.33091
exploration/Actions Min                -3.18275
exploration/Num Paths                   1
exploration/Average Returns          -117.957
evaluation/num steps total          20000
evaluation/num paths total             20
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.850386
evaluation/Rewards Std                  0.713137
evaluation/Rewards Max                 -0.000788714
evaluation/Rewards Min                 -7.12907
evaluation/Returns Mean              -850.386
evaluation/Returns Std                 54.2595
evaluation/Returns Max               -793.586
evaluation/Returns Min               -950.309
evaluation/Actions Mean                -0.286351
evaluation/Actions Std                  1.02503
evaluation/Actions Max                  3.16759
evaluation/Actions Min                 -4.42936
evaluation/Num Paths                    5
evaluation/Average Returns           -850.386
time/data storing (s)                   0.000626657
time/evaluation sampling (s)            2.94543
time/exploration real sampling (s)      0.0693455
time/exploration sim sampling (s)       6.976e-06
time/logging (s)                        0.0139208
time/saving (s)                         0.0121843
time/training (s)                      21.6655
time/epoch (s)                         24.707
time/total (s)                        100.385
Epoch                                   3
----------------------------------  ---------------
2020-02-20 20:44:13.669679 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 4 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.5829
trainer/QF2 Loss                        3.59854
trainer/Policy Loss                    16.8116
trainer/Q1 Predictions Mean           -21.25
trainer/Q1 Predictions Std             15.8409
trainer/Q1 Predictions Max              5.22002
trainer/Q1 Predictions Min            -49.6043
trainer/Q2 Predictions Mean           -21.4523
trainer/Q2 Predictions Std             15.9368
trainer/Q2 Predictions Max              5.08771
trainer/Q2 Predictions Min            -50.4433
trainer/Q Targets Mean                -21.5243
trainer/Q Targets Std                  16.1626
trainer/Q Targets Max                   4.77629
trainer/Q Targets Min                 -50.1246
trainer/Log Pis Mean                    1.19052
trainer/Log Pis Std                     1.64839
trainer/Log Pis Max                     4.42454
trainer/Log Pis Min                    -3.77329
trainer/Policy mu Mean                 -1.21864
trainer/Policy mu Std                   0.509381
trainer/Policy mu Max                   0.0791702
trainer/Policy mu Min                  -2.00146
trainer/Policy log std Mean            -0.644265
trainer/Policy log std Std              0.14015
trainer/Policy log std Max             -0.211944
trainer/Policy log std Min             -0.796257
trainer/Alpha                           0.441121
trainer/Alpha Loss                     -0.662347
exploration/num steps total           500
exploration/num paths total             5
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.40525
exploration/Rewards Std                 0.980502
exploration/Rewards Max                -0.0198504
exploration/Rewards Min                -5.94761
exploration/Returns Mean             -140.525
exploration/Returns Std                 0
exploration/Returns Max              -140.525
exploration/Returns Min              -140.525
exploration/Actions Mean               -0.300133
exploration/Actions Std                 1.10455
exploration/Actions Max                 2.74447
exploration/Actions Min                -3.66404
exploration/Num Paths                   1
exploration/Average Returns          -140.525
evaluation/num steps total          25000
evaluation/num paths total             25
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.19414
evaluation/Rewards Std                  1.74908
evaluation/Rewards Max                 -0.00090532
evaluation/Rewards Min                 -6.97298
evaluation/Returns Mean             -2194.14
evaluation/Returns Std               1640.08
evaluation/Returns Max               -815.542
evaluation/Returns Min              -4224.14
evaluation/Actions Mean                -0.499506
evaluation/Actions Std                  1.0545
evaluation/Actions Max                  3.81289
evaluation/Actions Min                 -4.14344
evaluation/Num Paths                    5
evaluation/Average Returns          -2194.14
time/data storing (s)                   0.000620807
time/evaluation sampling (s)            2.92699
time/exploration real sampling (s)      0.0716972
time/exploration sim sampling (s)       6.724e-06
time/logging (s)                        0.0137961
time/saving (s)                         0.0122374
time/training (s)                      22.1878
time/epoch (s)                         25.2132
time/total (s)                        125.604
Epoch                                   4
----------------------------------  ---------------
2020-02-20 20:44:38.235532 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 5 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.02672
trainer/QF2 Loss                        2.88869
trainer/Policy Loss                    18.7628
trainer/Q1 Predictions Mean           -22.6032
trainer/Q1 Predictions Std             14.2043
trainer/Q1 Predictions Max              4.05086
trainer/Q1 Predictions Min            -46.6369
trainer/Q2 Predictions Mean           -22.6668
trainer/Q2 Predictions Std             14.198
trainer/Q2 Predictions Max              4.0048
trainer/Q2 Predictions Min            -46.9788
trainer/Q Targets Mean                -22.777
trainer/Q Targets Std                  13.9654
trainer/Q Targets Max                   4.14131
trainer/Q Targets Min                 -46.4323
trainer/Log Pis Mean                    1.36465
trainer/Log Pis Std                     1.74764
trainer/Log Pis Max                     4.65336
trainer/Log Pis Min                    -6.42632
trainer/Policy mu Mean                 -1.21399
trainer/Policy mu Std                   0.512564
trainer/Policy mu Max                   0.180889
trainer/Policy mu Min                  -1.90642
trainer/Policy log std Mean            -0.624535
trainer/Policy log std Std              0.149198
trainer/Policy log std Max             -0.208029
trainer/Policy log std Min             -0.854443
trainer/Alpha                           0.355486
trainer/Alpha Loss                     -0.656976
exploration/num steps total           600
exploration/num paths total             6
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.959408
exploration/Rewards Std                 0.96235
exploration/Rewards Max                -0.0148975
exploration/Rewards Min                -5.71629
exploration/Returns Mean              -95.9408
exploration/Returns Std                 0
exploration/Returns Max               -95.9408
exploration/Returns Min               -95.9408
exploration/Actions Mean               -0.412125
exploration/Actions Std                 1.07525
exploration/Actions Max                 2.88674
exploration/Actions Min                -2.6842
exploration/Num Paths                   1
exploration/Average Returns           -95.9408
evaluation/num steps total          30000
evaluation/num paths total             30
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.76872
evaluation/Rewards Std                  1.80746
evaluation/Rewards Max                 -0.00182193
evaluation/Rewards Min                 -7.05841
evaluation/Returns Mean             -2768.72
evaluation/Returns Std               1740.17
evaluation/Returns Max               -627.467
evaluation/Returns Min              -4192.26
evaluation/Actions Mean                -0.659145
evaluation/Actions Std                  1.0225
evaluation/Actions Max                  3.22171
evaluation/Actions Min                 -4.18672
evaluation/Num Paths                    5
evaluation/Average Returns          -2768.72
time/data storing (s)                   0.000615112
time/evaluation sampling (s)            2.98963
time/exploration real sampling (s)      0.0693436
time/exploration sim sampling (s)       6.13e-06
time/logging (s)                        0.0133948
time/saving (s)                         0.0118358
time/training (s)                      21.4749
time/epoch (s)                         24.5598
time/total (s)                        150.169
Epoch                                   5
----------------------------------  ---------------
2020-02-20 20:45:01.479226 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 6 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.74578
trainer/QF2 Loss                        1.75667
trainer/Policy Loss                    19.9185
trainer/Q1 Predictions Mean           -23.624
trainer/Q1 Predictions Std             14.5421
trainer/Q1 Predictions Max              3.03644
trainer/Q1 Predictions Min            -46.5867
trainer/Q2 Predictions Mean           -23.5729
trainer/Q2 Predictions Std             14.5761
trainer/Q2 Predictions Max              3.1457
trainer/Q2 Predictions Min            -46.5665
trainer/Q Targets Mean                -23.4927
trainer/Q Targets Std                  14.4585
trainer/Q Targets Max                   2.75578
trainer/Q Targets Min                 -45.6577
trainer/Log Pis Mean                    1.6605
trainer/Log Pis Std                     1.73237
trainer/Log Pis Max                     5.23262
trainer/Log Pis Min                    -5.6756
trainer/Policy mu Mean                 -1.3524
trainer/Policy mu Std                   0.480823
trainer/Policy mu Max                  -0.121821
trainer/Policy mu Min                  -2.1047
trainer/Policy log std Mean            -0.641817
trainer/Policy log std Std              0.153737
trainer/Policy log std Max             -0.0715581
trainer/Policy log std Min             -0.884336
trainer/Alpha                           0.290091
trainer/Alpha Loss                     -0.420095
exploration/num steps total           700
exploration/num paths total             7
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.03087
exploration/Rewards Std                 1.34011
exploration/Rewards Max                -0.019115
exploration/Rewards Min                -7.13535
exploration/Returns Mean             -103.087
exploration/Returns Std                 0
exploration/Returns Max              -103.087
exploration/Returns Min              -103.087
exploration/Actions Mean               -0.605767
exploration/Actions Std                 1.01555
exploration/Actions Max                 2.78389
exploration/Actions Min                -3.51033
exploration/Num Paths                   1
exploration/Average Returns          -103.087
evaluation/num steps total          35000
evaluation/num paths total             35
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.450697
evaluation/Rewards Std                  0.604615
evaluation/Rewards Max                 -0.000562805
evaluation/Rewards Min                 -6.6881
evaluation/Returns Mean              -450.697
evaluation/Returns Std                 81.2766
evaluation/Returns Max               -381.989
evaluation/Returns Min               -603.739
evaluation/Actions Mean                -0.660441
evaluation/Actions Std                  1.00658
evaluation/Actions Max                  3.41879
evaluation/Actions Min                 -4.72224
evaluation/Num Paths                    5
evaluation/Average Returns           -450.697
time/data storing (s)                   0.00063033
time/evaluation sampling (s)            2.62007
time/exploration real sampling (s)      0.0722841
time/exploration sim sampling (s)       7.356e-06
time/logging (s)                        0.0142308
time/saving (s)                         0.0133259
time/training (s)                      20.5183
time/epoch (s)                         23.2388
time/total (s)                        173.412
Epoch                                   6
----------------------------------  ---------------
2020-02-20 20:45:27.347925 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 7 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.65299
trainer/QF2 Loss                        1.70846
trainer/Policy Loss                    21.5712
trainer/Q1 Predictions Mean           -24.1763
trainer/Q1 Predictions Std             14.6139
trainer/Q1 Predictions Max              2.10067
trainer/Q1 Predictions Min            -48.7772
trainer/Q2 Predictions Mean           -24.177
trainer/Q2 Predictions Std             14.57
trainer/Q2 Predictions Max              2.10982
trainer/Q2 Predictions Min            -49.0594
trainer/Q Targets Mean                -23.8953
trainer/Q Targets Std                  14.5382
trainer/Q Targets Max                   1.96907
trainer/Q Targets Min                 -47.9459
trainer/Log Pis Mean                    1.88913
trainer/Log Pis Std                     1.75315
trainer/Log Pis Max                     5.3188
trainer/Log Pis Min                    -4.27536
trainer/Policy mu Mean                 -1.40212
trainer/Policy mu Std                   0.485611
trainer/Policy mu Max                  -0.153704
trainer/Policy mu Min                  -2.15401
trainer/Policy log std Mean            -0.643969
trainer/Policy log std Std              0.172329
trainer/Policy log std Max             -0.00912149
trainer/Policy log std Min             -0.924337
trainer/Alpha                           0.247852
trainer/Alpha Loss                     -0.154645
exploration/num steps total           800
exploration/num paths total             8
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.547167
exploration/Rewards Std                 0.812275
exploration/Rewards Max                -0.000994502
exploration/Rewards Min                -3.95257
exploration/Returns Mean              -54.7167
exploration/Returns Std                 0
exploration/Returns Max               -54.7167
exploration/Returns Min               -54.7167
exploration/Actions Mean               -0.750087
exploration/Actions Std                 1.06493
exploration/Actions Max                 2.48504
exploration/Actions Min                -3.3344
exploration/Num Paths                   1
exploration/Average Returns           -54.7167
evaluation/num steps total          40000
evaluation/num paths total             40
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.17352
evaluation/Rewards Std                  1.60902
evaluation/Rewards Max                 -0.000805412
evaluation/Rewards Min                 -7.76393
evaluation/Returns Mean             -1173.52
evaluation/Returns Std               1514.16
evaluation/Returns Max               -370.472
evaluation/Returns Min              -4201.38
evaluation/Actions Mean                -0.708248
evaluation/Actions Std                  1.00454
evaluation/Actions Max                  3.37254
evaluation/Actions Min                 -4.62411
evaluation/Num Paths                    5
evaluation/Average Returns          -1173.52
time/data storing (s)                   0.000644721
time/evaluation sampling (s)            2.73795
time/exploration real sampling (s)      0.0647738
time/exploration sim sampling (s)       6.824e-06
time/logging (s)                        0.0133333
time/saving (s)                         0.012372
time/training (s)                      23.0329
time/epoch (s)                         25.8619
time/total (s)                        199.279
Epoch                                   7
----------------------------------  ---------------
2020-02-20 20:45:51.904466 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 8 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.87908
trainer/QF2 Loss                        1.78764
trainer/Policy Loss                    22.6234
trainer/Q1 Predictions Mean           -25.2705
trainer/Q1 Predictions Std             16.157
trainer/Q1 Predictions Max              3.07868
trainer/Q1 Predictions Min            -53.1809
trainer/Q2 Predictions Mean           -25.3791
trainer/Q2 Predictions Std             16.1294
trainer/Q2 Predictions Max              2.55337
trainer/Q2 Predictions Min            -52.5327
trainer/Q Targets Mean                -25.7241
trainer/Q Targets Std                  16.1342
trainer/Q Targets Max                   1.74717
trainer/Q Targets Min                 -51.9708
trainer/Log Pis Mean                    2.04418
trainer/Log Pis Std                     1.73521
trainer/Log Pis Max                     5.41055
trainer/Log Pis Min                    -2.8998
trainer/Policy mu Mean                 -1.42934
trainer/Policy mu Std                   0.474828
trainer/Policy mu Max                   0.163965
trainer/Policy mu Min                  -2.20517
trainer/Policy log std Mean            -0.674256
trainer/Policy log std Std              0.174994
trainer/Policy log std Max              0.0612559
trainer/Policy log std Min             -0.952117
trainer/Alpha                           0.224559
trainer/Alpha Loss                      0.0659886
exploration/num steps total           900
exploration/num paths total             9
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.846944
exploration/Rewards Std                 1.02903
exploration/Rewards Max                -0.00535988
exploration/Rewards Min                -4.46297
exploration/Returns Mean              -84.6944
exploration/Returns Std                 0
exploration/Returns Max               -84.6944
exploration/Returns Min               -84.6944
exploration/Actions Mean               -0.534073
exploration/Actions Std                 1.05698
exploration/Actions Max                 3.45779
exploration/Actions Min                -2.97225
exploration/Num Paths                   1
exploration/Average Returns           -84.6944
evaluation/num steps total          45000
evaluation/num paths total             45
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.94491
evaluation/Rewards Std                  1.93315
evaluation/Rewards Max                 -0.000159147
evaluation/Rewards Min                 -7.21659
evaluation/Returns Mean             -1944.91
evaluation/Returns Std               1863.92
evaluation/Returns Max               -387.951
evaluation/Returns Min              -4252.43
evaluation/Actions Mean                -0.67767
evaluation/Actions Std                  1.02322
evaluation/Actions Max                  3.84893
evaluation/Actions Min                 -4.48479
evaluation/Num Paths                    5
evaluation/Average Returns          -1944.91
time/data storing (s)                   0.000616827
time/evaluation sampling (s)            2.78786
time/exploration real sampling (s)      0.0662054
time/exploration sim sampling (s)       7.469e-06
time/logging (s)                        0.0133642
time/saving (s)                         0.0122794
time/training (s)                      21.669
time/epoch (s)                         24.5493
time/total (s)                        223.835
Epoch                                   8
----------------------------------  ---------------
2020-02-20 20:46:16.057553 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 9 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.90588
trainer/QF2 Loss                        3.98498
trainer/Policy Loss                    24.8946
trainer/Q1 Predictions Mean           -27.7875
trainer/Q1 Predictions Std             17.5576
trainer/Q1 Predictions Max              1.88728
trainer/Q1 Predictions Min            -57.1553
trainer/Q2 Predictions Mean           -27.7546
trainer/Q2 Predictions Std             17.6369
trainer/Q2 Predictions Max              1.86692
trainer/Q2 Predictions Min            -57.8856
trainer/Q Targets Mean                -27.5413
trainer/Q Targets Std                  17.7245
trainer/Q Targets Max                   1.73714
trainer/Q Targets Min                 -56.0886
trainer/Log Pis Mean                    1.86133
trainer/Log Pis Std                     1.97276
trainer/Log Pis Max                     5.50951
trainer/Log Pis Min                    -6.20913
trainer/Policy mu Mean                 -1.41524
trainer/Policy mu Std                   0.514327
trainer/Policy mu Max                   0.267326
trainer/Policy mu Min                  -2.28291
trainer/Policy log std Mean            -0.651191
trainer/Policy log std Std              0.169623
trainer/Policy log std Max              0.0612979
trainer/Policy log std Min             -0.922256
trainer/Alpha                           0.212909
trainer/Alpha Loss                     -0.21449
exploration/num steps total          1000
exploration/num paths total            10
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.535598
exploration/Rewards Std                 0.629987
exploration/Rewards Max                -0.00311004
exploration/Rewards Min                -3.46584
exploration/Returns Mean              -53.5598
exploration/Returns Std                 0
exploration/Returns Max               -53.5598
exploration/Returns Min               -53.5598
exploration/Actions Mean               -0.667907
exploration/Actions Std                 1.08339
exploration/Actions Max                 2.26613
exploration/Actions Min                -3.02524
exploration/Num Paths                   1
exploration/Average Returns           -53.5598
evaluation/num steps total          50000
evaluation/num paths total             50
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.21819
evaluation/Rewards Std                  1.63115
evaluation/Rewards Max                 -0.000269957
evaluation/Rewards Min                 -6.64606
evaluation/Returns Mean             -1218.19
evaluation/Returns Std               1520.53
evaluation/Returns Max               -379.358
evaluation/Returns Min              -4256.44
evaluation/Actions Mean                -0.655113
evaluation/Actions Std                  1.02319
evaluation/Actions Max                  3.29225
evaluation/Actions Min                 -4.00287
evaluation/Num Paths                    5
evaluation/Average Returns          -1218.19
time/data storing (s)                   0.000628048
time/evaluation sampling (s)            2.87792
time/exploration real sampling (s)      0.0723498
time/exploration sim sampling (s)       6.655e-06
time/logging (s)                        0.0133656
time/saving (s)                         0.0122399
time/training (s)                      21.1703
time/epoch (s)                         24.1468
time/total (s)                        247.987
Epoch                                   9
----------------------------------  ---------------
2020-02-20 20:46:41.058952 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 10 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        2.7545
trainer/QF2 Loss                        2.70903
trainer/Policy Loss                    27.4147
trainer/Q1 Predictions Mean           -30.1969
trainer/Q1 Predictions Std             19.1417
trainer/Q1 Predictions Max              1.94384
trainer/Q1 Predictions Min            -60.6996
trainer/Q2 Predictions Mean           -30.1614
trainer/Q2 Predictions Std             19.1714
trainer/Q2 Predictions Max              1.74044
trainer/Q2 Predictions Min            -60.5858
trainer/Q Targets Mean                -30.1758
trainer/Q Targets Std                  19.3567
trainer/Q Targets Max                   1.52391
trainer/Q Targets Min                 -60.7281
trainer/Log Pis Mean                    2.02522
trainer/Log Pis Std                     1.78525
trainer/Log Pis Max                     5.82592
trainer/Log Pis Min                    -5.51005
trainer/Policy mu Mean                 -1.41943
trainer/Policy mu Std                   0.579547
trainer/Policy mu Max                   0.470804
trainer/Policy mu Min                  -2.39317
trainer/Policy log std Mean            -0.688427
trainer/Policy log std Std              0.190252
trainer/Policy log std Max              0.0561203
trainer/Policy log std Min             -0.999933
trainer/Alpha                           0.200822
trainer/Alpha Loss                      0.0404872
exploration/num steps total          1100
exploration/num paths total            11
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.613224
exploration/Rewards Std                 0.775303
exploration/Rewards Max                -0.00688299
exploration/Rewards Min                -4.97999
exploration/Returns Mean              -61.3224
exploration/Returns Std                 0
exploration/Returns Max               -61.3224
exploration/Returns Min               -61.3224
exploration/Actions Mean               -0.731114
exploration/Actions Std                 1.04448
exploration/Actions Max                 2.41775
exploration/Actions Min                -3.92978
exploration/Num Paths                   1
exploration/Average Returns           -61.3224
evaluation/num steps total          55000
evaluation/num paths total             55
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -1.97658
evaluation/Rewards Std                  1.93486
evaluation/Rewards Max                 -0.000391914
evaluation/Rewards Min                 -7.03798
evaluation/Returns Mean             -1976.58
evaluation/Returns Std               1850.9
evaluation/Returns Max               -429.944
evaluation/Returns Min              -4259.95
evaluation/Actions Mean                -0.633407
evaluation/Actions Std                  1.04164
evaluation/Actions Max                  3.58837
evaluation/Actions Min                 -4.60137
evaluation/Num Paths                    5
evaluation/Average Returns          -1976.58
time/data storing (s)                   0.000657934
time/evaluation sampling (s)            2.87467
time/exploration real sampling (s)      0.0674472
time/exploration sim sampling (s)       7.496e-06
time/logging (s)                        0.0134991
time/saving (s)                         0.0104268
time/training (s)                      22.0287
time/epoch (s)                         24.9954
time/total (s)                        272.988
Epoch                                  10
----------------------------------  ---------------
2020-02-20 20:47:05.516603 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 11 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        2.4574
trainer/QF2 Loss                        2.58045
trainer/Policy Loss                    30.5435
trainer/Q1 Predictions Mean           -32.9787
trainer/Q1 Predictions Std             20.9639
trainer/Q1 Predictions Max              1.95677
trainer/Q1 Predictions Min            -65.3135
trainer/Q2 Predictions Mean           -33.0937
trainer/Q2 Predictions Std             20.9714
trainer/Q2 Predictions Max              1.72053
trainer/Q2 Predictions Min            -66.342
trainer/Q Targets Mean                -33.1671
trainer/Q Targets Std                  21.1255
trainer/Q Targets Max                   1.62812
trainer/Q Targets Min                 -65.1406
trainer/Log Pis Mean                    1.95205
trainer/Log Pis Std                     1.81179
trainer/Log Pis Max                     5.1905
trainer/Log Pis Min                    -3.87696
trainer/Policy mu Mean                 -1.23702
trainer/Policy mu Std                   0.86918
trainer/Policy mu Max                   1.72891
trainer/Policy mu Min                  -2.37087
trainer/Policy log std Mean            -0.663856
trainer/Policy log std Std              0.132313
trainer/Policy log std Max             -0.151174
trainer/Policy log std Min             -0.920441
trainer/Alpha                           0.186365
trainer/Alpha Loss                     -0.0805599
exploration/num steps total          1200
exploration/num paths total            12
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.782787
exploration/Rewards Std                 0.629141
exploration/Rewards Max                -0.0023153
exploration/Rewards Min                -2.53345
exploration/Returns Mean              -78.2787
exploration/Returns Std                 0
exploration/Returns Max               -78.2787
exploration/Returns Min               -78.2787
exploration/Actions Mean               -0.445905
exploration/Actions Std                 1.10778
exploration/Actions Max                 2.40411
exploration/Actions Min                -4.69582
exploration/Num Paths                   1
exploration/Average Returns           -78.2787
evaluation/num steps total          60000
evaluation/num paths total             60
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -2.0076
evaluation/Rewards Std                  1.91164
evaluation/Rewards Max                 -0.000627055
evaluation/Rewards Min                 -6.83804
evaluation/Returns Mean             -2007.6
evaluation/Returns Std               1848.52
evaluation/Returns Max               -459.713
evaluation/Returns Min              -4272.01
evaluation/Actions Mean                -0.556692
evaluation/Actions Std                  1.07937
evaluation/Actions Max                  3.38022
evaluation/Actions Min                 -4.43717
evaluation/Num Paths                    5
evaluation/Average Returns          -2007.6
time/data storing (s)                   0.000606874
time/evaluation sampling (s)            2.90106
time/exploration real sampling (s)      0.0685148
time/exploration sim sampling (s)       6.524e-06
time/logging (s)                        0.0131861
time/saving (s)                         0.012671
time/training (s)                      21.4532
time/epoch (s)                         24.4493
time/total (s)                        297.444
Epoch                                  11
----------------------------------  ---------------
2020-02-20 20:47:28.282782 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 12 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        2.4812
trainer/QF2 Loss                        2.24044
trainer/Policy Loss                    28.2719
trainer/Q1 Predictions Mean           -30.7567
trainer/Q1 Predictions Std             21.9598
trainer/Q1 Predictions Max              1.54893
trainer/Q1 Predictions Min            -70.5751
trainer/Q2 Predictions Mean           -30.7395
trainer/Q2 Predictions Std             21.9577
trainer/Q2 Predictions Max              1.44092
trainer/Q2 Predictions Min            -70.2501
trainer/Q Targets Mean                -30.5013
trainer/Q Targets Std                  22.0851
trainer/Q Targets Max                   1.36854
trainer/Q Targets Min                 -70.1702
trainer/Log Pis Mean                    1.92363
trainer/Log Pis Std                     1.71869
trainer/Log Pis Max                     5.21238
trainer/Log Pis Min                    -4.75934
trainer/Policy mu Mean                 -1.13882
trainer/Policy mu Std                   0.960817
trainer/Policy mu Max                   1.90079
trainer/Policy mu Min                  -2.3724
trainer/Policy log std Mean            -0.673965
trainer/Policy log std Std              0.0980448
trainer/Policy log std Max             -0.300159
trainer/Policy log std Min             -0.852362
trainer/Alpha                           0.188755
trainer/Alpha Loss                     -0.127338
exploration/num steps total          1300
exploration/num paths total            13
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.25991
exploration/Rewards Std                 1.71561
exploration/Rewards Max                -0.00221575
exploration/Rewards Min                -8.72571
exploration/Returns Mean             -125.991
exploration/Returns Std                 0
exploration/Returns Max              -125.991
exploration/Returns Min              -125.991
exploration/Actions Mean               -0.586964
exploration/Actions Std                 1.14136
exploration/Actions Max                 2.20834
exploration/Actions Min                -3.17735
exploration/Num Paths                   1
exploration/Average Returns          -125.991
evaluation/num steps total          65000
evaluation/num paths total             65
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.549857
evaluation/Rewards Std                  0.697596
evaluation/Rewards Max                 -0.000706436
evaluation/Rewards Min                 -7.02836
evaluation/Returns Mean              -549.857
evaluation/Returns Std                 85.0357
evaluation/Returns Max               -484.801
evaluation/Returns Min               -715.867
evaluation/Actions Mean                -0.560048
evaluation/Actions Std                  1.01627
evaluation/Actions Max                  2.99858
evaluation/Actions Min                 -4.71613
evaluation/Num Paths                    5
evaluation/Average Returns           -549.857
time/data storing (s)                   0.000643929
time/evaluation sampling (s)            2.73672
time/exploration real sampling (s)      0.0676171
time/exploration sim sampling (s)       6.527e-06
time/logging (s)                        0.0140575
time/saving (s)                         0.0106645
time/training (s)                      19.9294
time/epoch (s)                         22.7591
time/total (s)                        320.21
Epoch                                  12
----------------------------------  ---------------
2020-02-20 20:47:51.648242 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 13 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        1.88438
trainer/QF2 Loss                        1.55349
trainer/Policy Loss                    34.4537
trainer/Q1 Predictions Mean           -36.8227
trainer/Q1 Predictions Std             24.6483
trainer/Q1 Predictions Max              1.55527
trainer/Q1 Predictions Min            -75.3877
trainer/Q2 Predictions Mean           -36.7672
trainer/Q2 Predictions Std             24.6267
trainer/Q2 Predictions Max              1.38238
trainer/Q2 Predictions Min            -75.1137
trainer/Q Targets Mean                -36.7381
trainer/Q Targets Std                  24.7019
trainer/Q Targets Max                   0.752565
trainer/Q Targets Min                 -75.2882
trainer/Log Pis Mean                    2.2737
trainer/Log Pis Std                     1.7402
trainer/Log Pis Max                     5.58288
trainer/Log Pis Min                    -5.27294
trainer/Policy mu Mean                 -1.15581
trainer/Policy mu Std                   1.08691
trainer/Policy mu Max                   2.07628
trainer/Policy mu Min                  -2.38771
trainer/Policy log std Mean            -0.717328
trainer/Policy log std Std              0.109346
trainer/Policy log std Max             -0.331736
trainer/Policy log std Min             -1.04545
trainer/Alpha                           0.193399
trainer/Alpha Loss                      0.449737
exploration/num steps total          1400
exploration/num paths total            14
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -1.30205
exploration/Rewards Std                 1.49439
exploration/Rewards Max                -0.0176375
exploration/Rewards Min                -7.48999
exploration/Returns Mean             -130.205
exploration/Returns Std                 0
exploration/Returns Max              -130.205
exploration/Returns Min              -130.205
exploration/Actions Mean               -0.46357
exploration/Actions Std                 1.05808
exploration/Actions Max                 2.8648
exploration/Actions Min                -2.97866
exploration/Num Paths                   1
exploration/Average Returns          -130.205
evaluation/num steps total          70000
evaluation/num paths total             70
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.712265
evaluation/Rewards Std                  1.07558
evaluation/Rewards Max                 -0.00114525
evaluation/Rewards Min                 -7.31191
evaluation/Returns Mean              -712.265
evaluation/Returns Std                290.431
evaluation/Returns Max               -461.999
evaluation/Returns Min              -1096.72
evaluation/Actions Mean                -0.538218
evaluation/Actions Std                  1.03756
evaluation/Actions Max                  3.81341
evaluation/Actions Min                 -4.31116
evaluation/Num Paths                    5
evaluation/Average Returns           -712.265
time/data storing (s)                   0.000603695
time/evaluation sampling (s)            2.77515
time/exploration real sampling (s)      0.0686908
time/exploration sim sampling (s)       6.354e-06
time/logging (s)                        0.0140854
time/saving (s)                         0.0111582
time/training (s)                      20.4882
time/epoch (s)                         23.3579
time/total (s)                        343.575
Epoch                                  13
----------------------------------  ---------------
2020-02-20 20:48:14.494314 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 14 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.13885
trainer/QF2 Loss                        3.20658
trainer/Policy Loss                    32.0176
trainer/Q1 Predictions Mean           -34.4883
trainer/Q1 Predictions Std             26.1859
trainer/Q1 Predictions Max              1.1377
trainer/Q1 Predictions Min            -79.9822
trainer/Q2 Predictions Mean           -34.4962
trainer/Q2 Predictions Std             26.1541
trainer/Q2 Predictions Max              0.862655
trainer/Q2 Predictions Min            -81.1249
trainer/Q Targets Mean                -34.4746
trainer/Q Targets Std                  26.4229
trainer/Q Targets Max                   0.888718
trainer/Q Targets Min                 -80.9999
trainer/Log Pis Mean                    1.91463
trainer/Log Pis Std                     1.73184
trainer/Log Pis Max                     5.16865
trainer/Log Pis Min                    -6.42633
trainer/Policy mu Mean                 -1.0411
trainer/Policy mu Std                   1.03768
trainer/Policy mu Max                   2.06306
trainer/Policy mu Min                  -2.21151
trainer/Policy log std Mean            -0.716355
trainer/Policy log std Std              0.132759
trainer/Policy log std Max             -0.287621
trainer/Policy log std Min             -1.16762
trainer/Alpha                           0.192924
trainer/Alpha Loss                     -0.140479
exploration/num steps total          1500
exploration/num paths total            15
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.970827
exploration/Rewards Std                 1.21294
exploration/Rewards Max                -0.00164108
exploration/Rewards Min                -6.18925
exploration/Returns Mean              -97.0827
exploration/Returns Std                 0
exploration/Returns Max               -97.0827
exploration/Returns Min               -97.0827
exploration/Actions Mean               -0.511378
exploration/Actions Std                 1.14202
exploration/Actions Max                 2.86456
exploration/Actions Min                -3.15656
exploration/Num Paths                   1
exploration/Average Returns           -97.0827
evaluation/num steps total          75000
evaluation/num paths total             75
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.64665
evaluation/Rewards Std                  0.96578
evaluation/Rewards Max                 -0.000950986
evaluation/Rewards Min                 -7.586
evaluation/Returns Mean              -646.65
evaluation/Returns Std                245.407
evaluation/Returns Max               -477.948
evaluation/Returns Min              -1133.26
evaluation/Actions Mean                -0.537725
evaluation/Actions Std                  1.03497
evaluation/Actions Max                  3.60737
evaluation/Actions Min                 -4.68904
evaluation/Num Paths                    5
evaluation/Average Returns           -646.65
time/data storing (s)                   0.00060394
time/evaluation sampling (s)            2.71483
time/exploration real sampling (s)      0.0678283
time/exploration sim sampling (s)       6.002e-06
time/logging (s)                        0.0141516
time/saving (s)                         0.0108854
time/training (s)                      20.0297
time/epoch (s)                         22.838
time/total (s)                        366.42
Epoch                                  14
----------------------------------  ---------------
2020-02-20 20:48:38.611999 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 15 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        3.43509
trainer/QF2 Loss                        3.41723
trainer/Policy Loss                    36.786
trainer/Q1 Predictions Mean           -39.3211
trainer/Q1 Predictions Std             28.884
trainer/Q1 Predictions Max              1.27319
trainer/Q1 Predictions Min            -86.4871
trainer/Q2 Predictions Mean           -39.4116
trainer/Q2 Predictions Std             28.8545
trainer/Q2 Predictions Max              0.628251
trainer/Q2 Predictions Min            -86.6361
trainer/Q Targets Mean                -39.8285
trainer/Q Targets Std                  29.0909
trainer/Q Targets Max                   0.540358
trainer/Q Targets Min                 -86.4686
trainer/Log Pis Mean                    1.77476
trainer/Log Pis Std                     1.70671
trainer/Log Pis Max                     4.90584
trainer/Log Pis Min                    -2.96486
trainer/Policy mu Mean                 -0.909526
trainer/Policy mu Std                   1.12381
trainer/Policy mu Max                   2.31525
trainer/Policy mu Min                  -2.2385
trainer/Policy log std Mean            -0.709857
trainer/Policy log std Std              0.160126
trainer/Policy log std Max             -0.22295
trainer/Policy log std Min             -1.34852
trainer/Alpha                           0.188795
trainer/Alpha Loss                     -0.375485
exploration/num steps total          1600
exploration/num paths total            16
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.781829
exploration/Rewards Std                 0.826329
exploration/Rewards Max                -0.0136273
exploration/Rewards Min                -4.92827
exploration/Returns Mean              -78.1829
exploration/Returns Std                 0
exploration/Returns Max               -78.1829
exploration/Returns Min               -78.1829
exploration/Actions Mean               -0.534392
exploration/Actions Std                 1.11164
exploration/Actions Max                 2.30695
exploration/Actions Min                -3.68811
exploration/Num Paths                   1
exploration/Average Returns           -78.1829
evaluation/num steps total          80000
evaluation/num paths total             80
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.766728
evaluation/Rewards Std                  1.15109
evaluation/Rewards Max                 -0.000819148
evaluation/Rewards Min                 -7.35974
evaluation/Returns Mean              -766.728
evaluation/Returns Std                225.635
evaluation/Returns Max               -497.346
evaluation/Returns Min              -1170.62
evaluation/Actions Mean                -0.511522
evaluation/Actions Std                  1.04043
evaluation/Actions Max                  3.72616
evaluation/Actions Min                 -4.2441
evaluation/Num Paths                    5
evaluation/Average Returns           -766.728
time/data storing (s)                   0.000897711
time/evaluation sampling (s)            3.29196
time/exploration real sampling (s)      0.0822493
time/exploration sim sampling (s)       6.548e-06
time/logging (s)                        0.013786
time/saving (s)                         0.011471
time/training (s)                      20.7093
time/epoch (s)                         24.1096
time/total (s)                        390.536
Epoch                                  15
----------------------------------  ---------------
2020-02-20 20:49:01.435561 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 16 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        4.71182
trainer/QF2 Loss                        4.89442
trainer/Policy Loss                    37.909
trainer/Q1 Predictions Mean           -40.6807
trainer/Q1 Predictions Std             28.6925
trainer/Q1 Predictions Max              0.634909
trainer/Q1 Predictions Min            -92.2377
trainer/Q2 Predictions Mean           -40.563
trainer/Q2 Predictions Std             28.5905
trainer/Q2 Predictions Max              0.355106
trainer/Q2 Predictions Min            -93.0399
trainer/Q Targets Mean                -40.5746
trainer/Q Targets Std                  28.7848
trainer/Q Targets Max                   0.19003
trainer/Q Targets Min                 -90.278
trainer/Log Pis Mean                    1.99958
trainer/Log Pis Std                     1.72865
trainer/Log Pis Max                     5.18687
trainer/Log Pis Min                    -4.72135
trainer/Policy mu Mean                 -0.921151
trainer/Policy mu Std                   1.16488
trainer/Policy mu Max                   2.5039
trainer/Policy mu Min                  -2.16355
trainer/Policy log std Mean            -0.717674
trainer/Policy log std Std              0.191411
trainer/Policy log std Max             -0.177153
trainer/Policy log std Min             -1.46667
trainer/Alpha                           0.190748
trainer/Alpha Loss                     -0.000694352
exploration/num steps total          1700
exploration/num paths total            17
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.577591
exploration/Rewards Std                 0.563009
exploration/Rewards Max                -0.0150468
exploration/Rewards Min                -2.77517
exploration/Returns Mean              -57.7591
exploration/Returns Std                 0
exploration/Returns Max               -57.7591
exploration/Returns Min               -57.7591
exploration/Actions Mean               -0.593991
exploration/Actions Std                 1.05524
exploration/Actions Max                 2.24196
exploration/Actions Min                -3.23922
exploration/Num Paths                   1
exploration/Average Returns           -57.7591
evaluation/num steps total          85000
evaluation/num paths total             85
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.538376
evaluation/Rewards Std                  0.694238
evaluation/Rewards Max                 -0.000376178
evaluation/Rewards Min                 -7.19323
evaluation/Returns Mean              -538.376
evaluation/Returns Std                 42.2623
evaluation/Returns Max               -498.82
evaluation/Returns Min               -612.144
evaluation/Actions Mean                -0.557877
evaluation/Actions Std                  1.02067
evaluation/Actions Max                  3.27905
evaluation/Actions Min                 -4.56069
evaluation/Num Paths                    5
evaluation/Average Returns           -538.376
time/data storing (s)                   0.000608427
time/evaluation sampling (s)            2.81448
time/exploration real sampling (s)      0.0673764
time/exploration sim sampling (s)       7.739e-06
time/logging (s)                        0.0130396
time/saving (s)                         0.013738
time/training (s)                      19.9057
time/epoch (s)                         22.8149
time/total (s)                        413.358
Epoch                                  16
----------------------------------  ---------------
2020-02-20 20:49:25.023574 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 17 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        6.28257
trainer/QF2 Loss                        5.16782
trainer/Policy Loss                    35.5503
trainer/Q1 Predictions Mean           -38.0503
trainer/Q1 Predictions Std             29.0339
trainer/Q1 Predictions Max              0.460681
trainer/Q1 Predictions Min            -95.1047
trainer/Q2 Predictions Mean           -37.9481
trainer/Q2 Predictions Std             28.9434
trainer/Q2 Predictions Max              0.133449
trainer/Q2 Predictions Min            -96.7468
trainer/Q Targets Mean                -37.3883
trainer/Q Targets Std                  28.9147
trainer/Q Targets Max                  -0.125294
trainer/Q Targets Min                 -92.3653
trainer/Log Pis Mean                    2.2413
trainer/Log Pis Std                     1.72005
trainer/Log Pis Max                     5.54495
trainer/Log Pis Min                    -3.07457
trainer/Policy mu Mean                 -1.07726
trainer/Policy mu Std                   1.11557
trainer/Policy mu Max                   2.19953
trainer/Policy mu Min                  -2.32347
trainer/Policy log std Mean            -0.728616
trainer/Policy log std Std              0.203162
trainer/Policy log std Max             -0.15702
trainer/Policy log std Min             -1.58542
trainer/Alpha                           0.200746
trainer/Alpha Loss                      0.387476
exploration/num steps total          1800
exploration/num paths total            18
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.451258
exploration/Rewards Std                 0.470744
exploration/Rewards Max                -0.002482
exploration/Rewards Min                -1.83866
exploration/Returns Mean              -45.1258
exploration/Returns Std                 0
exploration/Returns Max               -45.1258
exploration/Returns Min               -45.1258
exploration/Actions Mean               -0.670483
exploration/Actions Std                 1.0775
exploration/Actions Max                 2.12241
exploration/Actions Min                -3.68599
exploration/Num Paths                   1
exploration/Average Returns           -45.1258
evaluation/num steps total          90000
evaluation/num paths total             90
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.629832
evaluation/Rewards Std                  0.901162
evaluation/Rewards Max                 -0.000589756
evaluation/Rewards Min                 -8.38838
evaluation/Returns Mean              -629.832
evaluation/Returns Std                175.106
evaluation/Returns Max               -477.754
evaluation/Returns Min               -931.708
evaluation/Actions Mean                -0.613659
evaluation/Actions Std                  1.00158
evaluation/Actions Max                  2.85278
evaluation/Actions Min                 -4.5651
evaluation/Num Paths                    5
evaluation/Average Returns           -629.832
time/data storing (s)                   0.000619467
time/evaluation sampling (s)            2.68055
time/exploration real sampling (s)      0.0632685
time/exploration sim sampling (s)       6.571e-06
time/logging (s)                        0.0129933
time/saving (s)                         0.0137639
time/training (s)                      20.8086
time/epoch (s)                         23.5798
time/total (s)                        436.945
Epoch                                  17
----------------------------------  ---------------
2020-02-20 20:49:48.954274 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 18 finished
----------------------------------  ---------------
replay_buffer/size                      0
trainer/QF1 Loss                        2.57378
trainer/QF2 Loss                        2.51408
trainer/Policy Loss                    35.6283
trainer/Q1 Predictions Mean           -38.3708
trainer/Q1 Predictions Std             28.5748
trainer/Q1 Predictions Max             -0.271627
trainer/Q1 Predictions Min            -95.2485
trainer/Q2 Predictions Mean           -38.2759
trainer/Q2 Predictions Std             28.5028
trainer/Q2 Predictions Max             -0.320892
trainer/Q2 Predictions Min            -95.7928
trainer/Q Targets Mean                -38.1336
trainer/Q Targets Std                  28.6333
trainer/Q Targets Max                  -0.176171
trainer/Q Targets Min                 -92.4231
trainer/Log Pis Mean                    1.93224
trainer/Log Pis Std                     1.68747
trainer/Log Pis Max                     4.99202
trainer/Log Pis Min                    -6.56983
trainer/Policy mu Mean                 -0.923908
trainer/Policy mu Std                   1.16752
trainer/Policy mu Max                   2.43711
trainer/Policy mu Min                  -2.24687
trainer/Policy log std Mean            -0.706026
trainer/Policy log std Std              0.188453
trainer/Policy log std Max             -0.238116
trainer/Policy log std Min             -1.61354
trainer/Alpha                           0.196106
trainer/Alpha Loss                     -0.110385
exploration/num steps total          1900
exploration/num paths total            19
exploration/path length Mean          100
exploration/path length Std             0
exploration/path length Max           100
exploration/path length Min           100
exploration/Rewards Mean               -0.86747
exploration/Rewards Std                 1.14968
exploration/Rewards Max                -0.00418178
exploration/Rewards Min                -5.83352
exploration/Returns Mean              -86.747
exploration/Returns Std                 0
exploration/Returns Max               -86.747
exploration/Returns Min               -86.747
exploration/Actions Mean               -0.480821
exploration/Actions Std                 1.12079
exploration/Actions Max                 3.84935
exploration/Actions Min                -3.53913
exploration/Num Paths                   1
exploration/Average Returns           -86.747
evaluation/num steps total          95000
evaluation/num paths total             95
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                -0.623312
evaluation/Rewards Std                  0.826495
evaluation/Rewards Max                 -0.000302472
evaluation/Rewards Min                 -7.41595
evaluation/Returns Mean              -623.312
evaluation/Returns Std                 88.1465
evaluation/Returns Max               -533.131
evaluation/Returns Min               -767.763
evaluation/Actions Mean                -0.583987
evaluation/Actions Std                  1.02415
evaluation/Actions Max                  3.4494
evaluation/Actions Min                 -4.92583
evaluation/Num Paths                    5
evaluation/Average Returns           -623.312
time/data storing (s)                   0.000610508
time/evaluation sampling (s)            2.68359
time/exploration real sampling (s)      0.0656675
time/exploration sim sampling (s)       6.6e-06
time/logging (s)                        0.0131635
time/saving (s)                         0.0139868
time/training (s)                      21.1457
time/epoch (s)                         23.9227
time/total (s)                        460.875
Epoch                                  18
----------------------------------  ---------------
2020-02-20 20:50:12.788394 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 19 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         4.90651
trainer/QF2 Loss                         4.89845
trainer/Policy Loss                     36.9206
trainer/Q1 Predictions Mean            -39.6221
trainer/Q1 Predictions Std              30.5266
trainer/Q1 Predictions Max              -1.00342
trainer/Q1 Predictions Min             -95.0973
trainer/Q2 Predictions Mean            -39.4348
trainer/Q2 Predictions Std              30.631
trainer/Q2 Predictions Max              -0.264792
trainer/Q2 Predictions Min             -95.6839
trainer/Q Targets Mean                 -39.0572
trainer/Q Targets Std                   30.7065
trainer/Q Targets Max                   -0.662457
trainer/Q Targets Min                  -93.5428
trainer/Log Pis Mean                     2.11202
trainer/Log Pis Std                      1.69313
trainer/Log Pis Max                      5.35529
trainer/Log Pis Min                     -5.6506
trainer/Policy mu Mean                  -1.0361
trainer/Policy mu Std                    1.14369
trainer/Policy mu Max                    2.16678
trainer/Policy mu Min                   -2.4891
trainer/Policy log std Mean             -0.711644
trainer/Policy log std Std               0.202839
trainer/Policy log std Max              -0.152309
trainer/Policy log std Min              -1.6058
trainer/Alpha                            0.187611
trainer/Alpha Loss                       0.187445
exploration/num steps total           2000
exploration/num paths total             20
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.19813
exploration/Rewards Std                  1.7999
exploration/Rewards Max                 -0.00302868
exploration/Rewards Min                 -7.86509
exploration/Returns Mean              -119.813
exploration/Returns Std                  0
exploration/Returns Max               -119.813
exploration/Returns Min               -119.813
exploration/Actions Mean                -0.581966
exploration/Actions Std                  1.07996
exploration/Actions Max                  2.62339
exploration/Actions Min                 -4.03935
exploration/Num Paths                    1
exploration/Average Returns           -119.813
evaluation/num steps total          100000
evaluation/num paths total             100
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.524217
evaluation/Rewards Std                   0.704993
evaluation/Rewards Max                  -0.000223944
evaluation/Rewards Min                  -7.18342
evaluation/Returns Mean               -524.217
evaluation/Returns Std                  88.8492
evaluation/Returns Max                -456.883
evaluation/Returns Min                -691.75
evaluation/Actions Mean                 -0.590713
evaluation/Actions Std                   1.03677
evaluation/Actions Max                   3.83383
evaluation/Actions Min                  -4.14156
evaluation/Num Paths                     5
evaluation/Average Returns            -524.217
time/data storing (s)                    0.000634184
time/evaluation sampling (s)             2.73442
time/exploration real sampling (s)       0.0681595
time/exploration sim sampling (s)        6.369e-06
time/logging (s)                         0.0141339
time/saving (s)                          0.0114778
time/training (s)                       20.9977
time/epoch (s)                          23.8265
time/total (s)                         484.709
Epoch                                   19
----------------------------------  ----------------
2020-02-20 20:50:36.454722 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 20 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         3.4584
trainer/QF2 Loss                         3.30724
trainer/Policy Loss                     36.1398
trainer/Q1 Predictions Mean            -38.7337
trainer/Q1 Predictions Std              30.3493
trainer/Q1 Predictions Max              -0.580056
trainer/Q1 Predictions Min             -95.4442
trainer/Q2 Predictions Mean            -38.5156
trainer/Q2 Predictions Std              30.4563
trainer/Q2 Predictions Max              -1.00751
trainer/Q2 Predictions Min             -95.7178
trainer/Q Targets Mean                 -38.4717
trainer/Q Targets Std                   30.4778
trainer/Q Targets Max                   -0.847805
trainer/Q Targets Min                  -94.9929
trainer/Log Pis Mean                     1.90896
trainer/Log Pis Std                      1.97779
trainer/Log Pis Max                      5.54549
trainer/Log Pis Min                     -6.00766
trainer/Policy mu Mean                  -0.935222
trainer/Policy mu Std                    1.17892
trainer/Policy mu Max                    2.33498
trainer/Policy mu Min                   -2.44779
trainer/Policy log std Mean             -0.705521
trainer/Policy log std Std               0.21957
trainer/Policy log std Max              -0.134
trainer/Policy log std Min              -1.62926
trainer/Alpha                            0.181231
trainer/Alpha Loss                      -0.15549
exploration/num steps total           2100
exploration/num paths total             21
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.742317
exploration/Rewards Std                  1.03608
exploration/Rewards Max                 -0.0069593
exploration/Rewards Min                 -5.58924
exploration/Returns Mean               -74.2317
exploration/Returns Std                  0
exploration/Returns Max                -74.2317
exploration/Returns Min                -74.2317
exploration/Actions Mean                -0.633984
exploration/Actions Std                  1.04906
exploration/Actions Max                  2.08092
exploration/Actions Min                 -3.27578
exploration/Num Paths                    1
exploration/Average Returns            -74.2317
evaluation/num steps total          105000
evaluation/num paths total             105
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.51247
evaluation/Rewards Std                   0.669155
evaluation/Rewards Max                  -0.000474627
evaluation/Rewards Min                  -8.44833
evaluation/Returns Mean               -512.47
evaluation/Returns Std                  87.2381
evaluation/Returns Max                -417.349
evaluation/Returns Min                -654.601
evaluation/Actions Mean                 -0.581596
evaluation/Actions Std                   1.03058
evaluation/Actions Max                   3.51353
evaluation/Actions Min                  -4.87614
evaluation/Num Paths                     5
evaluation/Average Returns            -512.47
time/data storing (s)                    0.000630083
time/evaluation sampling (s)             2.67549
time/exploration real sampling (s)       0.066002
time/exploration sim sampling (s)        6.769e-06
time/logging (s)                         0.012867
time/saving (s)                          0.013821
time/training (s)                       20.8884
time/epoch (s)                          23.6572
time/total (s)                         508.373
Epoch                                   20
----------------------------------  ----------------
2020-02-20 20:51:01.402462 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 21 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.43524
trainer/QF2 Loss                         2.48465
trainer/Policy Loss                     38.182
trainer/Q1 Predictions Mean            -40.7426
trainer/Q1 Predictions Std              31.8193
trainer/Q1 Predictions Max              -0.797947
trainer/Q1 Predictions Min             -98.2176
trainer/Q2 Predictions Mean            -40.5439
trainer/Q2 Predictions Std              31.7877
trainer/Q2 Predictions Max              -0.918316
trainer/Q2 Predictions Min            -100.461
trainer/Q Targets Mean                 -40.3779
trainer/Q Targets Std                   31.8817
trainer/Q Targets Max                   -1.28655
trainer/Q Targets Min                  -95.9256
trainer/Log Pis Mean                     1.92684
trainer/Log Pis Std                      1.88561
trainer/Log Pis Max                      5.72901
trainer/Log Pis Min                     -3.12985
trainer/Policy mu Mean                  -0.962236
trainer/Policy mu Std                    1.18539
trainer/Policy mu Max                    2.46855
trainer/Policy mu Min                   -2.53323
trainer/Policy log std Mean             -0.70753
trainer/Policy log std Std               0.203561
trainer/Policy log std Max              -0.12864
trainer/Policy log std Min              -1.58677
trainer/Alpha                            0.179352
trainer/Alpha Loss                      -0.125715
exploration/num steps total           2200
exploration/num paths total             22
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.3322
exploration/Rewards Std                  1.65442
exploration/Rewards Max                 -0.0152059
exploration/Rewards Min                 -7.71571
exploration/Returns Mean              -133.22
exploration/Returns Std                  0
exploration/Returns Max               -133.22
exploration/Returns Min               -133.22
exploration/Actions Mean                -0.52309
exploration/Actions Std                  1.15217
exploration/Actions Max                  2.80728
exploration/Actions Min                 -3.65302
exploration/Num Paths                    1
exploration/Average Returns           -133.22
evaluation/num steps total          110000
evaluation/num paths total             110
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.564665
evaluation/Rewards Std                   0.776146
evaluation/Rewards Max                  -0.0010978
evaluation/Rewards Min                  -7.3225
evaluation/Returns Mean               -564.665
evaluation/Returns Std                  44.2479
evaluation/Returns Max                -509.233
evaluation/Returns Min                -640.7
evaluation/Actions Mean                 -0.553799
evaluation/Actions Std                   1.02819
evaluation/Actions Max                   3.64833
evaluation/Actions Min                  -4.69603
evaluation/Num Paths                     5
evaluation/Average Returns            -564.665
time/data storing (s)                    0.000602225
time/evaluation sampling (s)             2.70129
time/exploration real sampling (s)       0.0685155
time/exploration sim sampling (s)        6.723e-06
time/logging (s)                         0.0135862
time/saving (s)                          0.0199843
time/training (s)                       22.1362
time/epoch (s)                          24.9402
time/total (s)                         533.32
Epoch                                   21
----------------------------------  ----------------
2020-02-20 20:51:26.495023 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 22 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         3.37775
trainer/QF2 Loss                         2.82206
trainer/Policy Loss                     34.3617
trainer/Q1 Predictions Mean            -36.5764
trainer/Q1 Predictions Std              30.2466
trainer/Q1 Predictions Max               2.20971
trainer/Q1 Predictions Min             -97.2896
trainer/Q2 Predictions Mean            -36.9073
trainer/Q2 Predictions Std              30.2402
trainer/Q2 Predictions Max               0.0510584
trainer/Q2 Predictions Min             -97.6887
trainer/Q Targets Mean                 -37.0452
trainer/Q Targets Std                   30.5692
trainer/Q Targets Max                   -1.45165
trainer/Q Targets Min                  -96.806
trainer/Log Pis Mean                     1.93576
trainer/Log Pis Std                      1.81468
trainer/Log Pis Max                      5.83816
trainer/Log Pis Min                     -3.6935
trainer/Policy mu Mean                  -0.879056
trainer/Policy mu Std                    1.22276
trainer/Policy mu Max                    2.42501
trainer/Policy mu Min                   -2.63038
trainer/Policy log std Mean             -0.740317
trainer/Policy log std Std               0.20442
trainer/Policy log std Max              -0.182431
trainer/Policy log std Min              -1.654
trainer/Alpha                            0.182746
trainer/Alpha Loss                      -0.109175
exploration/num steps total           2300
exploration/num paths total             23
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.745357
exploration/Rewards Std                  0.664991
exploration/Rewards Max                 -0.00566529
exploration/Rewards Min                 -2.60549
exploration/Returns Mean               -74.5357
exploration/Returns Std                  0
exploration/Returns Max                -74.5357
exploration/Returns Min                -74.5357
exploration/Actions Mean                -0.488571
exploration/Actions Std                  0.982861
exploration/Actions Max                  3.46088
exploration/Actions Min                 -2.91637
exploration/Num Paths                    1
exploration/Average Returns            -74.5357
evaluation/num steps total          115000
evaluation/num paths total             115
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.565887
evaluation/Rewards Std                   0.715561
evaluation/Rewards Max                  -0.000682371
evaluation/Rewards Min                  -7.99142
evaluation/Returns Mean               -565.887
evaluation/Returns Std                  56.2091
evaluation/Returns Max                -493.69
evaluation/Returns Min                -664.295
evaluation/Actions Mean                 -0.541686
evaluation/Actions Std                   1.0443
evaluation/Actions Max                   3.85503
evaluation/Actions Min                  -4.43068
evaluation/Num Paths                     5
evaluation/Average Returns            -565.887
time/data storing (s)                    0.000615417
time/evaluation sampling (s)             2.7017
time/exploration real sampling (s)       0.0663832
time/exploration sim sampling (s)        6.716e-06
time/logging (s)                         0.012825
time/saving (s)                          0.0140796
time/training (s)                       22.2884
time/epoch (s)                          25.084
time/total (s)                         558.411
Epoch                                   22
----------------------------------  ----------------
2020-02-20 20:51:51.000857 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 23 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                        12.9906
trainer/QF2 Loss                        12.9708
trainer/Policy Loss                     36.1512
trainer/Q1 Predictions Mean            -38.3448
trainer/Q1 Predictions Std              31.8322
trainer/Q1 Predictions Max              -2.10586
trainer/Q1 Predictions Min             -97.9459
trainer/Q2 Predictions Mean            -38.2787
trainer/Q2 Predictions Std              31.8497
trainer/Q2 Predictions Max              -1.95546
trainer/Q2 Predictions Min             -98.1446
trainer/Q Targets Mean                 -37.8059
trainer/Q Targets Std                   31.8057
trainer/Q Targets Max                   -1.56047
trainer/Q Targets Min                  -96.9488
trainer/Log Pis Mean                     2.04146
trainer/Log Pis Std                      1.91638
trainer/Log Pis Max                      5.52659
trainer/Log Pis Min                     -3.83354
trainer/Policy mu Mean                  -1.01246
trainer/Policy mu Std                    1.17954
trainer/Policy mu Max                    2.31891
trainer/Policy mu Min                   -2.32472
trainer/Policy log std Mean             -0.695167
trainer/Policy log std Std               0.132595
trainer/Policy log std Max              -0.185385
trainer/Policy log std Min              -1.50503
trainer/Alpha                            0.180548
trainer/Alpha Loss                       0.0709759
exploration/num steps total           2400
exploration/num paths total             24
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.11102
exploration/Rewards Std                  1.28717
exploration/Rewards Max                 -0.00584944
exploration/Rewards Min                 -6.03538
exploration/Returns Mean              -111.102
exploration/Returns Std                  0
exploration/Returns Max               -111.102
exploration/Returns Min               -111.102
exploration/Actions Mean                -0.460642
exploration/Actions Std                  1.09921
exploration/Actions Max                  2.7983
exploration/Actions Min                 -3.18209
exploration/Num Paths                    1
exploration/Average Returns           -111.102
evaluation/num steps total          120000
evaluation/num paths total             120
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.584964
evaluation/Rewards Std                   0.873853
evaluation/Rewards Max                  -0.00041114
evaluation/Rewards Min                  -7.79234
evaluation/Returns Mean               -584.964
evaluation/Returns Std                  76.0153
evaluation/Returns Max                -446.682
evaluation/Returns Min                -672.427
evaluation/Actions Mean                 -0.593699
evaluation/Actions Std                   1.03831
evaluation/Actions Max                   3.44293
evaluation/Actions Min                  -4.30044
evaluation/Num Paths                     5
evaluation/Average Returns            -584.964
time/data storing (s)                    0.000620224
time/evaluation sampling (s)             2.65486
time/exploration real sampling (s)       0.0674067
time/exploration sim sampling (s)        6.41e-06
time/logging (s)                         0.013014
time/saving (s)                          0.0145934
time/training (s)                       21.7475
time/epoch (s)                          24.498
time/total (s)                         582.916
Epoch                                   23
----------------------------------  ----------------
2020-02-20 20:52:15.451599 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 24 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.9355
trainer/QF2 Loss                         2.60413
trainer/Policy Loss                     35.299
trainer/Q1 Predictions Mean            -37.7331
trainer/Q1 Predictions Std              31.6448
trainer/Q1 Predictions Max              -1.81399
trainer/Q1 Predictions Min             -97.9997
trainer/Q2 Predictions Mean            -37.5212
trainer/Q2 Predictions Std              31.5058
trainer/Q2 Predictions Max              -1.78054
trainer/Q2 Predictions Min             -99.2567
trainer/Q Targets Mean                 -37.2414
trainer/Q Targets Std                   31.6229
trainer/Q Targets Max                   -1.67631
trainer/Q Targets Min                  -97.2834
trainer/Log Pis Mean                     2.0606
trainer/Log Pis Std                      1.86751
trainer/Log Pis Max                      5.58924
trainer/Log Pis Min                     -5.35768
trainer/Policy mu Mean                  -0.913935
trainer/Policy mu Std                    1.21221
trainer/Policy mu Max                    2.37391
trainer/Policy mu Min                   -2.57146
trainer/Policy log std Mean             -0.736526
trainer/Policy log std Std               0.179021
trainer/Policy log std Max              -0.197183
trainer/Policy log std Min              -1.7041
trainer/Alpha                            0.178073
trainer/Alpha Loss                       0.10456
exploration/num steps total           2500
exploration/num paths total             25
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.22352
exploration/Rewards Std                  1.50713
exploration/Rewards Max                 -0.00816355
exploration/Rewards Min                 -6.39708
exploration/Returns Mean              -122.352
exploration/Returns Std                  0
exploration/Returns Max               -122.352
exploration/Returns Min               -122.352
exploration/Actions Mean                -0.507528
exploration/Actions Std                  1.09287
exploration/Actions Max                  2.261
exploration/Actions Min                 -3.19013
exploration/Num Paths                    1
exploration/Average Returns           -122.352
evaluation/num steps total          125000
evaluation/num paths total             125
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.571803
evaluation/Rewards Std                   0.825418
evaluation/Rewards Max                  -4.4215e-05
evaluation/Rewards Min                  -8.48045
evaluation/Returns Mean               -571.803
evaluation/Returns Std                  81.7728
evaluation/Returns Max                -439.653
evaluation/Returns Min                -691.175
evaluation/Actions Mean                 -0.560132
evaluation/Actions Std                   1.04528
evaluation/Actions Max                   3.8597
evaluation/Actions Min                  -4.81352
evaluation/Num Paths                     5
evaluation/Average Returns            -571.803
time/data storing (s)                    0.000625937
time/evaluation sampling (s)             2.72171
time/exploration real sampling (s)       0.0678021
time/exploration sim sampling (s)        6.21e-06
time/logging (s)                         0.0128812
time/saving (s)                          0.0143237
time/training (s)                       21.6253
time/epoch (s)                          24.4426
time/total (s)                         607.365
Epoch                                   24
----------------------------------  ----------------
2020-02-20 20:52:39.624787 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 25 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.28449
trainer/QF2 Loss                         2.50154
trainer/Policy Loss                     36.0408
trainer/Q1 Predictions Mean            -38.4227
trainer/Q1 Predictions Std              31.6933
trainer/Q1 Predictions Max              -1.93481
trainer/Q1 Predictions Min             -98.9368
trainer/Q2 Predictions Mean            -38.4221
trainer/Q2 Predictions Std              31.9033
trainer/Q2 Predictions Max              -1.86784
trainer/Q2 Predictions Min            -100.344
trainer/Q Targets Mean                 -38.1591
trainer/Q Targets Std                   31.3332
trainer/Q Targets Max                   -1.95359
trainer/Q Targets Min                  -97.4369
trainer/Log Pis Mean                     1.77511
trainer/Log Pis Std                      1.89859
trainer/Log Pis Max                      5.7103
trainer/Log Pis Min                     -4.2107
trainer/Policy mu Mean                  -0.815449
trainer/Policy mu Std                    1.25073
trainer/Policy mu Max                    2.33145
trainer/Policy mu Min                   -2.49179
trainer/Policy log std Mean             -0.711971
trainer/Policy log std Std               0.176455
trainer/Policy log std Max              -0.177454
trainer/Policy log std Min              -1.67281
trainer/Alpha                            0.177296
trainer/Alpha Loss                      -0.389055
exploration/num steps total           2600
exploration/num paths total             26
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.646515
exploration/Rewards Std                  0.710802
exploration/Rewards Max                 -0.00201011
exploration/Rewards Min                 -3.63693
exploration/Returns Mean               -64.6515
exploration/Returns Std                  0
exploration/Returns Max                -64.6515
exploration/Returns Min                -64.6515
exploration/Actions Mean                -0.515322
exploration/Actions Std                  1.05209
exploration/Actions Max                  2.15567
exploration/Actions Min                 -3.1529
exploration/Num Paths                    1
exploration/Average Returns            -64.6515
evaluation/num steps total          130000
evaluation/num paths total             130
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.547101
evaluation/Rewards Std                   0.692552
evaluation/Rewards Max                  -0.000861833
evaluation/Rewards Min                  -7.64246
evaluation/Returns Mean               -547.101
evaluation/Returns Std                  49.5157
evaluation/Returns Max                -485.04
evaluation/Returns Min                -625.273
evaluation/Actions Mean                 -0.522629
evaluation/Actions Std                   1.0506
evaluation/Actions Max                   3.35781
evaluation/Actions Min                  -4.7621
evaluation/Num Paths                     5
evaluation/Average Returns            -547.101
time/data storing (s)                    0.000614334
time/evaluation sampling (s)             2.70853
time/exploration real sampling (s)       0.0657194
time/exploration sim sampling (s)        7.075e-06
time/logging (s)                         0.0141486
time/saving (s)                          0.0123212
time/training (s)                       21.3649
time/epoch (s)                          24.1663
time/total (s)                         631.539
Epoch                                   25
----------------------------------  ----------------
2020-02-20 20:53:04.474434 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 26 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         4.21537
trainer/QF2 Loss                         4.30037
trainer/Policy Loss                     35.4716
trainer/Q1 Predictions Mean            -37.9474
trainer/Q1 Predictions Std              30.6888
trainer/Q1 Predictions Max              -2.08224
trainer/Q1 Predictions Min             -98.2284
trainer/Q2 Predictions Mean            -37.9163
trainer/Q2 Predictions Std              30.8113
trainer/Q2 Predictions Max              -2.3555
trainer/Q2 Predictions Min             -99.8565
trainer/Q Targets Mean                 -37.3049
trainer/Q Targets Std                   30.481
trainer/Q Targets Max                   -2.26943
trainer/Q Targets Min                  -97.1962
trainer/Log Pis Mean                     1.9051
trainer/Log Pis Std                      1.9983
trainer/Log Pis Max                      6.15527
trainer/Log Pis Min                     -6.72942
trainer/Policy mu Mean                  -0.778891
trainer/Policy mu Std                    1.28306
trainer/Policy mu Max                    2.32293
trainer/Policy mu Min                   -2.56276
trainer/Policy log std Mean             -0.695914
trainer/Policy log std Std               0.193896
trainer/Policy log std Max              -0.138765
trainer/Policy log std Min              -1.70002
trainer/Alpha                            0.177003
trainer/Alpha Loss                      -0.164326
exploration/num steps total           2700
exploration/num paths total             27
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.6679
exploration/Rewards Std                  0.842407
exploration/Rewards Max                 -0.0129049
exploration/Rewards Min                 -4.67803
exploration/Returns Mean               -66.79
exploration/Returns Std                  0
exploration/Returns Max                -66.79
exploration/Returns Min                -66.79
exploration/Actions Mean                -0.58652
exploration/Actions Std                  1.06967
exploration/Actions Max                  2.19749
exploration/Actions Min                 -4.00249
exploration/Num Paths                    1
exploration/Average Returns            -66.79
evaluation/num steps total          135000
evaluation/num paths total             135
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.533409
evaluation/Rewards Std                   0.658377
evaluation/Rewards Max                  -0.000928725
evaluation/Rewards Min                  -7.47514
evaluation/Returns Mean               -533.409
evaluation/Returns Std                  29.306
evaluation/Returns Max                -495.946
evaluation/Returns Min                -567.859
evaluation/Actions Mean                 -0.549993
evaluation/Actions Std                   1.03436
evaluation/Actions Max                   2.95666
evaluation/Actions Min                  -3.86359
evaluation/Num Paths                     5
evaluation/Average Returns            -533.409
time/data storing (s)                    0.000623601
time/evaluation sampling (s)             2.70242
time/exploration real sampling (s)       0.0676794
time/exploration sim sampling (s)        6.822e-06
time/logging (s)                         0.012776
time/saving (s)                          0.014531
time/training (s)                       22.0421
time/epoch (s)                          24.8401
time/total (s)                         656.386
Epoch                                   26
----------------------------------  ----------------
2020-02-20 20:53:28.754080 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 27 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.70024
trainer/QF2 Loss                         2.55693
trainer/Policy Loss                     37.7275
trainer/Q1 Predictions Mean            -40.3423
trainer/Q1 Predictions Std              31.0199
trainer/Q1 Predictions Max              -2.3102
trainer/Q1 Predictions Min             -98.1663
trainer/Q2 Predictions Mean            -40.2047
trainer/Q2 Predictions Std              31.1449
trainer/Q2 Predictions Max              -2.59551
trainer/Q2 Predictions Min             -98.8813
trainer/Q Targets Mean                 -39.9344
trainer/Q Targets Std                   31.1643
trainer/Q Targets Max                   -2.50101
trainer/Q Targets Min                  -97.1273
trainer/Log Pis Mean                     2.1586
trainer/Log Pis Std                      1.81504
trainer/Log Pis Max                      5.86987
trainer/Log Pis Min                     -3.96186
trainer/Policy mu Mean                  -0.929157
trainer/Policy mu Std                    1.22739
trainer/Policy mu Max                    2.29038
trainer/Policy mu Min                   -2.78496
trainer/Policy log std Mean             -0.753754
trainer/Policy log std Std               0.169676
trainer/Policy log std Max              -0.0938881
trainer/Policy log std Min              -1.57821
trainer/Alpha                            0.175778
trainer/Alpha Loss                       0.275753
exploration/num steps total           2800
exploration/num paths total             28
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.64448
exploration/Rewards Std                  0.742165
exploration/Rewards Max                 -0.00840823
exploration/Rewards Min                 -4.19149
exploration/Returns Mean               -64.448
exploration/Returns Std                  0
exploration/Returns Max                -64.448
exploration/Returns Min                -64.448
exploration/Actions Mean                -0.569863
exploration/Actions Std                  1.01943
exploration/Actions Max                  2.22785
exploration/Actions Min                 -3.58133
exploration/Num Paths                    1
exploration/Average Returns            -64.448
evaluation/num steps total          140000
evaluation/num paths total             140
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.594306
evaluation/Rewards Std                   0.807608
evaluation/Rewards Max                  -0.000789549
evaluation/Rewards Min                  -7.94446
evaluation/Returns Mean               -594.306
evaluation/Returns Std                 121.68
evaluation/Returns Max                -471.702
evaluation/Returns Min                -748.55
evaluation/Actions Mean                 -0.55882
evaluation/Actions Std                   1.03472
evaluation/Actions Max                   3.28941
evaluation/Actions Min                  -4.26938
evaluation/Num Paths                     5
evaluation/Average Returns            -594.306
time/data storing (s)                    0.000651364
time/evaluation sampling (s)             2.71158
time/exploration real sampling (s)       0.0669564
time/exploration sim sampling (s)        6.723e-06
time/logging (s)                         0.0135094
time/saving (s)                          0.0117997
time/training (s)                       21.4675
time/epoch (s)                          24.272
time/total (s)                         680.665
Epoch                                   27
----------------------------------  ----------------
2020-02-20 20:53:50.999453 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 28 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.62428
trainer/QF2 Loss                         1.19871
trainer/Policy Loss                     35.538
trainer/Q1 Predictions Mean            -37.9257
trainer/Q1 Predictions Std              31.3488
trainer/Q1 Predictions Max              -0.921426
trainer/Q1 Predictions Min             -96.8037
trainer/Q2 Predictions Mean            -38.2327
trainer/Q2 Predictions Std              31.4354
trainer/Q2 Predictions Max              -2.32894
trainer/Q2 Predictions Min             -98.7481
trainer/Q Targets Mean                 -38.3536
trainer/Q Targets Std                   31.5422
trainer/Q Targets Max                   -2.62904
trainer/Q Targets Min                  -97.2313
trainer/Log Pis Mean                     1.94881
trainer/Log Pis Std                      1.84582
trainer/Log Pis Max                      5.80371
trainer/Log Pis Min                     -3.39758
trainer/Policy mu Mean                  -0.819048
trainer/Policy mu Std                    1.25023
trainer/Policy mu Max                    2.29297
trainer/Policy mu Min                   -2.47041
trainer/Policy log std Mean             -0.730528
trainer/Policy log std Std               0.180017
trainer/Policy log std Max              -0.243556
trainer/Policy log std Min              -1.66376
trainer/Alpha                            0.174599
trainer/Alpha Loss                      -0.0893442
exploration/num steps total           2900
exploration/num paths total             29
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.553843
exploration/Rewards Std                  0.481159
exploration/Rewards Max                 -0.00921512
exploration/Rewards Min                 -1.75923
exploration/Returns Mean               -55.3843
exploration/Returns Std                  0
exploration/Returns Max                -55.3843
exploration/Returns Min                -55.3843
exploration/Actions Mean                -0.548356
exploration/Actions Std                  1.12932
exploration/Actions Max                  2.72142
exploration/Actions Min                 -3.72343
exploration/Num Paths                    1
exploration/Average Returns            -55.3843
evaluation/num steps total          145000
evaluation/num paths total             145
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.5663
evaluation/Rewards Std                   0.782758
evaluation/Rewards Max                  -0.000112524
evaluation/Rewards Min                  -8.13881
evaluation/Returns Mean               -566.3
evaluation/Returns Std                  55.2419
evaluation/Returns Max                -486.581
evaluation/Returns Min                -634.256
evaluation/Actions Mean                 -0.544899
evaluation/Actions Std                   1.01926
evaluation/Actions Max                   3.25536
evaluation/Actions Min                  -4.57877
evaluation/Num Paths                     5
evaluation/Average Returns            -566.3
time/data storing (s)                    0.000618984
time/evaluation sampling (s)             2.68778
time/exploration real sampling (s)       0.0668007
time/exploration sim sampling (s)        6.487e-06
time/logging (s)                         0.013976
time/saving (s)                          0.0196397
time/training (s)                       19.4487
time/epoch (s)                          22.2375
time/total (s)                         702.91
Epoch                                   28
----------------------------------  ----------------
2020-02-20 20:54:13.745806 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 29 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.89993
trainer/QF2 Loss                         1.63344
trainer/Policy Loss                     32.359
trainer/Q1 Predictions Mean            -34.3619
trainer/Q1 Predictions Std              29.6381
trainer/Q1 Predictions Max              -3.03918
trainer/Q1 Predictions Min             -96.3278
trainer/Q2 Predictions Mean            -34.31
trainer/Q2 Predictions Std              29.8606
trainer/Q2 Predictions Max              -2.94046
trainer/Q2 Predictions Min             -97.6078
trainer/Q Targets Mean                 -34.066
trainer/Q Targets Std                   29.7785
trainer/Q Targets Max                   -2.45991
trainer/Q Targets Min                  -96.3166
trainer/Log Pis Mean                     1.86271
trainer/Log Pis Std                      1.968
trainer/Log Pis Max                      6.57079
trainer/Log Pis Min                     -5.17874
trainer/Policy mu Mean                  -0.821753
trainer/Policy mu Std                    1.24348
trainer/Policy mu Max                    2.54875
trainer/Policy mu Min                   -2.39517
trainer/Policy log std Mean             -0.745893
trainer/Policy log std Std               0.170738
trainer/Policy log std Max              -0.168876
trainer/Policy log std Min              -1.61169
trainer/Alpha                            0.173117
trainer/Alpha Loss                      -0.240777
exploration/num steps total           3000
exploration/num paths total             30
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.461195
exploration/Rewards Std                  0.456735
exploration/Rewards Max                 -0.00645567
exploration/Rewards Min                 -1.77092
exploration/Returns Mean               -46.1195
exploration/Returns Std                  0
exploration/Returns Max                -46.1195
exploration/Returns Min                -46.1195
exploration/Actions Mean                -0.626069
exploration/Actions Std                  1.09997
exploration/Actions Max                  2.77767
exploration/Actions Min                 -3.42616
exploration/Num Paths                    1
exploration/Average Returns            -46.1195
evaluation/num steps total          150000
evaluation/num paths total             150
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.515719
evaluation/Rewards Std                   0.588253
evaluation/Rewards Max                  -0.000169651
evaluation/Rewards Min                  -5.08034
evaluation/Returns Mean               -515.719
evaluation/Returns Std                  59.8148
evaluation/Returns Max                -472.543
evaluation/Returns Min                -633.11
evaluation/Actions Mean                 -0.534063
evaluation/Actions Std                   1.02022
evaluation/Actions Max                   3.57619
evaluation/Actions Min                  -4.34849
evaluation/Num Paths                     5
evaluation/Average Returns            -515.719
time/data storing (s)                    0.00124504
time/evaluation sampling (s)             2.74621
time/exploration real sampling (s)       0.0655469
time/exploration sim sampling (s)        6.785e-06
time/logging (s)                         0.0141518
time/saving (s)                          0.0125338
time/training (s)                       19.8988
time/epoch (s)                          22.7385
time/total (s)                         725.655
Epoch                                   29
----------------------------------  ----------------
2020-02-20 20:54:38.683758 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 30 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.67311
trainer/QF2 Loss                         1.58261
trainer/Policy Loss                     35.1631
trainer/Q1 Predictions Mean            -37.1261
trainer/Q1 Predictions Std              30.4034
trainer/Q1 Predictions Max              -2.56121
trainer/Q1 Predictions Min             -96.4485
trainer/Q2 Predictions Mean            -37.25
trainer/Q2 Predictions Std              30.3352
trainer/Q2 Predictions Max              -3.02165
trainer/Q2 Predictions Min             -96.9661
trainer/Q Targets Mean                 -37.1035
trainer/Q Targets Std                   30.3077
trainer/Q Targets Max                   -3.05107
trainer/Q Targets Min                  -96.409
trainer/Log Pis Mean                     2.16827
trainer/Log Pis Std                      1.78
trainer/Log Pis Max                      5.94105
trainer/Log Pis Min                     -4.23928
trainer/Policy mu Mean                  -0.90554
trainer/Policy mu Std                    1.24859
trainer/Policy mu Max                    2.35737
trainer/Policy mu Min                   -2.44316
trainer/Policy log std Mean             -0.752383
trainer/Policy log std Std               0.158378
trainer/Policy log std Max              -0.231518
trainer/Policy log std Min              -1.58247
trainer/Alpha                            0.171289
trainer/Alpha Loss                       0.296887
exploration/num steps total           3100
exploration/num paths total             31
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.05131
exploration/Rewards Std                  1.24818
exploration/Rewards Max                 -0.00795812
exploration/Rewards Min                 -7.20338
exploration/Returns Mean              -105.131
exploration/Returns Std                  0
exploration/Returns Max               -105.131
exploration/Returns Min               -105.131
exploration/Actions Mean                -0.47997
exploration/Actions Std                  1.04868
exploration/Actions Max                  2.23128
exploration/Actions Min                 -3.53881
exploration/Num Paths                    1
exploration/Average Returns           -105.131
evaluation/num steps total          155000
evaluation/num paths total             155
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.560814
evaluation/Rewards Std                   0.714853
evaluation/Rewards Max                  -0.000627042
evaluation/Rewards Min                  -7.97656
evaluation/Returns Mean               -560.814
evaluation/Returns Std                  96.0257
evaluation/Returns Max                -485.606
evaluation/Returns Min                -744.275
evaluation/Actions Mean                 -0.533425
evaluation/Actions Std                   1.03386
evaluation/Actions Max                   3.3356
evaluation/Actions Min                  -4.39623
evaluation/Num Paths                     5
evaluation/Average Returns            -560.814
time/data storing (s)                    0.00062446
time/evaluation sampling (s)             2.73955
time/exploration real sampling (s)       0.0677698
time/exploration sim sampling (s)        6.607e-06
time/logging (s)                         0.0135124
time/saving (s)                          0.0152677
time/training (s)                       22.092
time/epoch (s)                          24.9287
time/total (s)                         750.591
Epoch                                   30
----------------------------------  ----------------
2020-02-20 20:55:03.440667 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 31 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.55845
trainer/QF2 Loss                         1.28748
trainer/Policy Loss                     38.3522
trainer/Q1 Predictions Mean            -41.0722
trainer/Q1 Predictions Std              30.9826
trainer/Q1 Predictions Max              -2.74346
trainer/Q1 Predictions Min             -96.3417
trainer/Q2 Predictions Mean            -41.0624
trainer/Q2 Predictions Std              30.9772
trainer/Q2 Predictions Max              -3.1399
trainer/Q2 Predictions Min             -95.8085
trainer/Q Targets Mean                 -41.1652
trainer/Q Targets Std                   30.9519
trainer/Q Targets Max                   -3.3907
trainer/Q Targets Min                  -95.9493
trainer/Log Pis Mean                     1.99438
trainer/Log Pis Std                      1.86296
trainer/Log Pis Max                      5.81442
trainer/Log Pis Min                     -4.07501
trainer/Policy mu Mean                  -0.860007
trainer/Policy mu Std                    1.27131
trainer/Policy mu Max                    2.46864
trainer/Policy mu Min                   -2.61412
trainer/Policy log std Mean             -0.740334
trainer/Policy log std Std               0.162682
trainer/Policy log std Max              -0.234177
trainer/Policy log std Min              -1.51866
trainer/Alpha                            0.167843
trainer/Alpha Loss                      -0.0100228
exploration/num steps total           3200
exploration/num paths total             32
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.68304
exploration/Rewards Std                  0.766457
exploration/Rewards Max                 -0.00945352
exploration/Rewards Min                 -4.33996
exploration/Returns Mean               -68.304
exploration/Returns Std                  0
exploration/Returns Max                -68.304
exploration/Returns Min                -68.304
exploration/Actions Mean                -0.564834
exploration/Actions Std                  1.08501
exploration/Actions Max                  2.16149
exploration/Actions Min                 -3.71911
exploration/Num Paths                    1
exploration/Average Returns            -68.304
evaluation/num steps total          160000
evaluation/num paths total             160
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.559843
evaluation/Rewards Std                   0.730942
evaluation/Rewards Max                  -0.00136154
evaluation/Rewards Min                  -8.29228
evaluation/Returns Mean               -559.843
evaluation/Returns Std                 101.261
evaluation/Returns Max                -453.131
evaluation/Returns Min                -738.672
evaluation/Actions Mean                 -0.553056
evaluation/Actions Std                   1.03707
evaluation/Actions Max                   4.1783
evaluation/Actions Min                  -4.21301
evaluation/Num Paths                     5
evaluation/Average Returns            -559.843
time/data storing (s)                    0.000623868
time/evaluation sampling (s)             2.7847
time/exploration real sampling (s)       0.0666739
time/exploration sim sampling (s)        7.215e-06
time/logging (s)                         0.0128584
time/saving (s)                          0.0151713
time/training (s)                       21.8683
time/epoch (s)                          24.7484
time/total (s)                         775.345
Epoch                                   31
----------------------------------  ----------------
2020-02-20 20:55:28.131659 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 32 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         3.88183
trainer/QF2 Loss                         4.23403
trainer/Policy Loss                     32.7916
trainer/Q1 Predictions Mean            -35.4453
trainer/Q1 Predictions Std              28.13
trainer/Q1 Predictions Max              -3.60792
trainer/Q1 Predictions Min             -95.0284
trainer/Q2 Predictions Mean            -35.324
trainer/Q2 Predictions Std              28.1581
trainer/Q2 Predictions Max              -3.18612
trainer/Q2 Predictions Min             -95.0589
trainer/Q Targets Mean                 -35.4759
trainer/Q Targets Std                   28.3222
trainer/Q Targets Max                   -3.66213
trainer/Q Targets Min                  -95.8095
trainer/Log Pis Mean                     1.98622
trainer/Log Pis Std                      1.82048
trainer/Log Pis Max                      6.57184
trainer/Log Pis Min                     -2.88505
trainer/Policy mu Mean                  -0.876096
trainer/Policy mu Std                    1.21243
trainer/Policy mu Max                    2.44457
trainer/Policy mu Min                   -2.41721
trainer/Policy log std Mean             -0.720469
trainer/Policy log std Std               0.166253
trainer/Policy log std Max              -0.237316
trainer/Policy log std Min              -1.46862
trainer/Alpha                            0.161511
trainer/Alpha Loss                      -0.02513
exploration/num steps total           3300
exploration/num paths total             33
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.98781
exploration/Rewards Std                  1.03572
exploration/Rewards Max                 -0.00160843
exploration/Rewards Min                 -5.55013
exploration/Returns Mean               -98.781
exploration/Returns Std                  0
exploration/Returns Max                -98.781
exploration/Returns Min                -98.781
exploration/Actions Mean                -0.480359
exploration/Actions Std                  1.06573
exploration/Actions Max                  2.13051
exploration/Actions Min                 -3.38742
exploration/Num Paths                    1
exploration/Average Returns            -98.781
evaluation/num steps total          165000
evaluation/num paths total             165
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.57635
evaluation/Rewards Std                   0.822143
evaluation/Rewards Max                  -0.000273595
evaluation/Rewards Min                  -8.45597
evaluation/Returns Mean               -576.35
evaluation/Returns Std                  32.3745
evaluation/Returns Max                -524.262
evaluation/Returns Min                -624.363
evaluation/Actions Mean                 -0.547181
evaluation/Actions Std                   1.02862
evaluation/Actions Max                   3.39444
evaluation/Actions Min                  -4.53799
evaluation/Num Paths                     5
evaluation/Average Returns            -576.35
time/data storing (s)                    0.000623963
time/evaluation sampling (s)             2.75421
time/exploration real sampling (s)       0.0702521
time/exploration sim sampling (s)        6.655e-06
time/logging (s)                         0.0126832
time/saving (s)                          0.0150965
time/training (s)                       21.8312
time/epoch (s)                          24.684
time/total (s)                         800.035
Epoch                                   32
----------------------------------  ----------------
2020-02-20 20:55:52.865740 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 33 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         4.70969
trainer/QF2 Loss                         4.34768
trainer/Policy Loss                     31.2915
trainer/Q1 Predictions Mean            -33.5118
trainer/Q1 Predictions Std              27.6128
trainer/Q1 Predictions Max              -4.0818
trainer/Q1 Predictions Min             -95.9357
trainer/Q2 Predictions Mean            -33.5193
trainer/Q2 Predictions Std              27.7708
trainer/Q2 Predictions Max              -3.85197
trainer/Q2 Predictions Min             -97.9199
trainer/Q Targets Mean                 -33.3643
trainer/Q Targets Std                   27.7708
trainer/Q Targets Max                   -3.91939
trainer/Q Targets Min                  -95.0058
trainer/Log Pis Mean                     1.916
trainer/Log Pis Std                      2.09302
trainer/Log Pis Max                      6.09469
trainer/Log Pis Min                     -5.1502
trainer/Policy mu Mean                  -1.01677
trainer/Policy mu Std                    1.14272
trainer/Policy mu Max                    2.39612
trainer/Policy mu Min                   -2.51333
trainer/Policy log std Mean             -0.739597
trainer/Policy log std Std               0.166411
trainer/Policy log std Max              -0.260748
trainer/Policy log std Min              -1.5372
trainer/Alpha                            0.161025
trainer/Alpha Loss                      -0.153392
exploration/num steps total           3400
exploration/num paths total             34
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.02095
exploration/Rewards Std                  1.56549
exploration/Rewards Max                 -0.00634144
exploration/Rewards Min                 -7.94906
exploration/Returns Mean              -102.095
exploration/Returns Std                  0
exploration/Returns Max               -102.095
exploration/Returns Min               -102.095
exploration/Actions Mean                -0.590571
exploration/Actions Std                  1.00489
exploration/Actions Max                  1.87168
exploration/Actions Min                 -2.9279
exploration/Num Paths                    1
exploration/Average Returns           -102.095
evaluation/num steps total          170000
evaluation/num paths total             170
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.53641
evaluation/Rewards Std                   0.745687
evaluation/Rewards Max                  -0.000820468
evaluation/Rewards Min                  -7.80049
evaluation/Returns Mean               -536.41
evaluation/Returns Std                  82.9082
evaluation/Returns Max                -420.592
evaluation/Returns Min                -677.35
evaluation/Actions Mean                 -0.572476
evaluation/Actions Std                   1.01335
evaluation/Actions Max                   3.53156
evaluation/Actions Min                  -4.51644
evaluation/Num Paths                     5
evaluation/Average Returns            -536.41
time/data storing (s)                    0.000643621
time/evaluation sampling (s)             2.69491
time/exploration real sampling (s)       0.0677251
time/exploration sim sampling (s)        6.577e-06
time/logging (s)                         0.0127165
time/saving (s)                          0.0155586
time/training (s)                       21.9363
time/epoch (s)                          24.7278
time/total (s)                         824.768
Epoch                                   33
----------------------------------  ----------------
2020-02-20 20:56:18.247857 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 34 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.25951
trainer/QF2 Loss                         1.03661
trainer/Policy Loss                     35.8683
trainer/Q1 Predictions Mean            -37.6972
trainer/Q1 Predictions Std              30.5825
trainer/Q1 Predictions Max              -3.93056
trainer/Q1 Predictions Min             -95.195
trainer/Q2 Predictions Mean            -37.7948
trainer/Q2 Predictions Std              30.6552
trainer/Q2 Predictions Max              -4.30952
trainer/Q2 Predictions Min             -96.2134
trainer/Q Targets Mean                 -37.7783
trainer/Q Targets Std                   30.8414
trainer/Q Targets Max                   -4.26999
trainer/Q Targets Min                  -95.1415
trainer/Log Pis Mean                     2.20547
trainer/Log Pis Std                      1.98407
trainer/Log Pis Max                      6.62986
trainer/Log Pis Min                     -2.94527
trainer/Policy mu Mean                  -0.911193
trainer/Policy mu Std                    1.24362
trainer/Policy mu Max                    2.38124
trainer/Policy mu Min                   -2.42731
trainer/Policy log std Mean             -0.698379
trainer/Policy log std Std               0.148507
trainer/Policy log std Max              -0.295313
trainer/Policy log std Min              -1.40107
trainer/Alpha                            0.158975
trainer/Alpha Loss                       0.37791
exploration/num steps total           3500
exploration/num paths total             35
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.9944
exploration/Rewards Std                  1.4343
exploration/Rewards Max                 -0.012257
exploration/Rewards Min                 -7.47724
exploration/Returns Mean               -99.44
exploration/Returns Std                  0
exploration/Returns Max                -99.44
exploration/Returns Min                -99.44
exploration/Actions Mean                -0.572746
exploration/Actions Std                  1.13713
exploration/Actions Max                  3.8317
exploration/Actions Min                 -2.91526
exploration/Num Paths                    1
exploration/Average Returns            -99.44
evaluation/num steps total          175000
evaluation/num paths total             175
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.533929
evaluation/Rewards Std                   0.71894
evaluation/Rewards Max                  -0.000145437
evaluation/Rewards Min                  -7.21713
evaluation/Returns Mean               -533.929
evaluation/Returns Std                 152.646
evaluation/Returns Max                -424.722
evaluation/Returns Min                -837.157
evaluation/Actions Mean                 -0.586043
evaluation/Actions Std                   1.023
evaluation/Actions Max                   3.2237
evaluation/Actions Min                  -5.06807
evaluation/Num Paths                     5
evaluation/Average Returns            -533.929
time/data storing (s)                    0.000630325
time/evaluation sampling (s)             2.67607
time/exploration real sampling (s)       0.0682249
time/exploration sim sampling (s)        7.436e-06
time/logging (s)                         0.0138754
time/saving (s)                          0.0160004
time/training (s)                       22.6022
time/epoch (s)                          25.377
time/total (s)                         850.15
Epoch                                   34
----------------------------------  ----------------
2020-02-20 20:56:41.138705 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 35 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         3.06301
trainer/QF2 Loss                         3.09141
trainer/Policy Loss                     33.2804
trainer/Q1 Predictions Mean            -35.0035
trainer/Q1 Predictions Std              29.1129
trainer/Q1 Predictions Max              -4.64163
trainer/Q1 Predictions Min             -95.7085
trainer/Q2 Predictions Mean            -34.6107
trainer/Q2 Predictions Std              28.9048
trainer/Q2 Predictions Max              -4.7262
trainer/Q2 Predictions Min             -96.1847
trainer/Q Targets Mean                 -35.137
trainer/Q Targets Std                   29.4119
trainer/Q Targets Max                   -4.44531
trainer/Q Targets Min                  -94.9421
trainer/Log Pis Mean                     1.95691
trainer/Log Pis Std                      1.91839
trainer/Log Pis Max                      6.01251
trainer/Log Pis Min                     -4.20676
trainer/Policy mu Mean                  -0.833036
trainer/Policy mu Std                    1.24413
trainer/Policy mu Max                    2.41915
trainer/Policy mu Min                   -2.4492
trainer/Policy log std Mean             -0.729376
trainer/Policy log std Std               0.166818
trainer/Policy log std Max              -0.339756
trainer/Policy log std Min              -1.45385
trainer/Alpha                            0.156553
trainer/Alpha Loss                      -0.0799011
exploration/num steps total           3600
exploration/num paths total             36
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.900943
exploration/Rewards Std                  1.63738
exploration/Rewards Max                 -0.0122725
exploration/Rewards Min                 -7.69734
exploration/Returns Mean               -90.0943
exploration/Returns Std                  0
exploration/Returns Max                -90.0943
exploration/Returns Min                -90.0943
exploration/Actions Mean                -0.535084
exploration/Actions Std                  1.07035
exploration/Actions Max                  2.89795
exploration/Actions Min                 -3.04055
exploration/Num Paths                    1
exploration/Average Returns            -90.0943
evaluation/num steps total          180000
evaluation/num paths total             180
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.527508
evaluation/Rewards Std                   0.706459
evaluation/Rewards Max                  -0.00142579
evaluation/Rewards Min                  -7.03607
evaluation/Returns Mean               -527.508
evaluation/Returns Std                  69.521
evaluation/Returns Max                -439.874
evaluation/Returns Min                -589.589
evaluation/Actions Mean                 -0.601987
evaluation/Actions Std                   1.00894
evaluation/Actions Max                   3.42779
evaluation/Actions Min                  -4.62889
evaluation/Num Paths                     5
evaluation/Average Returns            -527.508
time/data storing (s)                    0.000642575
time/evaluation sampling (s)             2.74102
time/exploration real sampling (s)       0.0691531
time/exploration sim sampling (s)        6.655e-06
time/logging (s)                         0.0141574
time/saving (s)                          0.0161355
time/training (s)                       20.0436
time/epoch (s)                          22.8848
time/total (s)                         873.04
Epoch                                   35
----------------------------------  ----------------
2020-02-20 20:57:06.827441 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 36 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.00582
trainer/QF2 Loss                         1.99352
trainer/Policy Loss                     34.0075
trainer/Q1 Predictions Mean            -36.1675
trainer/Q1 Predictions Std              28.8972
trainer/Q1 Predictions Max              -4.65245
trainer/Q1 Predictions Min             -95.9985
trainer/Q2 Predictions Mean            -36.061
trainer/Q2 Predictions Std              28.8004
trainer/Q2 Predictions Max              -4.76734
trainer/Q2 Predictions Min             -95.5587
trainer/Q Targets Mean                 -36.3915
trainer/Q Targets Std                   29.1122
trainer/Q Targets Max                   -4.85223
trainer/Q Targets Min                  -94.9951
trainer/Log Pis Mean                     2.066
trainer/Log Pis Std                      1.93706
trainer/Log Pis Max                      6.3726
trainer/Log Pis Min                     -4.69253
trainer/Policy mu Mean                  -0.946051
trainer/Policy mu Std                    1.26923
trainer/Policy mu Max                    2.324
trainer/Policy mu Min                   -2.66713
trainer/Policy log std Mean             -0.721931
trainer/Policy log std Std               0.150967
trainer/Policy log std Max              -0.2666
trainer/Policy log std Min              -1.39726
trainer/Alpha                            0.155123
trainer/Alpha Loss                       0.123006
exploration/num steps total           3700
exploration/num paths total             37
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.492527
exploration/Rewards Std                  0.551153
exploration/Rewards Max                 -0.00154359
exploration/Rewards Min                 -2.00589
exploration/Returns Mean               -49.2527
exploration/Returns Std                  0
exploration/Returns Max                -49.2527
exploration/Returns Min                -49.2527
exploration/Actions Mean                -0.599804
exploration/Actions Std                  1.00462
exploration/Actions Max                  1.89676
exploration/Actions Min                 -3.53844
exploration/Num Paths                    1
exploration/Average Returns            -49.2527
evaluation/num steps total          185000
evaluation/num paths total             185
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.566272
evaluation/Rewards Std                   0.878611
evaluation/Rewards Max                  -0.00105935
evaluation/Rewards Min                  -8.24785
evaluation/Returns Mean               -566.272
evaluation/Returns Std                 136.549
evaluation/Returns Max                -396.333
evaluation/Returns Min                -798.25
evaluation/Actions Mean                 -0.64702
evaluation/Actions Std                   1.02595
evaluation/Actions Max                   3.53232
evaluation/Actions Min                  -4.74348
evaluation/Num Paths                     5
evaluation/Average Returns            -566.272
time/data storing (s)                    0.000633545
time/evaluation sampling (s)             2.6801
time/exploration real sampling (s)       0.0657172
time/exploration sim sampling (s)        7.129e-06
time/logging (s)                         0.0223677
time/saving (s)                          0.0159261
time/training (s)                       22.905
time/epoch (s)                          25.6898
time/total (s)                         898.736
Epoch                                   36
----------------------------------  ----------------
2020-02-20 20:57:30.742520 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 37 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         6.6699
trainer/QF2 Loss                         6.36614
trainer/Policy Loss                     36.7914
trainer/Q1 Predictions Mean            -38.671
trainer/Q1 Predictions Std              30.1597
trainer/Q1 Predictions Max              -4.23986
trainer/Q1 Predictions Min             -96.7443
trainer/Q2 Predictions Mean            -38.8483
trainer/Q2 Predictions Std              30.2158
trainer/Q2 Predictions Max              -4.81331
trainer/Q2 Predictions Min             -95.3966
trainer/Q Targets Mean                 -39.2253
trainer/Q Targets Std                   30.1814
trainer/Q Targets Max                   -5.08927
trainer/Q Targets Min                  -95.4847
trainer/Log Pis Mean                     2.12371
trainer/Log Pis Std                      1.89243
trainer/Log Pis Max                      5.94466
trainer/Log Pis Min                     -4.11323
trainer/Policy mu Mean                  -0.839609
trainer/Policy mu Std                    1.26924
trainer/Policy mu Max                    2.57102
trainer/Policy mu Min                   -2.44934
trainer/Policy log std Mean             -0.741736
trainer/Policy log std Std               0.170457
trainer/Policy log std Max              -0.238463
trainer/Policy log std Min              -1.46848
trainer/Alpha                            0.148764
trainer/Alpha Loss                       0.235702
exploration/num steps total           3800
exploration/num paths total             38
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.612422
exploration/Rewards Std                  0.574584
exploration/Rewards Max                 -0.00232586
exploration/Rewards Min                 -2.81501
exploration/Returns Mean               -61.2422
exploration/Returns Std                  0
exploration/Returns Max                -61.2422
exploration/Returns Min                -61.2422
exploration/Actions Mean                -0.601104
exploration/Actions Std                  1.11723
exploration/Actions Max                  2.1928
exploration/Actions Min                 -3.97605
exploration/Num Paths                    1
exploration/Average Returns            -61.2422
evaluation/num steps total          190000
evaluation/num paths total             190
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.66101
evaluation/Rewards Std                   0.951337
evaluation/Rewards Max                  -0.000222277
evaluation/Rewards Min                  -7.69875
evaluation/Returns Mean               -661.01
evaluation/Returns Std                 275.985
evaluation/Returns Max                -455.89
evaluation/Returns Min               -1199.68
evaluation/Actions Mean                 -0.59228
evaluation/Actions Std                   1.0281
evaluation/Actions Max                   4.29554
evaluation/Actions Min                  -4.29913
evaluation/Num Paths                     5
evaluation/Average Returns            -661.01
time/data storing (s)                    0.000628507
time/evaluation sampling (s)             2.75828
time/exploration real sampling (s)       0.0656163
time/exploration sim sampling (s)        7.556e-06
time/logging (s)                         0.0141422
time/saving (s)                          0.0133294
time/training (s)                       21.036
time/epoch (s)                          23.888
time/total (s)                         922.639
Epoch                                   37
----------------------------------  ----------------
2020-02-20 20:57:55.726144 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 38 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.37364
trainer/QF2 Loss                         2.28189
trainer/Policy Loss                     32.3839
trainer/Q1 Predictions Mean            -34.3469
trainer/Q1 Predictions Std              30.2975
trainer/Q1 Predictions Max              -5.60104
trainer/Q1 Predictions Min             -98.1528
trainer/Q2 Predictions Mean            -34.2327
trainer/Q2 Predictions Std              30.152
trainer/Q2 Predictions Max              -5.64686
trainer/Q2 Predictions Min             -96.4477
trainer/Q Targets Mean                 -34.1257
trainer/Q Targets Std                   30.3693
trainer/Q Targets Max                   -5.46256
trainer/Q Targets Min                  -95.4123
trainer/Log Pis Mean                     1.68542
trainer/Log Pis Std                      1.86376
trainer/Log Pis Max                      6.04912
trainer/Log Pis Min                     -6.71098
trainer/Policy mu Mean                  -0.943049
trainer/Policy mu Std                    1.09818
trainer/Policy mu Max                    2.32134
trainer/Policy mu Min                   -2.36118
trainer/Policy log std Mean             -0.728081
trainer/Policy log std Std               0.149217
trainer/Policy log std Max              -0.277487
trainer/Policy log std Min              -1.51193
trainer/Alpha                            0.148289
trainer/Alpha Loss                      -0.600392
exploration/num steps total           3900
exploration/num paths total             39
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.63007
exploration/Rewards Std                  0.767412
exploration/Rewards Max                 -0.00666465
exploration/Rewards Min                 -4.55151
exploration/Returns Mean               -63.007
exploration/Returns Std                  0
exploration/Returns Max                -63.007
exploration/Returns Min                -63.007
exploration/Actions Mean                -0.689683
exploration/Actions Std                  1.15089
exploration/Actions Max                  2.80645
exploration/Actions Min                 -3.48394
exploration/Num Paths                    1
exploration/Average Returns            -63.007
evaluation/num steps total          195000
evaluation/num paths total             195
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.561189
evaluation/Rewards Std                   0.847456
evaluation/Rewards Max                  -0.000749367
evaluation/Rewards Min                  -8.07523
evaluation/Returns Mean               -561.189
evaluation/Returns Std                  49.9422
evaluation/Returns Max                -506.827
evaluation/Returns Min                -622.04
evaluation/Actions Mean                 -0.611424
evaluation/Actions Std                   1.02285
evaluation/Actions Max                   3.11929
evaluation/Actions Min                  -4.21257
evaluation/Num Paths                     5
evaluation/Average Returns            -561.189
time/data storing (s)                    0.000651345
time/evaluation sampling (s)             2.82254
time/exploration real sampling (s)       0.0649394
time/exploration sim sampling (s)        6.486e-06
time/logging (s)                         0.0127673
time/saving (s)                          0.0158317
time/training (s)                       22.0579
time/epoch (s)                          24.9747
time/total (s)                         947.62
Epoch                                   38
----------------------------------  ----------------
2020-02-20 20:58:20.836227 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 39 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.11194
trainer/QF2 Loss                         0.950819
trainer/Policy Loss                     33.1788
trainer/Q1 Predictions Mean            -34.9485
trainer/Q1 Predictions Std              29.5993
trainer/Q1 Predictions Max              -5.34581
trainer/Q1 Predictions Min             -95.8238
trainer/Q2 Predictions Mean            -35.0932
trainer/Q2 Predictions Std              29.6388
trainer/Q2 Predictions Max              -5.54122
trainer/Q2 Predictions Min             -97.226
trainer/Q Targets Mean                 -35.0091
trainer/Q Targets Std                   29.54
trainer/Q Targets Max                   -5.77042
trainer/Q Targets Min                  -95.5473
trainer/Log Pis Mean                     1.96399
trainer/Log Pis Std                      1.79944
trainer/Log Pis Max                      5.9448
trainer/Log Pis Min                     -5.61755
trainer/Policy mu Mean                  -0.884789
trainer/Policy mu Std                    1.22659
trainer/Policy mu Max                    2.39886
trainer/Policy mu Min                   -2.65484
trainer/Policy log std Mean             -0.741091
trainer/Policy log std Std               0.154198
trainer/Policy log std Max              -0.298208
trainer/Policy log std Min              -1.44326
trainer/Alpha                            0.143561
trainer/Alpha Loss                      -0.0699039
exploration/num steps total           4000
exploration/num paths total             40
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.58632
exploration/Rewards Std                  2.35739
exploration/Rewards Max                 -0.0109794
exploration/Rewards Min                 -9.20341
exploration/Returns Mean              -158.632
exploration/Returns Std                  0
exploration/Returns Max               -158.632
exploration/Returns Min               -158.632
exploration/Actions Mean                -0.513028
exploration/Actions Std                  1.12771
exploration/Actions Max                  3.0143
exploration/Actions Min                 -3.4817
exploration/Num Paths                    1
exploration/Average Returns           -158.632
evaluation/num steps total          200000
evaluation/num paths total             200
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.564088
evaluation/Rewards Std                   0.867211
evaluation/Rewards Max                  -0.00085707
evaluation/Rewards Min                  -7.71351
evaluation/Returns Mean               -564.088
evaluation/Returns Std                 105.657
evaluation/Returns Max                -399.888
evaluation/Returns Min                -703.874
evaluation/Actions Mean                 -0.641986
evaluation/Actions Std                   1.01472
evaluation/Actions Max                   3.28682
evaluation/Actions Min                  -4.39415
evaluation/Num Paths                     5
evaluation/Average Returns            -564.088
time/data storing (s)                    0.000611895
time/evaluation sampling (s)             2.69687
time/exploration real sampling (s)       0.0676777
time/exploration sim sampling (s)        6.248e-06
time/logging (s)                         0.0147597
time/saving (s)                          0.0205805
time/training (s)                       22.3052
time/epoch (s)                          25.1057
time/total (s)                         972.731
Epoch                                   39
----------------------------------  ----------------
2020-02-20 20:58:50.042007 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 40 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                        12.8616
trainer/QF2 Loss                        13.3199
trainer/Policy Loss                     36.8674
trainer/Q1 Predictions Mean            -38.825
trainer/Q1 Predictions Std              29.9496
trainer/Q1 Predictions Max              -5.76363
trainer/Q1 Predictions Min             -95.2905
trainer/Q2 Predictions Mean            -38.5285
trainer/Q2 Predictions Std              29.7871
trainer/Q2 Predictions Max              -5.82795
trainer/Q2 Predictions Min             -94.638
trainer/Q Targets Mean                 -38.2917
trainer/Q Targets Std                   29.7852
trainer/Q Targets Max                   -5.89212
trainer/Q Targets Min                  -95.6117
trainer/Log Pis Mean                     2.19928
trainer/Log Pis Std                      1.91503
trainer/Log Pis Max                      7.00875
trainer/Log Pis Min                     -5.60529
trainer/Policy mu Mean                  -0.822202
trainer/Policy mu Std                    1.28676
trainer/Policy mu Max                    2.60734
trainer/Policy mu Min                   -2.44981
trainer/Policy log std Mean             -0.791145
trainer/Policy log std Std               0.198537
trainer/Policy log std Max              -0.21992
trainer/Policy log std Min              -1.63049
trainer/Alpha                            0.142191
trainer/Alpha Loss                       0.388723
exploration/num steps total           4100
exploration/num paths total             41
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.723369
exploration/Rewards Std                  1.29407
exploration/Rewards Max                 -0.00428072
exploration/Rewards Min                 -7.44052
exploration/Returns Mean               -72.3369
exploration/Returns Std                  0
exploration/Returns Max                -72.3369
exploration/Returns Min                -72.3369
exploration/Actions Mean                -0.639557
exploration/Actions Std                  0.945216
exploration/Actions Max                  1.66589
exploration/Actions Min                 -3.2932
exploration/Num Paths                    1
exploration/Average Returns            -72.3369
evaluation/num steps total          205000
evaluation/num paths total             205
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.533355
evaluation/Rewards Std                   0.72658
evaluation/Rewards Max                  -0.0006358
evaluation/Rewards Min                  -7.78355
evaluation/Returns Mean               -533.355
evaluation/Returns Std                  79.4363
evaluation/Returns Max                -436.53
evaluation/Returns Min                -673.634
evaluation/Actions Mean                 -0.606153
evaluation/Actions Std                   1.01883
evaluation/Actions Max                   3.26763
evaluation/Actions Min                  -4.39863
evaluation/Num Paths                     5
evaluation/Average Returns            -533.355
time/data storing (s)                    0.000645288
time/evaluation sampling (s)             3.87214
time/exploration real sampling (s)       0.0740129
time/exploration sim sampling (s)        7.025e-06
time/logging (s)                         0.014572
time/saving (s)                          0.0152173
time/training (s)                       25.2224
time/epoch (s)                          29.199
time/total (s)                        1001.93
Epoch                                   40
----------------------------------  ----------------
2020-02-20 20:59:16.443603 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 41 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.32037
trainer/QF2 Loss                         1.21579
trainer/Policy Loss                     36.1479
trainer/Q1 Predictions Mean            -38.5725
trainer/Q1 Predictions Std              28.1474
trainer/Q1 Predictions Max              -5.86249
trainer/Q1 Predictions Min             -96.9153
trainer/Q2 Predictions Mean            -38.5751
trainer/Q2 Predictions Std              28.2088
trainer/Q2 Predictions Max              -5.99485
trainer/Q2 Predictions Min             -98.5518
trainer/Q Targets Mean                 -38.4397
trainer/Q Targets Std                   28.0756
trainer/Q Targets Max                   -6.1407
trainer/Q Targets Min                  -95.7885
trainer/Log Pis Mean                     2.20045
trainer/Log Pis Std                      1.87918
trainer/Log Pis Max                      6.03117
trainer/Log Pis Min                     -4.08077
trainer/Policy mu Mean                  -0.854444
trainer/Policy mu Std                    1.28728
trainer/Policy mu Max                    2.44897
trainer/Policy mu Min                   -2.45189
trainer/Policy log std Mean             -0.793603
trainer/Policy log std Std               0.196311
trainer/Policy log std Max              -0.292982
trainer/Policy log std Min              -1.58098
trainer/Alpha                            0.141173
trainer/Alpha Loss                       0.392434
exploration/num steps total           4200
exploration/num paths total             42
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.15843
exploration/Rewards Std                  1.97219
exploration/Rewards Max                 -0.00559114
exploration/Rewards Min                 -9.11848
exploration/Returns Mean              -115.843
exploration/Returns Std                  0
exploration/Returns Max               -115.843
exploration/Returns Min               -115.843
exploration/Actions Mean                -0.664246
exploration/Actions Std                  1.20347
exploration/Actions Max                  3.22987
exploration/Actions Min                 -3.40695
exploration/Num Paths                    1
exploration/Average Returns           -115.843
evaluation/num steps total          210000
evaluation/num paths total             210
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.543432
evaluation/Rewards Std                   0.796826
evaluation/Rewards Max                  -0.0012656
evaluation/Rewards Min                  -8.10713
evaluation/Returns Mean               -543.432
evaluation/Returns Std                  67.8445
evaluation/Returns Max                -450.609
evaluation/Returns Min                -652.265
evaluation/Actions Mean                 -0.593523
evaluation/Actions Std                   1.01644
evaluation/Actions Max                   2.82111
evaluation/Actions Min                  -4.43294
evaluation/Num Paths                     5
evaluation/Average Returns            -543.432
time/data storing (s)                    0.000634398
time/evaluation sampling (s)             2.77742
time/exploration real sampling (s)       0.0698319
time/exploration sim sampling (s)        6.329e-06
time/logging (s)                         0.014993
time/saving (s)                          0.0174822
time/training (s)                       23.5139
time/epoch (s)                          26.3943
time/total (s)                        1028.34
Epoch                                   41
----------------------------------  ----------------
2020-02-20 20:59:46.652059 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 42 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         0.993477
trainer/QF2 Loss                         0.937724
trainer/Policy Loss                     37.1187
trainer/Q1 Predictions Mean            -39.0008
trainer/Q1 Predictions Std              29.9326
trainer/Q1 Predictions Max              -5.92845
trainer/Q1 Predictions Min             -98.6965
trainer/Q2 Predictions Mean            -39.2562
trainer/Q2 Predictions Std              29.8463
trainer/Q2 Predictions Max              -6.56118
trainer/Q2 Predictions Min             -97.0615
trainer/Q Targets Mean                 -39.0054
trainer/Q Targets Std                   29.7876
trainer/Q Targets Max                   -6.33218
trainer/Q Targets Min                  -95.7417
trainer/Log Pis Mean                     2.1116
trainer/Log Pis Std                      1.93679
trainer/Log Pis Max                      6.13349
trainer/Log Pis Min                     -5.22738
trainer/Policy mu Mean                  -0.977237
trainer/Policy mu Std                    1.16569
trainer/Policy mu Max                    2.32928
trainer/Policy mu Min                   -2.59156
trainer/Policy log std Mean             -0.774156
trainer/Policy log std Std               0.183331
trainer/Policy log std Max              -0.185797
trainer/Policy log std Min              -1.56987
trainer/Alpha                            0.138147
trainer/Alpha Loss                       0.220899
exploration/num steps total           4300
exploration/num paths total             43
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.692656
exploration/Rewards Std                  0.971683
exploration/Rewards Max                 -0.00975481
exploration/Rewards Min                 -6.09346
exploration/Returns Mean               -69.2656
exploration/Returns Std                  0
exploration/Returns Max                -69.2656
exploration/Returns Min                -69.2656
exploration/Actions Mean                -0.617023
exploration/Actions Std                  1.06109
exploration/Actions Max                  2.14639
exploration/Actions Min                 -2.95468
exploration/Num Paths                    1
exploration/Average Returns            -69.2656
evaluation/num steps total          215000
evaluation/num paths total             215
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.595871
evaluation/Rewards Std                   0.871502
evaluation/Rewards Max                  -0.00081104
evaluation/Rewards Min                  -7.64323
evaluation/Returns Mean               -595.871
evaluation/Returns Std                 105.992
evaluation/Returns Max                -487.34
evaluation/Returns Min                -786.147
evaluation/Actions Mean                 -0.602732
evaluation/Actions Std                   1.01423
evaluation/Actions Max                   3.40336
evaluation/Actions Min                  -4.41049
evaluation/Num Paths                     5
evaluation/Average Returns            -595.871
time/data storing (s)                    0.000640785
time/evaluation sampling (s)             2.84543
time/exploration real sampling (s)       0.0713793
time/exploration sim sampling (s)        7.534e-06
time/logging (s)                         0.0138769
time/saving (s)                          0.0174073
time/training (s)                       27.2501
time/epoch (s)                          30.1989
time/total (s)                        1058.54
Epoch                                   42
----------------------------------  ----------------
2020-02-20 21:00:14.662586 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 43 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         2.81528
trainer/QF2 Loss                         2.48231
trainer/Policy Loss                     31.2248
trainer/Q1 Predictions Mean            -32.919
trainer/Q1 Predictions Std              26.7847
trainer/Q1 Predictions Max              -6.03086
trainer/Q1 Predictions Min             -93.9463
trainer/Q2 Predictions Mean            -33.1551
trainer/Q2 Predictions Std              26.7078
trainer/Q2 Predictions Max              -6.45436
trainer/Q2 Predictions Min             -93.6465
trainer/Q Targets Mean                 -33.5331
trainer/Q Targets Std                   27.048
trainer/Q Targets Max                   -6.55878
trainer/Q Targets Min                  -94.4744
trainer/Log Pis Mean                     1.72069
trainer/Log Pis Std                      1.88984
trainer/Log Pis Max                      5.91838
trainer/Log Pis Min                     -4.41772
trainer/Policy mu Mean                  -0.855863
trainer/Policy mu Std                    1.18389
trainer/Policy mu Max                    2.29334
trainer/Policy mu Min                   -2.5159
trainer/Policy log std Mean             -0.761326
trainer/Policy log std Std               0.182514
trainer/Policy log std Max              -0.237804
trainer/Policy log std Min              -1.63511
trainer/Alpha                            0.138102
trainer/Alpha Loss                      -0.552931
exploration/num steps total           4400
exploration/num paths total             44
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -1.03308
exploration/Rewards Std                  1.69933
exploration/Rewards Max                 -0.00645134
exploration/Rewards Min                 -7.36962
exploration/Returns Mean              -103.308
exploration/Returns Std                  0
exploration/Returns Max               -103.308
exploration/Returns Min               -103.308
exploration/Actions Mean                -0.625227
exploration/Actions Std                  1.05955
exploration/Actions Max                  2.28869
exploration/Actions Min                 -4.0904
exploration/Num Paths                    1
exploration/Average Returns           -103.308
evaluation/num steps total          220000
evaluation/num paths total             220
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.510966
evaluation/Rewards Std                   0.698766
evaluation/Rewards Max                  -0.000530543
evaluation/Rewards Min                  -7.67002
evaluation/Returns Mean               -510.966
evaluation/Returns Std                  45.6918
evaluation/Returns Max                -440.278
evaluation/Returns Min                -583.517
evaluation/Actions Mean                 -0.614993
evaluation/Actions Std                   1.01942
evaluation/Actions Max                   2.79262
evaluation/Actions Min                  -5.34472
evaluation/Num Paths                     5
evaluation/Average Returns            -510.966
time/data storing (s)                    0.000646296
time/evaluation sampling (s)             3.19799
time/exploration real sampling (s)       0.0833097
time/exploration sim sampling (s)        6.89e-06
time/logging (s)                         0.012931
time/saving (s)                          0.0166167
time/training (s)                       24.6913
time/epoch (s)                          28.0028
time/total (s)                        1086.55
Epoch                                   43
----------------------------------  ----------------
2020-02-20 21:00:41.653159 EST | [name-of-experiment_2020_02_20_20_42_08_0000--s-0] Epoch 44 finished
----------------------------------  ----------------
replay_buffer/size                       0
trainer/QF1 Loss                         1.32446
trainer/QF2 Loss                         1.10595
trainer/Policy Loss                     35.6904
trainer/Q1 Predictions Mean            -37.8882
trainer/Q1 Predictions Std              29.0452
trainer/Q1 Predictions Max              -6.74653
trainer/Q1 Predictions Min             -96.7905
trainer/Q2 Predictions Mean            -37.7791
trainer/Q2 Predictions Std              28.9858
trainer/Q2 Predictions Max              -6.69938
trainer/Q2 Predictions Min             -97.8541
trainer/Q Targets Mean                 -37.3658
trainer/Q Targets Std                   28.8364
trainer/Q Targets Max                   -6.57376
trainer/Q Targets Min                  -94.8305
trainer/Log Pis Mean                     2.08202
trainer/Log Pis Std                      1.93707
trainer/Log Pis Max                      6.60244
trainer/Log Pis Min                     -3.013
trainer/Policy mu Mean                  -0.882009
trainer/Policy mu Std                    1.29098
trainer/Policy mu Max                    2.68294
trainer/Policy mu Min                   -2.48582
trainer/Policy log std Mean             -0.783911
trainer/Policy log std Std               0.202248
trainer/Policy log std Max              -0.247059
trainer/Policy log std Min              -1.52445
trainer/Alpha                            0.139258
trainer/Alpha Loss                       0.161694
exploration/num steps total           4500
exploration/num paths total             45
exploration/path length Mean           100
exploration/path length Std              0
exploration/path length Max            100
exploration/path length Min            100
exploration/Rewards Mean                -0.762977
exploration/Rewards Std                  0.952973
exploration/Rewards Max                 -0.00893743
exploration/Rewards Min                 -5.67246
exploration/Returns Mean               -76.2977
exploration/Returns Std                  0
exploration/Returns Max                -76.2977
exploration/Returns Min                -76.2977
exploration/Actions Mean                -0.5465
exploration/Actions Std                  1.07722
exploration/Actions Max                  2.58668
exploration/Actions Min                 -2.9201
exploration/Num Paths                    1
exploration/Average Returns            -76.2977
evaluation/num steps total          225000
evaluation/num paths total             225
evaluation/path length Mean           1000
evaluation/path length Std               0
evaluation/path length Max            1000
evaluation/path length Min            1000
evaluation/Rewards Mean                 -0.558016
evaluation/Rewards Std                   0.828579
evaluation/Rewards Max                  -0.000210987
evaluation/Rewards Min                  -8.33757
evaluation/Returns Mean               -558.016
evaluation/Returns Std                  57.8753
evaluation/Returns Max                -457.112
evaluation/Returns Min                -612.271
evaluation/Actions Mean                 -0.59193
evaluation/Actions Std                   1.02651
evaluation/Actions Max                   3.31973
evaluation/Actions Min                  -5.37564
evaluation/Num Paths                     5
evaluation/Average Returns            -558.016
time/data storing (s)                    0.000608837
time/evaluation sampling (s)             2.74477
time/exploration real sampling (s)       0.0672903
time/exploration sim sampling (s)        6.907e-06
time/logging (s)                         0.0127712
time/saving (s)                          0.0163548
time/training (s)                       24.1423
time/epoch (s)                          26.9841
time/total (s)                        1113.54
Epoch                                   44
----------------------------------  ----------------
